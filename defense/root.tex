\documentclass{beamer}

%\usetheme{Boxes}
%\usetheme{boxes}
\usetheme{Boadilla}
%\usetheme{Madrid}

\usepackage[utf8]{inputenc}
\usepackage[english,british]{babel}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{url}
\usepackage{moreverb}
\usepackage{fancyvrb}
\usepackage{eulervm}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage{mathpazo}

\newcommand{\todo}[1]{\textcolor{red}{[TODO] #1}}

\definecolor{primary}{RGB}{0,91,144}
\colorlet{primary-lgt}{primary!50!white}
\colorlet{primary-drk}{primary!50!black}
\definecolor{secondary}{RGB}{176,43,59} 
\colorlet{secondary-lgt}{secondary!50!white}
\colorlet{secondary-drk}{secondary!50!black}
\definecolor{ternary}{RGB}{131,154,40}
\colorlet{ternary-lgt}{ternary!50!white}
\colorlet{ternary-drk}{ternary!50!black}
\definecolor{grey}{RGB}{175,175,175}

\newenvironment<>{cblock}[1]{%
  \setbeamercolor{block title}{fg=white,bg=primary}%
  \begin{block}#2{#1}}{\end{block}}

\newenvironment<>{ccolumn}[2]{\begin{column}[t]{#1}\begin{cblock}{#2}}{\end{cblock}\end{column}}

\usepackage[
  backend=biber,
  maxbibnames=1,
  natbib=true,
  citestyle=authoryear
]{biblatex}

\addbibresource{thesis.bib} % The filename of the bibliography

\hypersetup{colorlinks=true, linkcolor=black, urlcolor=primary, citecolor=black}

\title{{\bf Addressing data scarcity with deep transfer learning and self-training in digital pathology}}
\author{Romain Mormont}
\institute{Montefiore Institute, University of Liège, Belgium}
\date{October 21, 2022}

\beamertemplatenavigationsymbolsempty
\setbeamertemplate{sections/subsections in toc}[circle]
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{itemize subitem}[square]


\setbeamercolor{section in head/foot}{parent=palette primary}
\setbeamercolor{date in head/foot}{parent=palette primary}

\setbeamertemplate{footline}
{
  \leavevmode%
  \hbox{%
    \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{section in head/foot}%
      \usebeamerfont{section in head/foot} \hfill \insertsection%
    \end{beamercolorbox}%
		\begin{beamercolorbox}[wd=.05\paperwidth,ht=2.25ex,dp=1ex,center]{section in head/foot}%
			\ifx\insertsubsection\empty%
			\else%
			\usebeamerfont{section in head/foot} $\vartriangleright$
			\fi%
		\end{beamercolorbox}%
		\begin{beamercolorbox}[wd=0.3\paperwidth,ht=2.25ex,dp=1ex,center]{section in head/foot}%
			\usebeamerfont{subsection in head/foot} \insertsubsection
		\end{beamercolorbox}%
    \begin{beamercolorbox}[wd=.35\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
      \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
      \usebeamertemplate{page number in head/foot}\hspace*{2ex}
    \end{beamercolorbox}
  }%
  \vskip0pt%
}


\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\X}[1]{\textcolor{blue}{#1}}
\newcommand{\y}[1]{\textcolor{red}{#1}}
\newcommand{\model}[1]{\textcolor{mygreen}{#1}}
\newcommand{\loss}[1]{\textcolor{lightblue}{#1}}



\begin{document}
\setbeamertemplate{caption}{\raggedright\insertcaption\par}
\renewcommand{\inserttotalframenumber}{20}
\section{PhD defense}
% Title page ==================================================================

\begin{frame}
\titlepage

\begin{center}
PhD defense
\end{center}
\end{frame}


\section{Introduction}
\begin{frame}{Introduction}
	\hfill
	\begin{figure}
		\begin{subfigure}{0.27\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/old_microscope.jpg}
		\end{subfigure}
		\begin{subfigure}{0.27\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/new_microscope.jpg}
		\end{subfigure}
		\begin{subfigure}{0.27\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/dpath_microscope.jpg}
		\end{subfigure}
	\end{figure}
	\begin{center}
	``\textit{Digital pathology incorporates the acquisition, management, sharing and \textbf{interpretation} of pathology information — including slides and data — in a digital environment}" 
	\end{center}
	\hfill
	{\tiny\textit{left:} institutions.ville-geneve.ch, \textit{center:} verywellhealth.com, \textit{right:} healthcare-in-europe.com}
\end{frame}

\subsection{Pathology workflow}
\begin{frame}{From the body to the computer}
	\vfill

	A pathology workflow: from a biopsy...
	\begin{figure}[h]
		\centering
		\includegraphics[width=\textwidth]{images/pathology_workflow.png}
	\end{figure}
	
	\vfill
	
	... to a whole-slide image and computer-assisted pathology.
	\begin{figure}
		\includegraphics[width=\textwidth]{images/whole-slide-to-cap.png}
		\caption*{\small Left image: 163840 x 95744 pixels$^2$, 2.3 Gb.}
	\end{figure}
	
	\vfill
\end{frame}

\subsection{Computer-aided pathology}
\begin{frame}{Why computer-aided pathology}

\vfill

\begin{center} Some pathology analysis tasks are \textbf{tedious}, \textbf{time-consuming} and/or \textbf{costly}. \end{center} 

Computational methods can make improve analysis reliability and speed therefore \textbf{improving patient outcome and costs}. 

\vfill

\begin{figure}
	\begin{subfigure}{0.25\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/mitosis_zoomout.png}
		\caption{Counting}
	\end{subfigure}
	\begin{subfigure}{0.25\textwidth}
		\centering
		\vfill
		\includegraphics[width=\textwidth]{images/highdatavolume.png}
		\vfill
		\caption{Multi-slide analysis}
	\end{subfigure}
	\begin{subfigure}{0.25\textwidth}
		\centering
		\includegraphics[height=0.2\textheight]{images/whole-slide-dim.png}
		\caption{Needle in a haystack}
	\end{subfigure}
\end{figure}

\vfill
The field concerned with these methods is called \textbf{computational pathology}.

\vfill
\end{frame}

\begin{frame}{Computational pathology is challenging}
	Few of the challenges:
	\begin{itemize}
		\item {high variability}: content, staining, acquisition,...
		\item {data scarcity}: annotating data is expensive and tedious
		\item {big data}: up to millions of biological objects per multi-gigapixel image
	\end{itemize}
	\vfill
	\begin{center}
		\large
		\textbf{Machine learning methods} are excellent candidates for tackling these !
	\end{center}
\end{frame}

\begin{frame}{More on data scarcity}
Causes:
\begin{itemize}
	\item highly specialized annotators required
	\item simplification of the underlying medical problems
	\item privacy concerns
	\item deep learning methods are data hungry 
\end{itemize}

Consequences:
\begin{itemize}
	\item small datasets (compared to natural image domain)
	\item lack of variety on the problem targets 
	\item weakly- or sparsely-labeled datasets 
\end{itemize}
\end{frame}

\begin{frame}{Working around data scarcity}
Making more annotations within the same budget:
\begin{itemize}
	\item AI-assisted annotation
	\item appropriate UI/UX tools 
	\item citizen science/crowdsourcing
\end{itemize}

Using proper computational methods:
\begin{itemize}
	\item \textbf{transfer learning}
	\item \textbf{self-training}
	\item weakly-supervised learning
	\item self-supervised learning
\end{itemize}
\end{frame}

\begin{frame}{Contributions}
	All contributions address data scarcity in digital pathology:
	\begin{itemize}
		\item \fullcite{mormont2018comparison}
		\item \fullcite{mormont2020multi}
		\item \fullcite{mormont2022self}
	\end{itemize}
\end{frame}

\section{Transfer from ImageNet}
\subsection{Introduction}

\begin{frame}{}
	\vfill
	\begin{center}
		\Large
		Contribution \#1 \\
		\vspace{0.75cm}
		\textbf{Transfer learning from ImageNet} \\
		\vspace{0.75cm}
		{\small (thesis chapter 4)}
	\end{center}
	 
	\vfill

	{
		\footnotesize
		\textbf{Article}: \fullcite{mormont2018comparison}
	}	
\end{frame}

\begin{frame}{Deep transfer learning: how to?}
	\begin{figure}
		\centering
		\includegraphics[width=0.9\textwidth]{images/transfer-fe.pdf}
		\caption*{Feature extraction}
	\end{figure}
	\begin{figure}
		\centering
		\includegraphics[width=0.9\textwidth]{images/transfer-ft.pdf}
		\caption*{Fine tuning}
	\end{figure}
\end{frame}

\begin{frame}{Our contribution}

\textbf{Goal}: devise \textbf{guidelines and best practices} for deep transfer learning in computational pathology

\vfill

\begin{itemize}
	\item {Fine-tuning \textit{vs.} feature extraction}: which one works better ? 
 	\item Which {pre-trained network} works better ? 
 	\item {Where to extract} features ? 
 	\item ...
\end{itemize} 

\vfill
We have carried out several experiments \textbf{with ImageNet} as source task:
\vfill
\begin{itemize}
	 \item \textbf{Feature extraction} vs. \textbf{fine-tuning}
	 \item \textbf{Networks}: ResNet50, DenseNet201, VGG16/19, InceptionResNetV2, \textit{etc}.
	 \item \textbf{Features classifiers}: SVM , extra-trees (ET), \textit{etc}.
	 \item Feature extraction at {increasing depth}
	 \item Feature relevance evaluation
\end{itemize}
\end{frame}


\subsection{Data}
\begin{frame}{Datasets}
8 image classification datasets. 

\begin{table}
    \center 
    \footnotesize
    \begin{tabular}{|c|c|c|cc|}
        \hline
        \multirow{2}{*}{Dataset} & \multirow{2}{*}{Domain} & \multirow{2}{*}{Cls} & \multicolumn{2}{c|}{Total} \\
        \cline{4-5}
        & & & Images & Slides \\
        \hline
        Necrosis (N) & Histo & 2 & 882 & 13 \\ % ulg_lbtd2_chimio_necrose
        ProliferativePattern (P) & Cyto & 2 & 1857 & 36 \\ % patterns_no_aug
        CellInclusion (C) & Cyto & 2 & 3638 & 45 \\ % cells_no_aug
        MouseLba  (M) & Cyto & 8 & 4284 & 20 \\ % ulg_lbtd_lba
        HumanLba (H) & Cyto & 9 & 5420 & 64 \\ % ulb_anapath_lba
        Lung (L) & Histo & 10 & 6331 & 882 \\ % ulg_lbtd_tissus
        Breast (B) & Histo & 2 & 23032 & 34 \\ % ulg_lbtd2_chimio_necrose
        Glomeruli (G) & Histo & 2 & 29213 & 205 \\ % glomeruli_no_aug
        \hline
    \end{tabular}
\end{table}
\begin{figure}
    \center
    \begin{subfigure}{0.1\textwidth}\includegraphics[scale=0.35]{images/illus_necrose.png}\end{subfigure}
    \begin{subfigure}{0.1\textwidth}\includegraphics[scale=0.35]{images/illus_patterns.png}\end{subfigure}
    \begin{subfigure}{0.1\textwidth}\includegraphics[scale=0.35]{images/illus_cells.png}\end{subfigure}
    \begin{subfigure}{0.1\textwidth}\includegraphics[scale=0.35]{images/illus_lbtd_lba.png}\end{subfigure}
    \begin{subfigure}{0.1\textwidth}\includegraphics[scale=0.35]{images/illus_anapath.png}\end{subfigure}
    \begin{subfigure}{0.1\textwidth}\includegraphics[scale=0.35]{images/illus_tissus.png}\end{subfigure}
    \begin{subfigure}{0.1\textwidth}\includegraphics[scale=0.35]{images/illus_breast.png}\end{subfigure}
    \begin{subfigure}{0.1\textwidth}\includegraphics[scale=0.35]{images/illus_glomeruli.png}\end{subfigure}
\end{figure}

\end{frame}

\subsection{Experiments and results}
\begin{frame}{Results}
\framesubtitle{Fine-tuning is the best performing strategy}

\begin{center}
	{Fine-tuning} is often the best performing method \\
	... but {extracted features} are {close} on most problems and {less computationally expensive} !
\end{center}
\vfill
\begin{table}
	\center
	\tiny
	\begin{tabular}{l|ccccc|ccc}   
	  & \multicolumn{8}{c}{\textbf{Datasets}} \\
	  \hline 
	  \textbf{Strategy} & \textbf{Cell} & \textbf{Prolif} & \textbf{Glom} & \textbf{Necro} & \textbf{Breast} & \textbf{Mouse} & \textbf{Lung} & \textbf{Human} \\
	  \hline
	  Baseline (ET-FL)      & {\color{grey}0.9250} & {\color{grey}0.8268} & {\color{grey}0.9551} & {\color{grey}0.9805}	& {\color{grey}0.9345} & {\color{grey}0.7568} & {\color{grey}0.8547} & {\color{grey}0.6960} \\
	  Last layer    & {\color{grey}0.9822} & {\color{grey}0.8893} & {\color{grey}0.9938} & \textit{0.9982} & {\color{grey}0.9603} & {\color{grey}0.7996} & {\color{grey}0.9133}	& {\color{grey}0.7820} \\
	  Feat. select.	& {\color{grey}0.9676}	& {\color{grey}0.8861}	& {\color{grey}0.9843}	& \textbf{0.9994}	& {\color{grey}0.9597}	& {\color{grey}0.7438}	& {\color{grey}0.8941}	& {\color{grey}0.7703} \\
	  Merg. networks	& \textit{0.9897}	& \textbf{0.8984}	& {\color{grey}0.9948}	& {\color{grey}0.9864}	& {\color{grey}0.9549}	& \textit{0.8169}	& {\color{grey}0.9155}	& {\color{grey}0.7928} \\
	  Merg. layers	& {\color{grey}0.9808}	& {\color{grey}0.8906}	& {\color{grey}0.9944}	& {\color{grey}0.9964}	& {\color{grey}0.9639}	& {\color{grey}0.7941}	& {\color{grey}0.9268}	& {\color{grey}0.7977} \\
	  Inner ResNet	& {\color{grey}0.9748}	& \textit{0.8959}	& {\color{grey}0.9949}	& {\color{grey}0.9964}	& {\color{grey}0.9664}	& {\color{grey}0.8131}	& \textit{0.9291}	& \textit{0.8113} \\
	  Inner DenseNet	& {\color{grey}0.9862}	& \textbf{0.8984}	& \textit{0.9962}	& {\color{grey}0.9917}	& {\color{grey}0.9699}	& {\color{grey}0.8012}	& {\color{grey}0.9268}	& {\color{grey}0.7967} \\
	  Inner IncResV2	& {\color{grey}0.9873}	& {\color{grey}0.8948}	& \textit{0.9962}	& \textit{0.9982}	& \textit{0.9720}	& {\color{grey}0.8137}	& {\color{grey}0.9234}	& {\color{grey}0.7713} \\
%       Inner IncV3		& {\color{grey}0.9836}	& {\color{grey}0.8899}	& {\color{grey}0.9951}	& {\color{grey}0.9964}	& {\color{grey}0.9731}	& {\color{grey}0.8104}	& {\color{grey}0.9201}	& {\color{grey}0.7507} \\
	   {Fine-tuning}		& \textbf{0.9926}	& {\color{grey}0.8797}	& \textbf{0.9977}	& {\color{grey}0.9970}	& \textbf{0.9873}	& \textbf{0.8727}	& \textbf{0.9405}	& \textbf{0.8641} \\
%%      \hline
%%      \textbf{Best} & 0.9926 & 0.8984 & 0.9977 & 0.9994 & 0.9873 & 0.8727 & 0.9405 & 0.8641 \\
	  \hline
	  \textbf{Metric} & \multicolumn{5}{c|}{Roc AUC} & \multicolumn{3}{c}{Accuracy (multi-class)} \\
	\end{tabular}
	\caption{\footnotesize Best in \textbf{bold}, second best in \textit{italic}}
\end{table}
\end{frame}


\begin{frame}{Results}
\framesubtitle{When working with extracted features, use some inner layer features}
	\vfill	
	\begin{figure}
		\center
		\includegraphics[scale=0.7]{images/all_per_layer_hori_bars.png}
	\end{figure}
	\vfill
\end{frame}


\begin{frame}{Results}
\framesubtitle{More recent networks like DenseNet or ResNet work better}
\begin{figure}
	\center
	\includegraphics[scale=0.65]{images/last_baseline_bars.png}
\end{figure}

{\footnotesize
	\textbf{See also:} \fullcite{kornblith2019better}
}
\end{frame}

\begin{frame}{Conclusion}
	\vfill
	Main {takeaways}:
	\vfill
	\begin{itemize}
		\item {Fine tuning} is the best performing method
		\item {Feature extraction} often close to fine-tuning and less computationally expensive
		\item Prefer {inner layers feature extraction} to last layers feature extraction
		\item Use {more recent networks} such as DenseNet and ResNet
	\end{itemize}
	\vfill
	
\end{frame}

\section{Transfer from pathology data}
\subsection{Introduction}

\begin{frame}{}
	\vfill
	\begin{center}
		\Large
		Contribution \#2 \\
		\vspace{0.75cm}
		\textbf{Multi-task pre-training from pathology data} \\
		\vspace{0.75cm}
		{\small (thesis chapter 5)}
	\end{center}
	 
	\vfill

	{
		\footnotesize
		\textbf{Article}: \fullcite{mormont2020multi}
	}	
\end{frame}

\begin{frame}{Using a pathology problem as a source task ?}
	\vfill
	We have shown that, with ImageNet as a source task, fine-tuning was often a better approach than feature extraction.
	\vfill 
	This suggests that ImageNet features are not perfect and appear to lack specificity.
	\vfill
	\begin{center}
	What about \textbf{using pathology data for the source task} to close the specificity gap ?
	\end{center}
	\vfill
\end{frame}


\begin{frame}{Challenge ?}
	\vfill
	Data scarcity $\rightarrow$ no ImageNet-like dataset in pathology ! 
	\vfill
	\begin{columns}
		\begin{column}[t]{0.49\textwidth}
			\begin{figure}
				\includegraphics[width=.5\textwidth]{images/illuimagenet.jpg}
			\end{figure}
		\end{column}
		\begin{column}[t]{0.49\textwidth}
			\begin{figure}
				\includegraphics[width=.5\textwidth]{images/lizard.png}
			\end{figure}
		\end{column}
	\end{columns}
	\begin{columns}
		\begin{column}[t]{0.49\textwidth}
			{
			  \footnotesize
				ImageNet \parencite{deng2009imagenet}:
				\begin{itemize}
					\item \textbf{Images}: $1.281$ million pictures
					\item \textbf{Annot.}: one label per image
					\item \textbf{Classes}: 1000
				\end{itemize}
			}
		\end{column}
		\begin{column}[t]{0.49\textwidth}
			{
				\footnotesize
				Lizard \parencite{graham2021lizard}:
				\begin{itemize}
					\item \textbf{Images}: 291 regions
					\item \textbf{Annot.}: 500k nuclei
					\item \textbf{Classes}: 6 type of cells
				\end{itemize}
			}
		\end{column}
	\end{columns}
	\vfill
	\begin{center}
		\textbf{Question}: how to pretrain a model on a sufficiently large and versatile source pathology dataset ? 
	\end{center}
\end{frame}

\subsection{Data}
\begin{frame}{Several source tasks?}
	No single large dataset ? Let us use many small datasets ! 

	\begin{figure}
		\captionsetup{font={tiny}}
    \center
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_necrose.png}
        \caption{Necrosis}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_patterns.png}
        \caption{ProliferativePattern}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_cells.png}
        \caption{CellInclusion}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_breast.png}
        \caption{Breast}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_glomeruli.png}
        \caption{Glomeruli}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_lbtd_lba.png}
        \caption{MouseLba}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_anapath.png}
        \caption{HumanLba}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_tissus.png}
        \caption{Lung}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_bonemarrow.png}
        \caption{BoneMarrow}
    \end{subfigure}    
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_camelyon16.png}
        \caption{Camelyon 16}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_bach18.png}
        \caption{BACH18 Micro}
    \end{subfigure} \\
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_janowczyk1.png}
        \caption{Janowczyk1}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_janowczyk2.png}
        \caption{Janowczyk2}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_janowczyk5.png}
        \caption{Janowczyk5}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_janowczyk6.png}
        \caption{Janowczyk6}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_janowczyk7.png}
        \caption{Janowczyk7}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_lbpstroma.png}
        \caption{Stroma LBP}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_mitos14.png}
        \caption{MITOS-ATYPIA}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_tupac.png}
        \caption{TUPAC2016 Mitosis}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_umcm.png}
        \caption{UMCM Colorectal}
    \end{subfigure}
    \begin{subfigure}[t]{0.079\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/illus/illus_warwick.png}
        \caption{Warwick CRC}
    \end{subfigure}
	\end{figure}

\end{frame}

\subsection{Our approach}
\begin{frame}{Multi-task pre-training architecture}
	Use all source classification tasks simultaneously to train the model.
	\begin{figure}
		\includegraphics[width=\textwidth]{images/multitask-training.pdf}
	\end{figure}
	\begin{itemize}
		\item A shared backbone $\theta_s$ (\textit{e.g.}~: ResNet50, DenseNet121)
		\item A task-specific head $\theta_t$ for each task $t$
		\item Training batch sampling: one batch contains samples from different tasks. 
		\item Hyperparameters: learning rate $\gamma$, head learning rate multiplier $\tau_\gamma$
		\item \textbf{Transfer}: after pre-training, $\theta_s$ can be fine-tuned or used as a feature extractor
	\end{itemize}
\end{frame}

\subsection{Evaluation protocol}
\begin{frame}{Evaluation protocol}
\framesubtitle{Leave-one-task-out}
	We want to evaluate how a pre-trained model transfers to a new unseen tasks. 
	\begin{figure}
		\includegraphics[width=\textwidth]{images/transfer-score-process.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Evaluation protocol}
\framesubtitle{Best model selection and evaluation}
	\begin{figure}
		\includegraphics[width=\textwidth]{images/final-perf-process.pdf}
	\end{figure}
\end{frame}

\subsection{Results}
\begin{frame}{Experiments and results}
\framesubtitle{\textbf{Feature extraction} with multi-task pre-trained models}
\vfill
Compared to ImageNet feature extractors, ours either improve or provide comparable results. 
\vfill
\begin{figure}
	\begin{subfigure}{0.48\textwidth}
		\center
		\includegraphics[width=\textwidth]{images/resnet_fe.png}
		\caption{DenseNet121}
	\end{subfigure}
	\begin{subfigure}{0.48\textwidth}
		\center
		\includegraphics[width=\textwidth]{images/dense_fe.png}
		\caption{ResNet50}
	\end{subfigure}
\end{figure}
\vfill
\end{frame}

\begin{frame}{Results}
	\framesubtitle{\textbf{Fine tuning} with multi-task pre-trained models}
	\vfill
	Compared to fine-tuned ImageNet models, ours provide comparable results 
	
	\begin{center}
		Fine-tuning is able to recover lack of specificity of ImageNet features. 
	\end{center}
	
	\vfill
	
	\begin{figure}
		\begin{subfigure}{0.48\textwidth}
			\center
			\includegraphics[width=\textwidth]{images/resnet_ft.png}
			\caption{DenseNet121}
		\end{subfigure}
		\begin{subfigure}{0.48\textwidth}
			\center
			\includegraphics[width=\textwidth]{images/dense_ft.png}
			\caption{ResNet50}
		\end{subfigure}
	\end{figure}
\end{frame}

\begin{frame}{Results}
	\framesubtitle{Comparing to training from scratch and joint training}

	Joint training only improves over fine-tuning on small datasets \\
	
	This experiment a	also confirms previously published results including ours:
	\begin{itemize}
		\item fine-tuning outperforms feature extraction
		\item training from scratch is subpar compared to all other approaches
	\end{itemize}

	\begin{figure}
		\begin{subfigure}{\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/densenet_all.png}
			\caption{DenseNet121}
		\end{subfigure}
	\end{figure}

\end{frame}

\subsection{Conclusion}
\begin{frame}{Conclusion}
	Compared to ImageNet pretrained models, our models:
 	\begin{itemize}
		\item used as \textbf{feature extractors} yield comparable or better performance
		\item \textbf{fine-tuned} yield comparable performance
	\end{itemize}
	\vfill
	Also \textbf{training from scratch underperforms} compared to all other evaluated transfer approaches on our tasks.
\end{frame}

\section{Self-training}
\subsection{Introduction}

\begin{frame}{}
	\vfill
	\begin{center}
		\Large
		Contribution \#3 \\
		\vspace{0.75cm}
		\textbf{Self-training for segmentation from sparsely-labeled data} \\
		\vspace{0.75cm}
		{\small (thesis chapter 6)}
	\end{center}
	 
	\vfill

	{
		\footnotesize
		\textbf{Article}: \fullcite{mormont2022self}
	}	
\end{frame}

\begin{frame}{Thyroid FNAB: an imperfectly-annotated cytology dataset}
	\vfill
	\begin{center}
	A sparsely annotated dataset from the Erasme Hospital, Brussels (Dr. Isabelle Salmon's team) for thyroid nodule malignancy assessment.
	\end{center}
	\vfill
	\begin{columns}
		\begin{column}[t]{0.48\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/sparse_thyroid_example1.png}
		\end{column}
		\begin{column}[t]{0.48\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/sparse_thyroid_example2.png}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Binary segmentation with a sparse dataset ?}
	\vfill
	\begin{center}
		Let's generate pseudo-labels for unlabeled pixels before training !
	\end{center}
	\vfill
	\begin{center}
	  How ? With a \textbf{self-training} algorithm !
	\end{center}
	\vfill
\end{frame}

\subsection{Problem settings}
\begin{frame}{Problem settings}
\framesubtitle{A sparsely-labeled dataset}
	\begin{columns}[t]
		\begin{ccolumn}{0.48\textwidth}{$\mathcal{D}_l$ - exhaustively-labeled set}
			
			\begin{figure}
				\center
				\includegraphics[width=\textwidth]{images/exhaustive_set.png}
			\end{figure}
		\begin{center}
				{\footnotesize $n_l$ images and masks. All pixels have a 0 (background) or 1 (foreground) label.}
		\end{center}
		\end{ccolumn}
		% --------------------------
		\begin{ccolumn}{0.48\textwidth}{$\mathcal{D}_s$ - sparsely-labeled set}
			\begin{figure}
				\center
				\includegraphics[width=\textwidth]{images/sparse_set.png}
			\end{figure}
		\begin{center}
				{\footnotesize $n_s$ images and masks. Unlabeled pixels have label 0 (background) and labeled pixels are exclusively foreground.}
		\end{center}
		\end{ccolumn}
	\end{columns}
\end{frame}

\begin{frame}{Problem settings}
\framesubtitle{A segmentation architecture}
	\vfill
	A downscaled UNet \parencite{ronneberger2015unet} architecture: reduce the number of parameters by a factor 8. 
	\vfill
	\begin{figure}
		\centering
		\includegraphics[height=0.7\textheight]{images/unet.png}
	\end{figure}
	\vfill
\end{frame}

\subsection{Our algorithm}
\begin{frame}{Our self-training algorithm}
	\begin{figure}
		\centering
		\includegraphics[width=\textwidth]{images/selftrain-algo.png}
	\end{figure}
\end{frame}

\begin{frame}{Pseudo-labeling: hard \textit{vs.} soft}
	Generating a pseudo-label $y^{({pl})}_{ij}$ for a pixel from the model prediction $\hat{y}_{ij}$ for this pixel:
	\[
		y^{({pl})}_{ij} = \begin{cases}
			1,\,\text{if}\, y_{ij} = 1 \\
			g(\hat{y}_{ij}),\,\text{otherwise}
		\end{cases}
	\]

	\begin{columns}
		\begin{ccolumn}{0.48\textwidth}{Soft labeling}
			\tiny
			Use the output of the model as-is:
			\[
				g(x) = x
			\]
			The pseudo-label can be seen as a probability in $\left]0,1\right[$ .
		\end{ccolumn}
		\begin{ccolumn}{0.48\textwidth}{Hard labeling}
			\tiny
			Binarize the model output with the threshold:
			\[
				g(x) = \begin{cases}
				1,\,\text{if}\, x > T_e\\
				0,\,\text{otherwise}
				\end{cases}
			\]
			Determine the threshold by optimizing the Dice coefficient on the exhaustively-labeled set $\mathcal{D}_l$: 
			\begin{align*}
			Dice_T(\mathbf{y},\hat{\mathbf{y}}) = \dfrac{2 \times \sum_{i,j} \left[\mathbb{1}_{\hat{y}_{ij} \geq T} \times y_{ij}\right]}{\sum_{i,j} \mathbb{1}_{\hat{y}_{ij} \geq T} + \sum_{i,j} y_{ij}} \\
			T_e = \arg \underset{T}{\max} \sum_{(\mathbf{x}, \mathbf{y}) \in \mathcal{D}_l} \text{Dice}_T\left(\mathbf{y},h( \mathbf{x}; \theta_{e})\right)
			\end{align*}
		
		\end{ccolumn}
	\end{columns}
\end{frame}

\begin{frame}{Weighting strategies}
	We weight the pixel contribution in the loss:
	\[
		\mathcal{L} = \frac{1}{|\mathbf{y}|}\sum_{i}\sum_{j} w_{ij} \ell(\hat{y}_{ij}; y_{ij}) 
	\] 
	The weighting strategy is an hyperparameter:
	\begin{itemize}
		\item \textbf{Constant}: $w^{(cst)}_{ij} = C > 0$ where $C$ is an hyperparameter
		\item \textbf{Entropy}: $w^{(ent)}_{ij}$ is the entropy of the model prediction $\hat{y}_{ij}$
		\item \textbf{Consistency}: $w^{(cty)}_{ij}$ is a consistency score between model predictions of pixel $(i,j)$ and close pixels
		\item \textbf{Merged}: $w^{(mgd)}_{ij}$ combines the \textit{entropy} and \textit{consistency} strategies
	\end{itemize}
	Eventually, $w_{ij}$ is obtained by normalizing the weights computed over a patch so that they sum to 1.
\end{frame}

\subsection{Experiments}
\begin{frame}{Experiments}
\framesubtitle{3 public datasets}
	\centering
	\begin{columns}
		\begin{ccolumn}{0.30\textwidth}{MoNuSeg}
			\centering \includegraphics[width=\textwidth]{images/monuseg_rep.png}
			\begin{raggedright} \tiny \parencite{kumar2019multi} \end{raggedright}
		\end{ccolumn}
		% --------------------------
		\begin{ccolumn}{0.30\textwidth}{SegPC}
			\centering \includegraphics[width=\textwidth]{images/segpc_rep.png}
			\begin{raggedright} \tiny \parencite{gupta2021segpc} \end{raggedright}
		\end{ccolumn}
		% --------------------------
		\begin{ccolumn}{0.30\textwidth}{GlaS}
			\centering \includegraphics[width=\textwidth]{images/glas_rep.png}
			\begin{raggedright} \tiny \parencite{sirinukunwattana2017gland} \end{raggedright}
		\end{ccolumn}
	\end{columns}
	\vfill
	Sparsity is simulated by randomly removing $\rho \%$ of annotations in $n_s$ images.
	\vfill
\end{frame}

\begin{frame}{Experiments}
\framesubtitle{3 baselines}
	\vfill
	\begin{columns}[t]
		\begin{ccolumn}{0.30\textwidth}{(1) $|\mathcal{D}_s| = 0$ }
			\begin{figure} \centering \includegraphics[height=0.3\textheight]{images/baseline1.png} \end{figure}%
\vspace{-0.8cm}
\begin{flushright}{\tiny (orignal dataset)}\end{flushright}
		\end{ccolumn}
		\begin{ccolumn}{0.30\textwidth}{(2) $\mathcal{D}_l$ only}
			\begin{figure} \centering \includegraphics[height=0.3\textheight]{images/baseline2.png} \end{figure}
		\end{ccolumn}
		\begin{ccolumn}{0.30\textwidth}{(3) $\mathcal{D}_l \cup \mathcal{D}_s$}
			\begin{figure} \centering \includegraphics[height=0.3\textheight]{images/baseline3.png} \end{figure}%
\vspace{-0.5cm}
\begin{raggedright}{\tiny (unlab. pixels assigned to background)}\end{raggedright}%
		\end{ccolumn}
	\end{columns}
	\vfill
	\begin{center}
		To be considered of interest, our method should be as close as possible to (1) (upper bound) and outperform baselines (2) and (3).
	\end{center}
	\vfill
\end{frame}

\subsection{Results}
\begin{frame}{Results}
\framesubtitle{Hard vs. soft labeling}
	\vfill
	For the given data scarcity regime ($\rho = 90\%$):
	\begin{itemize}
		\item The best performance are obtained with hard labels.  
		\item Soft labeling yields more stability as performance are less impacted by the choice of a weighting strategy  
	\end{itemize}

	\vfill

	\begin{figure}
		\centering
		\includegraphics[width=\textwidth]{images/plot_hard_vs_soft_test_pxl_self_hard_dice.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Results}
\framesubtitle{Self-training at fixed $n_l$ - MoNuSeg}
	\vfill
	\begin{itemize}
		\item There is always a cut-off point at which \textbf{exploiting additional sparse annotations with self-training becomes beneficial} !
		\item Self-training struggles at very high data scarcity
		\item Using $\mathcal{D}_s$ as if it was exhaustively annotated is a bad idea
	\end{itemize}
	\begin{center}
		For MoNuSeg, the upper baseline is reached with only $\sim 30\%$ of the original annotations. \\
		{\tiny (70\% of the annotations were useless ??)}
	\end{center}	
	\vfill
	\begin{figure}
		\centering
		\includegraphics[width=\textwidth]{images/monuseg_test_pxl_self_hard_dice_rho.pdf}
	\end{figure}
\end{frame}


\begin{frame}{Results}
	\framesubtitle{Self-training at fixed $n_l$ - SegPC and GlaS}
	\begin{figure}
		\begin{subfigure}{\textwidth}
			\centering
			\caption{Glas}
			\includegraphics[width=\textwidth]{images/glas_test_pxl_self_hard_dice_rho.pdf}
		\end{subfigure}
		\begin{subfigure}{\textwidth}
			\centering
			\caption{SegPC}
			\includegraphics[width=\textwidth]{images/segpc_test_pxl_self_hard_dice_rho.pdf}
		\end{subfigure}
	\end{figure}
\end{frame}

\begin{frame}{Results}
\framesubtitle{Label sparsely or exhaustively ?}
	\begin{itemize}
		\item The answser is dataset-dependant !
		\item MoNuSeg: annotation budget better spent on sparse labeling with self-training
		\item Others: annotation better spent on exhaustive labeling and using supervised training
	\end{itemize}
	\begin{figure}
		\centering
		\begin{subfigure}{0.32\textwidth}
			\includegraphics[width=\textwidth]{images/annot_strat_monuseg.pdf}
			\caption{MoNuSeg}
		\end{subfigure}
		\begin{subfigure}{0.32\textwidth}
			\includegraphics[width=\textwidth]{images/annot_strat_glas.pdf}
			\caption{GlaS}
		\end{subfigure}
		\begin{subfigure}{0.32\textwidth}
			\includegraphics[width=\textwidth]{images/annot_strat_segpc.pdf}
			\caption{SegPC}
		\end{subfigure}
	\end{figure}
\end{frame}

\begin{frame}{Results}
\framesubtitle{Application to Thyroid FNAB}

\vfill
\begin{center}
	Self-training significantly outperforms the ``$\mathcal{D}_l$ only'' and ``$\mathcal{D}_l \cup \mathcal{D}_s$'' baselines.
\end{center}

\vfill
\begin{table}[t]
  \centering 
  \begin{tabular}{|c|c|}
    \hline
    Method & $\text{Dice}^*$ (\%)\\
    \hline
    Self-training & $89.05 \pm 0.85$ \\
    $\mathcal{D}_l$ only & $80.30 \pm 5.39$\\  
    $\mathcal{D}_l \cup \mathcal{D}_s$ & $83.62 \pm 3.52$\\
    \hline
  \end{tabular}
\end{table}
\vfill

\begin{figure}
	\centering
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/thyroid_wsi_example1.png}	
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/thyroid_wsi_example2.png}	
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/thyroid_wsi_example3.png}	
	\end{subfigure}
\end{figure}
\end{frame}


\begin{frame}{Self-trained model integration in a workflow}
	Workflow to detect atypical cells.  
	\begin{figure}
		\includegraphics[width=\textwidth]{images/pipeline.pdf}		
	\end{figure}
\end{frame}

\begin{frame}{Conclusion}
	\begin{itemize}
		\item Self-training can be used to obtain competitive segmentation performance with less annotations !
		\item Some datasets might benefit more from sparse labeling then exhaustive labeling
	\end{itemize}

	Future works:
	\begin{itemize}
		\item From binary to multi-class segmentation
		\item How does the type of dataset impact the performance ?
		\item Study the interest of the method in low scarcity conditions
	\end{itemize}
\end{frame}

\section{Conclusion of the thesis}
\begin{frame}{Conclusion of the thesis}
	\begin{exampleblock}{}
		Data scarcity in digital pathology is not an inextricable issue and there exists algorithmic solutions to at least alleviate it.
	\end{exampleblock}
	\vfill
	\begin{itemize}
		\item Transfer learning is an effective approach: feature extraction is a strong baseline, fine-tuning is better
		\item Multi-task pre-training is a viable strategy to pre-train a model on pathology data
		\item Self-training with sparsely-labeled datasets can be used to cut down the dataset size requirements for training a segmentation model 
	\end{itemize}
\end{frame}

\section{Future works}
\begin{frame}{Perspectives}
	\begin{itemize}
		\item 
	\end{itemize}
\end{frame}

\begin{frame}
	\vfill
	{\Large Thank you !}
	\vfill
\end{frame}

%\begin{frame}{Deep learning}
%\vspace*{-0.5cm}
%\begin{columns}
%%
%   \begin{ccolumn}{0.48\textwidth}{blue}{Training from scratch}
%   		Initialize weights randomly and train as usual.
%        \begin{itemize}
%			\item[$-$] Needs lots of data
%			\item[$-$] Needs much computing resources 
%		\end{itemize}
%    \end{ccolumn}
%
%
%\end{columns}
%\begin{columns}
%
%	\begin{ccolumn}{0.48\textwidth}   {red}{Off-the-shelf}
%		Use pre-trained weight, then use features off-the-shelf.
%		\begin{itemize}
%			\item[$+$] Needs less data then training from scratch
%			\item[$+$] Needs less computing resources 
%		\end{itemize}
%	\end{ccolumn}
%
%
%
%	\begin{ccolumn}{0.48\textwidth}{green}{Fine-tuning}
%		Initialize with pre-trained weigths, and train for few epochs.
%		\begin{itemize}
%			\item[$+$] Needs less data than training from scratch
%			\item[$-$] Needs much computing resources 
%		\end{itemize}
%	\end{ccolumn}
%
%\end{columns}	
%
%\end{frame}
%\begin{frame}{Deep transfer learning: how to ?}
%\textbf{Aim}: evaluate which \textbf{deep transfer learning} strategy yields best results using the 8 datasets.
%
%\begin{itemize}
%	\item Deep learning works well with images 
%	\item Transfer learning usually requires less data and computing resources {\color{red} (cite)}
%	\item DTL has been applied to biomedical problems since 2015 {\color{red} (cite)}
%\end{itemize} 
%
%But... \textbf{few people have actually studied how DTL should be applied} !
%
%\begin{itemize}
%	\item most publications use "old" networks (AlexNet, VGGs, GoogLeNet,...)
%	\item studies which do are often too small scale to draw general conclusions 
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Framework}
%\begin{figure}
%	\includegraphics[scale=0.35]{images/offtheshelf_schema.png}
%\end{figure}
%\end{frame}
%
%\begin{frame}{Transfer strategies}
%\begin{columns}
%
%\begin{column}{0.31\textwidth} 
%	\begin{orangebox}{Last layer features}
%		
%	\end{orangebox}
%\end{column}
%
%\begin{column}{0.31\textwidth} 
%	\begin{greenbox}{Feature selection}
%		
%	\end{greenbox}
%\end{column}
%
%\begin{column}{0.31\textwidth}
%	\begin{redbox}{Merging models}
%		
%	\end{redbox} 
%\end{column}
%
%\end{columns}
%
%\begin{columns}
%
%\begin{column}{0.31\textwidth} 
%	\begin{purplebox}{Merging networks}
%		
%	\end{purplebox}
%\end{column}
%
%\begin{column}{0.31\textwidth} 
%	\begin{brownbox}{Fine-tuning}
%		
%	\end{brownbox}
%\end{column}
%
%\begin{colorbox}{0.31\textwidth}{bluebox}{Inner layers}
%\end{colorbox}
%
%\end{columns}
%\end{frame}
\end{document}
