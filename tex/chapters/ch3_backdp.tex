\chapter{Digital pathology}
\label{chap:backdp}

\begin{overview}{Overview}
  The goal of this chapter is to provide \acrlong{dp} background and keys to understand our contributions. We will focus our attention on topics relevant to this thesis. 
  
  Section \ref{sec:backdp:whatisdp} introduces and defines medical terms such as \textit{pathology}, defines \acrfirstit{dp} and introduces the notion of \acrfirstit{wsi}. Section \ref{sec:backdp:wsi} presents the journey of a sample from the body to the \acrshort{wsi}, introducing the different sources of variability introduced by the whole conversion process. 
\end{overview}

% analogintelligence.com image dp illustration

\section{What is \acrlong{dp}?}
\label{sec:backdp:whatisdp}

Nowadays, medicine and healthcare rely heavily on analysis of human body samples to study and diagnose diseases. The branch of medicine focusing on this analysis is called \textit{pathology} which includes histology-based pathology (\aka histopathology) and cytology-based pathology (\aka cytopathology). Both of these sub-branches involve the study of microscope glass slides containing samples (see Figure \ref{fig:backdp:glassslides}). Histology samples are tissue sections cut from a bodily specimen. Cytology is concerned with samples of free cells or tissue fragments. \TODO{experimental histology ?}

\begin{figure}
  \centering
  \subfloat{\includegraphics[height=150pt]{backdp/microscope-slide.png}}\hspace{1cm}
  \subfloat{\includegraphics[height=150pt]{backdp/wsi.png}}
  \caption{\textit{Left:} microscope slides with tissue samples (source: \parencite{img:glassslides}). \textit{Right:} a whole-slide image of dimensions 30720 x 25600 pixels}
  \label{fig:backdp:glassslides}
\end{figure}

The trend of digitalization affecting our societies also impacts pathology as, using dedicated scanners, these glass slides can now be digitized into large image files called \acrfirstit{wsi}. These files associated with subject metadata are stored in computer systems commonly called \acrfirst{pacs} or \acrfirst{lis}. In this context, \acrfirstit{dp} can be defined as ``\textit{the acquisition, management, sharing and interpretation of pathology information — including slides and data — in a digital environment}'' \parencite{doolan2019whatisdp}. Working with \acrshort{wsi} instead of physical slides has several advantages and drawbacks. Aside from easier sharing and storing of slides, digitization also opens the way for automated analysis sofware to automatically extract relevant information, typically with the help of \acrlong{ai} and \acrlong{ml}. The branch of \acrlong{dp} interested in such analysis is called \acrfirstit{cpath} and holds great promises for the future. Indeed, \acrlong{cpath} techniques have the potential to relieve pathologists from easy but time-consuming tasks allowing them to focus on challenging cases and research therefore reducing healthcare cost and improving diagnosis quality. On a larger scale, they hold promises for increased coverage and quality of healthcare around the world and especially in low incomes countries where the number of pathologists per inhabitant is typically insufficient. According to the \acrshort{who} cancer report \parencite{world2020report}, the ratio of pathologist per inhabitant was approximately 1 per 15000 in high income countries in 2020 but dramatically drops to 1 per million or less in many low income countries. A throrough list of advantages and drawbacks of \acrlong{dp} can be found in Table 1 of \parencite{jahn2020digital}.

Interestingly, although digitization technologies are quite mature, adoption of \acrlong{dp} in healthcare facilities is not simple. Many still heavily rely on glass slides for day-to-day operations. Indeed, the transformation requires to modernize the whole hardware (scanners, workstation) and software (slide viewers, information system) infrastructures and to re-think entirely the processes of the facility \parencite{temprana2022digipatics}. This obviously requires significant investments both in time and money and careful planning to carry it out successfully which is not always comptabile with the workload of pathology services or research laboratories. Other difficulties might arise, slowing down the transformation, such as reluctance to change and lack of confidence in modern tools for slide visualization and analysis. 
%Moreover, some tasks cannot be performed easily on \acrlong{wsi}s but can on a glass slide (\eg exploring the depth of a sample in cytology by changing the focal plane). 

As far as automated analysis is concerned, it remains quite a challenge. Whole-slide images typically contain several billions of pixels at full resolution which implies longer processing times and memory issues compared to classical images. Moreover, for most tasks, the image content is complex and traditional computer vision methods (\eg thresholding) would often fail to distinguish structures of interest. This complexity is increased by the presence of artifacts \parencite{taqi2018review} appearing during the conversion process of a bodily specimen to an image. An artifact is a visual or physical alteration of a sample that can hamper its analysis, automated or not. Artifacts introduce a source of variability that algorithms must learn to deal with. At worst, they can prevent any meaningful analysis by hiding, destroying or changing the appearance of the structures of interest (more on artifacts in Section \ref{sec:backdp:wsi}). The ability of learning techniques to train models that capture complex relationships in data makes \acrlong{ml} an ideal candidate to tackle \acrlong{cpath} tasks. However, data scarcity is a prevalent issue in the field as quality data, especially annotated, can be difficult to obtain for various reasons: privacy concerns, time-consuming and expensive nature of the annotation process, \etc.     

Overall, \acrlong{dp} holds great promises but presents significant and interesting challenges on several fronts. This thesis focuses on the \acrshort{ml}-based automated analysis aspects of \acrlong{cpath} and studies how to tackle data scarcity in particular.

\section{A journey from the body to the computer}
\label{sec:backdp:wsi}

Turning a bodily specimen into \acrlong{wsi}s is a long and complex multi-step process typically involving the work of several highly-specialized technicians and machines. Some steps can nowadays be automated but the chain remains mostly manual. In this section, we describe the different steps of this procedure which is summarized in Figure \ref{fig:backdp:overallprocess}. An alternate presentation of the process can be found in \parencite{mccann2014automated} (including illustrations). Sample preparation can differ more or less dependending on the nature of the sample (\eg histology, cytology, hematology) or target imaging technique (\eg brightfield, fluoresence, multispectral). For the sake of brevity, our description focuses on histology with a tissue section prepared for brightfield microscopy and scanning, brightfield being one of the most common modality used in histo- and cytopathology. We will also present few technical details related to \acrshort{wsi} files structure and visualization. 

Throughout the section, we will discuss some of the possible artifacts resulting from the transformation procedure. Our presentation of artifacts will not be exhaustive. A more thorough list of pre-scan artifacts with illustrations can be found in \parencite{taqi2018review}. \TODO{discuss how often alterations can appear !!  talk about the effect of automation on the presence of artifacts !!}

\begin{figure}
  \centering
  \includegraphics{backdp/overallprocess.png}
  \caption{Summary of the transformation of a specimen into a \acrlong{wsi}.}
  \label{fig:backdp:overallprocess}
\end{figure}

\subsection{Specimens collection, fixation, cutting and dehydratation}

Whether it is for research or diagnosis, the slide preparation process starts with a specimen, a piece of human or animal body for which a question must be answered. The specimen can be as large as a whole organ but can also be as small as a drop of bodily material commonly referred to as a \textit{biopsy}. Before going through with the preparation process, a specimen must be fixated. The goal of fixation is to put a stop to the natural decay of the specimen and increase its structural stability \parencite{rolls2012process}. This can be achieved, for instance, by immersing the specimen in a formaldehyde bath (\ie the fixative solution) for period of time depending on its size (\ie few hours to a whole day). 

When the specimen has been fixated, it is then placed into a standardized container called a \textit{cassette} (see Figure \ref{fig:backdp:cassette}). If it is too large for the cassette (\eg an organ), one or more volumes of interest are cut from the specimen. Depending on the later examination, the orientation of the cut can be crucial to exhibit relevant tissue structures of the specimen. In the remaininder, we will call a \textit{sample} the content of this cassette.

For a proper analysis, the cellular morphology of the sample must be preserved. This is most commonly achieved by infiltrating the tissue with paraffin wax. Infiltration however does not work on a raw fixated tissue because paraffin is hydrophobic. Therefore, one must first perform \textit{dehydratation}, that is, replacing water naturally present in the sample with a product miscible with parrafin. This is done by first immersing the sample into a succession of alcoholic solutions baths. Although this process achieves dehydratation, alcohol does not mix with paraffin neither. Therefore, the sample is then immersed into one or more xylene-based solutions baths, xylene being miscible with both alcohol and paraffin. The sample, infiltrated with xylene, is finally immersed in a paraffin bath under vacuum. The dehydratation process takes few hours and is often automated using dedicated machines.

The earliest source of artifacts is the specimen extraction process itself. The specimen can indeed be damaged by the use of certain tools (\eg burned by an electrical scalpel) or treatment at the extraction site. Unlike these, the following artifacts are caused by the early stages of the slide preparation process. Bad fixation can lead to decaying tissue (\ie autolysis) and structural degradation (\eg tissue shrinkage). Improper cutting can also cause tissue damage like tearing and squeezing. Improper dehydratation can leave some parts of the sample with remaining water, alcholol or xylene. Tissues can also be exposed to the different solutions for an excessive duration. These processing errors can for instance cause tearing, shrinkage, interference with the staining process (see Section \ref{ssec:backdp:staining}) and affect the structural properties of the tissue (\eg tissue becomes brittle).

\TODO{big figure illustrating ``all'' possible artifactsand variations}

% \begin{figure}
%   \centering
%   \subfloat[This is an example of well-fixed tissue showing good nuclear and cytoplasmic morphology with minimal shrinkage showing clearly defined basement membranes and cell margins.]{\includegraphics[scale=1.0]{backdp/fixationgood.png}\label{fig:backdp:goodfixation}}\quad
  
%   \subfloat[This is an example of poorly-fixed tissue showing inferior nuclear and cytoplasmic morphology with excessive shrinkage and poorly defined cell margins.]{\includegraphics[scale=1.0]{backdp/fixationbad.jpg}\label{fig:backdp:badfixation}}
    
%   \caption{Examples of good and bad fixations (images and sub-captions from \parencite{rolls2012process}).}
%   \label{fig:backdp:fixation}
% \end{figure}

\begin{figure}
  \centering
  \subfloat[Cassette with fixated samples \\(source: \parencite{stidworthy2011getting})]{\includegraphics[height=150pt]{backdp/cassette.jpg}\label{fig:backdp:cassette}}\quad
  \subfloat[Cassette with parrafin-embedded samples \\(source: \parencite{img:cassetteparrafin})]{\includegraphics[height=150pt]{backdp/cassette-parrafin.jpg}\label{fig:backdp:cassette-parrafin}}
  \caption{Tissue cassettes.}
\end{figure}

\subsection{Embedding, microtomy and glass-slide application}
\label{ssec:backdp:embedding}

At this point, the sample in the cassette has been infiltrated with paraffin. The next step consists in embedding the infiltrated sample in a block of paraffin to allow easier cutting. The sample is placed in a small container attached to the back of the cassette. This container will serve as a mold for casting the block of parrafin (see Figure \ref{fig:backdp:cassette-parrafin}). When the block has solidified, the sample can now be cut into thin slices to be applied on the glass slides. Cutting is performed with a dedicated tool called a \textit{microtome} (see Figure \ref{fig:backdp:microtome}). Operated by a technician, the microtome allows slices to be cut to an extremely small and precise thickness of around 3 or 4 $\mu m$. The slices are then floated onto a water bath which helps mouting them on glass slides. 

There exists automated embedding machines, but microtomy remains mostly manual. Although modern microtomes are equipped with automation features making them more ergonomic and convenient to use, they are still operated by technicians. To the best of our knowledge there is no system that produce unstained slide from an paraffin-embedded sample without active human intervention. 

\begin{figure}
  \centering
  \includegraphics[scale=0.25]{backdp/microtome.jpg}
  \caption{A microtome}
  \label{fig:backdp:microtome}
\end{figure}

These steps should be performed carefully not to introduce artifacts. For instance, a warm and soft parrafin block, a dull microtome blade or a calcification in the tissue can cause compression artifacts (\ie tissue displacement causing material accumulation). A calcificaiton is solid calcium deposit present in the tissue that the blade cannot cut through. This deposit is therefore pushed through the tissue compressing underlying tissues. Another possible source of artifact is contamination of the water bath with previous samples, hair, dust which can in turn contaminate the floating slices.   

\subsection{Staining}
\label{ssec:backdp:staining}
Mounted tissue slices are almost completely transparent which would prevent any meaningful analysis. They must therefore be stained to highlight structures of interest (see Figure \ref{fig:backdp:stainedvsnonstained}). Similarly as for dehydratation, this process consists in immersing the slide into a succession of stainning solutions. The nature of these solutions will depend on the content that should be highlighted for the future analysis. The most common and standard staining in histology is called \acrfirstit{heeo}. Hematoxylin stains nucleic acids in a deep blue-purple color (typically cell nuclei) and eosin nonspecifically stains proteins in a pink color (typically extracellular matrix and cytoplasm). The \acrshort{heeo} stain, although most common, is not the only available. There exists many other staining techniques such as \acrfirstit{ihc} which exploits the binding nature of some antibodies with specific proteins. Markers can then be used to highlight the antibodies, hence the proteins of interest they have bound with. Because the \acrshort{ihc} staining can be very selective, a counterstain (\eg hematoxylin) is often applied to highlight   the rest of the tissue. Counterstain is particularily important when different slices of a tissue stained with different techniques must be compared together because it helps matching the slices spatially. An example of a tissue stained with \acrshort{heeo} and \acrshort{ihc} is given in Figure \ref{fig:backdp:heeo-ihc}. When the sample has been stained and cleaned from remaining excess of staining solutions, one must apply a cover slip on the sample in order to ensure that sample lies in one single plane as the focal plane of microscopes and scanners is usually quite narrow. The cover slip also protects the sample from external contamination and degradation. 

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{backdp/staindvsnonstained.png}
  \caption{\textit{Left:} an unstained tissue section mounted on a glass slide. \textit{Middle:} \acrshort{heeo}-stained section coming from the same tissue. \textit{Right:} the source tissue embedded in a paraffin block \\ (source: \parencite{abbasi2019all})}
  \label{fig:backdp:stainedvsnonstained}
\end{figure}

The choice of a staining and its clean application are crucial for an efficient analysis. The staining baths can become contaminated with samples, by-products resulting from chemical reactions (\eg precipitation or crystallization of chemical components resulting in the presence of pigments in the sample) or external objects (\eg hair, dust). The baths tend to degrade over time as moving slides from one bath to another transfer some staining solutions as well. The degradation of the staining solutionscan cause variation in staining intensities between earlier and later samples. The bath duration is important for proper staining and bathing samples for less or more time than recommanded can respectively cause under- or over-staining. Moreover, insufficient cleaning after staining can leave spots of stain on the slide. Improper application of the cover slip can for instance cause the presence of air bubbles. Nowadays, the staining process can be automated with dedicated machine reducing the variability of the process. 

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{backdp/heeo-ihc.jpg}
  \caption{Two slices of the same tissue stained with \acrshort{heeo} (left) and \acrshort{ihc} (right). \\(source: \parencite{litjens2018camelyon})}
  \label{fig:backdp:heeo-ihc}
\end{figure}

\subsection{Scanning}
\label{ssec:backdp:scanning}

A glass slide coming out of the staining process described in Section \ref{ssec:backdp:staining} is ready to be analysed with an optical microscope but can also be digitized by a slide scanner into a \acrshort{wsi}. In this section, we will briefly describe few key elements related to slide scanning. A more thorough technical presentation and discussion of scanning technologies can be found in \parencite{patel2021contemporary}. 

%(see Figure \TODO{illustrate a scanner})

Scanners are equipped with high-precision lenses and sensors allowing them to generate very high-resolution images required for a proper analysis. Magnification and resolution are two popular metrics used to describe the visual quality of the resulting image. Magnification advertised by scanner vendors refers to the maximum size of the objects in the scanned image with typical values being $\times$20 or $\times$40 (\ie objects appear 20 or 40 times larger than they actually are). Resolution determines the extent to which smaller objects can be resolved. Resolution is reported for a given magnification level and often lies around 0.25 $\mu$m/pixel at $\times$40 for modern scanners but higher magnification and resolution are possible. These magnifications are standard and, coupled with an adequate resolution, are usually sufficient for routine analysis of \acrshort{heeo} or \acrshort{ihc} slides \parencite{zarella2019practical}.

\begin{figure}
  \centering
  \subfloat[][$\times$1.25, 7.24 $\mu$m/pixel]{\includegraphics[height=200pt]{backdp/magres_x1.25_cell.png}}
  \hspace{0.5cm}
  \subfloat[][$\times$40, 0.226 $\mu$m/pixel]{\includegraphics[height=200pt]{backdp/magres_x40_cell.png}}
  \caption{Two images (approximately 200 $\times$ 200) extracted from the same WSI at different magnification and resolution (reported in subcaptions). They are both centered on the same cell.}
\end{figure}

Given the need for high magnification and resolution, it is not possible to capture the whole slide in a single shot with currently available sensors. Therefore, scanners typically capture a sample step by step either tile by tile or in a in-line fashion and then assemble together the different parts using a stitching algorithm. Obviously, as a slide is scanned in several shots, the scanner must ensure that focus is correct for all the shots in order to avoid blur. Common focus strategies are for instance re-focusing every tile, or every $n^{\text{th}}$ tile. When it comes to the focus strategy, there is usually a trade-off between scanning time and focus precision: focusing every tile allows for ideal focus but takes time, whereas focusing every $n^{\text{th}}$ is faster but incurs a risk of incorrect focus and blur between the re-focusing steps. Modern scanners implement efficient strategies to reduce the scanning time per slide which nowadays ranges from 30 seconds to several minutes. These strategies include improved focus strategies and automatic tissue detection allowing to skip the empty parts of the slide during scanning. Most scanners also allow to load batches of few hundreds of slide at once therefore reducing the time spent by the operator interacting with the machine.

For histology, it is usually sufficient for scanners to scan one focal plane of the slide (\ie that of the tissue slice). For cytology, however, it is not always enough. Indeed, the slide preparation process for a cytology sample differs and the material are not always aligned within a single focal plane. With an optical microscope, it is possible to navigate continuously over the focal planes by adjusting the lenses positions. When it comes to scanning such samples, one has to resort to a technique called "\textit{z-stacking}". It consists in capturing the material at different depths along the $z$-axis. This process can be significantly longer than single focal plane scanning, as multiplies this time by the number of slices of the stack. The result is a finite number of images which can be viewed in different ways (\eg one by one, grid view, video, \etc) but do not offer the simplicity or continuous exploration provided by optical microscope for this task. 

Scanning can also introduce artifacts including stiching problems (\ie misalignment between scanned tiles or lines), blur due to incorrect focus, tissue detection failure causing parts of the slide to be missing from the \acrshort{wsi}. The scanner should obviously remain as clean as possible to avoir external components to pollute the image (\eg dust, glass shards, hair, \etc). 

\subsection{File formats and compression}
\label{ssec:backdp:storingviewing}

For each glass slide, a scanner generates one or more files to store the image data but also any metadata related to the case (information or identifiers of laboratory, patient, specimen, staining, \etc). How the data is organized in such a file is specified by a \textit{file format} of which there exists many. Some file formats are closed and proprietary (\eg scanner- or vendor-specific file formats) but others have open specifications (\eg DICOM, OME-TIFF\TODO{??}). The most involved formats usually combine a descriptive part to store case-related metadata using, for instance, the XML language (\eg in OME-TIFF) and a subfile format for the image itself (\eg TIFF). 

Regarding the internal structure of the latter, the image is usually splitted into a set of tiles (\eg 1024 $\times$ 1024) rather than being stored as a single image array. The file contains metadata to provide efficient access to these tiles. This organization makes sense because, in practice, one rarely has to load the whole image in memory at full resolution at once which, given the size of a raw image, would be impossible on most computers anyway. It is however a common use case for a practitioner to look at an entire slide (or a large region of interest) at a lower magnification and resolution. Downsizing, downsampling and aggregating tiles to obtain a low resolution view of a slide (or a region of interest) is an expensive operation and cannot realistically be performed on-the-fly without significantly increasing loading times. Therefore, a typical image files also stores versions of the image at different zoom levels. The $i^{\text{th}}$ zoom level ($i \in \mathbb{N}_0$) is a version of the image which has been downsized by a factor $2^i$. The number of zoom levels varies depending on the size of the image at full resolution and is such that the image at the lowest zoom level (\ie largest $i$) has a reasonably low size (\eg below $1000 \times 1000$). Simiarly as for zoom level 0, other zoom levels are stored as set of tiles. Such a file is called \textit{pyramidal} because of this structure (see Figure \ref{fig:backdp:pyramidalimage}).

\begin{figure}
  \centering
  \includegraphics[scale=0.7]{backdp/pyramidalimage.jpg}
  \caption{Pyramidal view of a whole-slide image (source: \parencite{marini2022multi_scale_tools})}
  \label{fig:backdp:pyramidalimage}
\end{figure}

A whole-slide image file is not a lightweight one. For example, a raw $10^5 \times 10^5$ RGB image (3 bytes per pixels) contains 30 gigabytes of information. This is massive and not scalable if one has to consider the multitude of slides to be scanned and stored by an hospital for instance. Therefore, compression algorithms are very frequently used to reduce the size of image files. Popular choices are JPEG (lossy compression) and JPEG2000 (lossless compression). Compared to its lossless counterpart, lossy compression allows for better compression rate and disk usage reduction. However, the loss of information results in visual alteration that become more and more severe as the compression rate increases. Therefore, lossy compression should be used carefully to avoid destroying image features relevant to the analysis. 

\subsection{Visualization and hardware considerations}
\label{ssec:backdp:visualization}

An image file is not worth much if it cannot viewed. Nowadays, there exists many open or proprietary software and tools for visualizing \acrshort{wsi}. Some are desktop applications designed to run on personal computer and workstations (\eg QuPath \parencite{bankhead2017qupath}, ASAP \parencite{cpg2022asap}) or directly provided by scanner vendors to run on dedicated hardware alongside the scanner. Some others are web-based (\eg Cytomine \parencite{maree2016collaborative}, OMERO \parencite{allan2012omero}) for which the viewer can be accessed through a web browser and content is served over the HTTP protocol. The most popular tools are not limited to a viewer but usually include many more features. For instance, Cytomine includes image annotation tools, integration of computer vision and \acrlong{ml} algorithms, authentication, project-based content organization, an \acrshort{api} to interact with data programmatically,...

Whereas it is included by-design in web-based tools, remote slide viewing is also possible with some desktop applications. This feature is particulary important in a laboratory environment. Without it, the system would need to download an entire \acrshort{wsi} file before being able to display. This would add a significant latency in the slide reviewing process. Remote slide viewing allows for a seamless and more efficient interaction between the practitioner and the viewing system.

Beyond software, hardware considerations are important for efficient viewing, especially when implemented at the scale of a laboratory or pathology service \parencite{temprana2022digipatics}. Personnal workstations should be equipped with a quality screen able to render image without loss of quality (color and resolution). They also must be equipped with adequate computing resources (memory and \acrshort{cpu}/\acrshort{gpu}) in particular if analysis algorithms are supposed to be executed on them. The network should be robust enough to support the exchange of information (\ie download and upload of large files and data). Servers should have enough disk space to store the digitized slides and data and implement mechanisms in order to minimize the risk of data corruption or loss. 

% \TODO{variations of appearance due to using a tool vs another \parencite{cheng2020evaluating} ??}

\TODO{conclusion on this part}

\section{Whole-slide analysis}
\label{sec:backdp:typicalanalysistasks}

\TODO{experimental histology - response to experimental treatment ?}

When a slide has finally been prepared, pathologists take over and start the analysis, whether on a computer screen or through the lenses of an optical microscope. One of the most frequent condition that has to be evaluated is malignancy of tumors but other diagnosis can also be performed on the basis of pathology slides. The indicators of malignancy can vary greatly from a disease to another and even from one type of cancer to another. The approach of the slide by the pathologist and the analysis can therefore vary accordingly. 

An important tool for diagnosis and prognosis are grading systems. A grading system is a set of mostly objective criteria for quantifying the severity of a disease. These criteria can be quantitative (\eg couting cells, measuring tumor area) or qualitative (\eg evaluating cell or tissue morphology, detect metastases) and consider macroscopic or microscopic elements. Beyond grading, slide analysis can also provide direct information for establishing a proper treatment. For instance, localizing a malignant tumor in its support tissue can be important for guiding a future surgical operation. 

All the types of tasks involved in slide analysis are not equal. They have varying degrees of complexity and tediousness. Some tasks are rather simple but take a significant amount of time (\eg counting mitosis, see Section \ref{ssec:backdp:analysis_quantification}), others require strong focus for an extended period of time which increases the risk of error (\eg thyroid nodule malignancy, see Section \ref{ssec:backdp:ssec:backdp:analysis_localization}). Moreover, some diagnosis require to screen more than just one slide, in which case the volume of information to review can be significant. 

When a task has to be performed frequently and is tedious, error-prone or involves reviewing a large amount of data, there is a great potential for \acrlong{cpath} methods to improve analysis quality and speed. These methods can either provide quantitative or qualitative information that directly contribute to the diagnosis or simply assist the practitioner during the reviewing (\eg image registration, see Section \ref{ssec:backdp:analysis_registration}).


%Evaluating whether a tumor is still in-situ (\ie cancer cells have not spread from the location where they first formed) or proliferative is also paramount for determining a proper treatment as proliferation opens the possibility of metastases.  
% Some analysis require to look at tissues at a more microscopic level. A single cell with a specific morphology in a slide\footnote{The extent of a tissue on a slide can be several orders of magnitude larger than the dimensions of the cells it contains.} can determine the malignant nature of a sample.

localization / classification / quantification

\parencite{abels2019computational}


\subsection{Mitotic count}
\label{ssec:backdp:analysis_mitotic_count}

Mitosis is one of the phases of the cell cycle during which the chromosomes (replicated in a previous phase) are separated into two new nuclei. Tumor often exhibits a larger mitosis rate compared to healthy tissues. Many grading systems therefore use a measure of the mitosis rate as an indicator of cancer severity. For instance, the Bloom-Richardson system \parencite{roychowdhury2022bloomrichardson} for histologic grading of breast cancer includes mitotic count as one of its three indicators. In particular, the mitosis must be counted in 10 field of views of a specified size. The final grade is determined based on a point system. Each indicator provides 1 (least severe) to 3 (most severe) point(s) which are added together (3-5 pts = grade I, 6-7 pts = grade II and 8-9 pts = grade III). 

Counting mitosis is not necessarily complicated but it is cumbersome. It is no surprise that \acrlong{cpath} methods have been investigated to automate the task. In 2014 was organized the MITOS-ATYPIA-14 challenge \parencite{roux2014mitos} where participants had to come up with the best \acrlong{ml} model to count the number of mitosis in different field of views (see Figure \ref{fig:backdp:mitos_atypia}). In addition to the mitotic count, they had to provide predict a global nuclear atypia score for each field of view, nuclear atypia being another indicator of the Bloom-Richardson system. 

\begin{figure}
  \centering
  \subfloat{\includegraphics[height=190pt]{backdp/mitosis_zoomout.png}}
  \hspace{0.5cm}
  \subfloat{\includegraphics[height=190pt]{backdp/mitosis_zoomin.png}}
  \caption{\TODO{todo}}
  \label{fig:backdp:mitos_atypia}
\end{figure}

\subsection{Thyroid nodule malignancy}
\label{ssec:backdp:analysis_thyroid}

\subsection{Camelyon}
\label{ssec:backdp:analysis_camelyon}

% thyroid 
% metastases (in-situ)

% kidney (cell appearance): Formanhn / WHO-ISUP 
% breast (cell appearance): Bloom Richardson 
% prostate (glandular structure): Gleason Score
% whole slide classification

\subsection{Quantification}
\label{ssec:backdp:analysis_quantification}

% mitosis detection
% quantifying the extent of some parts of a tumor

\subsection{Registration}
\label{ssec:backdp:analysis_registration}


\TODO{illustrate with some of my datasets}

\TODO{annotations ?? \parencite{wahab2022semantic}}


\section{Computational pathology and machine learning}
\label{sec:backdp:ml}

Digital pathology has opened the way for the application of \acrlong{ml} to automate analysis and diagnosis. Albeit promising, application of \acrlong{ml} remains challenging for various reasons. In this section, we discuss some of these challenges in Sections \ref{ssec:backdp:dataleakage} and \ref{ssec:backdp:datascarcity} then present some techniques that can be used, if not to overcome them, at least to alleviate them.

\subsection{Data leakage}
\label{ssec:backdp:dataleakage}

In Section \ref{ssec:backml:modelselinpractice}, we introduced the notion of data leakage that occurs when samples in different splits of a dataset are not independent from each other. Data leakage usually results in poor generalization performance when a \acrlong{ml} model is used in production. Obviously, this should be avoided at all cost especially when the prediction of this model impacts the patient diagnosis and treatment. It is worth noting that data leakage is considered an important obstacle for the application of \acrlong{ml} in biology and medicine in general, not only in \acrlong{dp} \parencite{ching2018opportunities}. 

In \acrlong{dp}, data leakage can occur in many, sometimes subtle, ways. Therefore, learning pipelines should be built carefully. One important potential source of data leakage is due to the fact that a whole-slide image does not only convey information about the tissue it contains but also about the slide preparation process (see Section \ref{sec:backdp:wsi}). For instance, staining solutions decay over time. Therefore, the staining intensity can reflect whether the sample has been dipped in a recently-changed or an heavily-used staining bath. When staining is performed manually, the intensity might also reflect the identity of the technician who performed the staining operation. Indeed, some technicians might dip slides for a little longer than others resulting in a stronger intensity. When a dataset is built by different laboratories, the origin of a slide can traceable due to differences in the slide preparation processes between the sites. 

These slide idiosyncracies are harmless as long as they are not correlated with the learning problem target. Whereas it might seem unlikely to happen, some simple though unfortunate choices can lead to correlation. For instance, supposing a problem of malignancy assessment, if one laboratory provides all the healthy samples and another all the malignant samples, there is significant risk that the learning algorithm would exploit the differences resulting from the preparation processes. This would obviously lead to poor generalization when the model will be applied to slide coming from another laboratory for instance. This could occur similarly if malignant and healthy slides are provided by the same laboratory but the former are prepared in the morning and the latter in the afternoon.

The slide preparation process is not the only culprit for data leakage. One other possible source occurs at the patient level. \parencite{bussola2021ai} \TODO{test score worse for patient wise splitting than for tile wise splitting in the article. That's unexpected.} have empirically studied patient-wise and random dataset splitting strategies. They have shown that overfitting and data leakage indeed occurs when splitting samples randomly. 

In general, it is difficult to completely prevent data leakage as one does not always have control over the whole \acrshort{wsi} generation process. However, good practices surely help reducing the problem to an acceptable minimum. A detailed list of guidelines and good practices to reduce the risk of data leakage can be found in \parencite{maree2017need}. This includes collecting data as representative as possible of the different variations that could naturally occur during the generation process. For model evaluation and selection, splitting the dataset into subsets should be performed considering the characteristics of the samples that could correlate with the target (\ie patient, laboratory, technician, staining, equipement, time of the day/week, \etc).

\subsection{Data scarcity}
\label{ssec:backdp:datascarcity}
% and imperfect annotations

As introduced in Section \ref{ssec:backml:sl}, \textit{data scarcity} refers to a context where data is lacking which usually hampers the performance of \acrlong{ml} methods, especially \acrlong{dl}. Data scarcity is often cited as one of the major challenges in \acrlong{dp} \parencite{tizhoosh2018artificial,litjens2017survey,robertson2018digital,komura2018machine}. A misconception would be to consider that data simply does not exist in a sufficient quantity. Indeed, hospitals and research institutes have accumulated a significant amount of data in different formats over the years (\eg imaging, text, \etc). What makes \acrlong{dp} but also the whole field of medical and biological image analysis a data scarce domain is a combination of factors preventing this data to be usable for \acrlong{ml} in a straightforward way.

Many successes in the application of machine and \acrlong{dl} to natural images problems were made possible by the availability of numerous large and exhaustively annotated datasets. For instance, the ImageNet classification dataset features 1000 fine-grained classes for 1.2 million images and has been a key element in \acrlong{dl} innovations since AlexNet. Another example is \acrfirstit{coco} by Microsoft \parencite{lin2014microsoft}, a large-scale dataset for segmentation, detection and captioning (\TODO{present captioning in chapter 1}). It contains more than 200k images annotated with fine segmentation masks over objects from 91 distinct categories (\ie humans, furnitures, animals, \etc). It counts more than 800k unique annotated instances of these categories. Initially published by Google in 2016, the most recent iteration of Open Images Dataset \parencite{kuznetsova2020open} contains approximately 9.2 million images each annotated with one or more labels from 19.8k concepts for a total of 30 million image-level labels. \TODO{figure to provide examples of these dataset}. 

Those were only few examples of the plethora of datasets available in the natural image domain. In \acrlong{dp}, the growing interest for \acrshort{ml}-based solutions has encouraged researchers and practitioners to increasingly share their data and annotations. Although the data scarcity situation is slowly improving, dataset size, versatility and variety are still subpar compared to the natural image domain.

\subsubsection{A mini review of Grand Challenge pathology datasets from 2021}
\label{sssec:backdp:grandchallenge}

\begin{table}
  \centering
  \footnotesize
  \begin{tabular}{|ccc|ccccccc|}
    \hline
    \multicolumn{3}{|c|}{Challenge} & \# \acrshort{wsi} & \# \acrshort{roi} & \acrshort{roi} size & Crowd & AI-assist. & Task & \# targ. \\
    \hline
    \multirow{3}{*}{(1)} & \multirow{3}{*}{TIGER} & tils & 82 & / &  / & yes & yes & REG & $r \in \left[1, 100\right]$\\
    & & rois & 195 & 2032 & $<$ 1.5k $\times$ 1.5k & no & no & SEG & 7\\
    & & bulk & 93 & / & / & no & no & SEG & 2 \\
    \hdashline
    (2) & \multicolumn{2}{c|}{CoNiC} & / & 4981 & 256 $\times$ 256 & no & yes & \acrshort{seg}, \acrshort{clf} & 6 \\
    (3) & \multicolumn{2}{c|}{MIDOG} 2021 & 50 & 200 & $<$ 5k $\times$ 5k & no & yes & \acrshort{det}, \acrshort{cnt} & 2 \\
    (4) & \multicolumn{2}{c|}{BCNB} & 1058 & / & / & no & no & \acrshort{clf} & 16$^{(a)}$ \\
    (5) & \multicolumn{2}{c|}{WSSS4LUAD} & 87 & 10k & $<$ 300 $\times$ 300 & no & no & \acrshort{clf} & 2\\
    (6) & \multicolumn{2}{c|}{BCSS} & / & 151 & $<$ 7k $\times$ 10k & no & no & \acrshort{seg} & 7\\
    (7) & \multicolumn{2}{c|}{NuCLS} & / & 3944 & $<$ 300 $\times$ 300 & yes & yes & \acrshort{seg}, \acrshort{det} & 12 \\
    (8) & \multicolumn{2}{c|}{PAIP 2021} & 150 & / & / & no & no & \acrshort{seg}$^{(b)}$ & 4 \\
    \hline
  \end{tabular}
  \caption{List of challenges published in 2021 with the ``histology'' modality on the Grand Challenge website. The ``Crowd'' and 
  ``AI-assist.'' columns relate to the annotation process and how it was performed. The former indicates whether or not non-pathologists 
  were involved in the process (\ie \textit{yes} for crowdsourcing). The latter indicates whether or not the annotation process involved 
  some kind of AI assistance. The ``\# targ.'' indicates the number of target categories/classes of the underlying \acrlong{ml} problem. 
  References for the challenges: $(1)$ \TODO{\cite{}}, $(2)$ \cite{graham2021conic}, $(3)$ \cite{aubreville2021mitosis}, $(4)$ \cite{xu2021predicting}, 
  $(5)$ \cite{han2021multilayer}, $(6)$ \cite{amgad2019structured}, $(7)$ \cite{amgad2021nucls}, $(8)$ \TODO{\cite{}}. $(a)$ The BCNB challenge 
  proposes 6 differents classification tasks each with up to 4 classes (for a total of 16 classes). $(b)$ The expected output is not a 
  classical segmentation mask but rather the boundary of the structure of interest.}
  \label{tab:backdp:datascarcity-grandchallenge}
\end{table}

In order to illustrate this point, we performed a search on the Grand Challenge website \parencite{grandchallenge}, a popular plateform which runs \acrlong{ml} challenges related to biomedical images, each challenge coming with an open-access dataset. We searched for \textit{histology} challenges published in 2021 and found 8 results (see Table \ref{tab:backdp:datascarcity-grandchallenge} for a brief description of relevant aspects of these datasets).

A first observation is that, whether the dataset is a set of \acrshort{wsi} or \textit{\acrlong{roi}s} (\acrshort{roi}), the number of provided images is several orders of magnitude below the number of images available in natural image datasets. It can be argued that the dimensions of \acrshort{wsi} and \acrshort{roi} images in \acrlong{dp} datasets is significantly larger that natural images (\eg average image size in ImageNet is 469 $\times$ 387 pixels, the largest of the 151 \acrshort{roi} of BCSS reaches approximately 7k $\times$ 10k pixels), however this must be in perspective in relation to the prediction task. For instance, hundreds of \acrshort{wsi}s represent a very large amount of raw data (\eg few terabytes) but, when the target task is whole-slide classification, this only amounts to a hundreds of annotated samples which is significantly fewer compared to ImageNet or others and can be considered a rather small sample size from a \acrshort{ml} perspective.  

Beyond the size of datasets, it is interesting to note that most prediction tasks in \acrlong{dp} only feature few classes (at most 12 in our Grand Challenge sample). It is common to encounter tasks presented as binary (\eg malignant \vs benign) even though it is often a simplification of the underlying biological or medical problem. Datasets with a large variety of classes have shown to be effective for learning efficient models on natural images. Therefore, on this matter, \acrlong{dp} is still lagging behind significantly. To the best of our knowledge, there is no single dataset that features more than 100 classes. \TODO{what is the largest?}

It is also interesting to consider how these datasets were built. Among them, four datasets use internal data acquired and annotated specifically for the challenge (PAIP 2021, MIDOG 2021, BCNB, BCSS), two of them mix both internal and external data (TIGER, WSSS4LUAD) and the last two use exclusively data from external sources (CoNiC, NuCLS). Regarding the use of external data, it either means that external \acrshort{wsi} were imported and annotated for the challenge or that data from other datasets were combined together. For instance, the CoNiC challenge is based on the Lizard dataset \parencite{graham2021lizard} which combines data from the \acrfirstit{tcga} \parencite{weinstein2013cancer}, PanNuke \parencite{gamper2019pannuke}, CRAG \parencite{graham2019mild}, CoNSeP \parencite{graham2019hover}, GlaS \parencite{sirinukunwattana2017gland} and DigestPath \parencite{li2019signet}. The \acrshort{tcga} is an open platform gathering various data related to cancer genomic including a little more than 30k \acrlong{wsi}s, making it one of the largest open database of \acrshort{wsi} to date. It is no surprise that, over the years, many datasets have been built using the \acrshort{tcga} as one of their sources. This also includes WSSS4LUAD, TIGER and NuCLS from our Grand Challenge sample. Interestingly, one of the components of Lizard, PanNuke has also been built from \acrfirst{tcga} and three external datasets of which two were built on top of the same platform (MoNuSeg \parencite{kumar2019multi} and CMP17 \parencite{vu2019methods}). Although, it raises the question of the risk of data leakage, assembling existing datasets to form a larger one certainly helps fighting data scarcity.

\subsubsection{Causes}
\label{sssec:backdp:ds-causes}

The mini-review performed in the previous section was not aimed at being a thorough evaluation of \acrshort{dp} datasets characteristics. It rather serves as a way to highlight different consequences of data scarcity in the field: small dataset size, lack variety and versatility, \etc. The scarcity in \acrlong{dp} is caused by a combination of factors.  

One of the main causes of scarcity is the cost of the annotation process. Pathology is not a simple subject and slide evaluation requires years of training and experience. Whereas classifying pictures into object categories can be done by mostly anyone, creating a ground truth \acrlong{dp} dataset requires trained pathologists for whom time is a precious and expensive resource. Therefore, annotation cost in \acrlong{dp} is significantly higher compared to the natural image domain. This is aggravated by the fact that disagreement between pathologists is not uncommon and quality ground truth usually requires confronting and aggregating annotations by several experts. 


Privacy and ethical concerns also play a role. Indeed, medical data is sensitive and cannot be shared without patient consent, rightfully so, preventing data to be made available for \acrlong{ml}. Privacy might incur additional costs as it might be necessary to keep track where data records have been used, for instance, in the context of the european \acrfirst{gdpr}. Indeed, in case a patient revokes his data sharing agreement, under the right-to-be-forgotten, models might have to be re-trained \parencite{humerick2017taking}. Overall, it can discourage researchers and partictioners to make their data available. 

The use of data-hungry \acrlong{dl} algorithms does not help and is aggravated by the need for a dataset to contain enough samples to account for the variability incurred by the slides content and preparation process. 

% On another front, complexity and duration of the slide preparation process makes it more difficult to produce new \acrshort{wsi} than natural images like pictures which can be captured with any smartphone nowadays. Currently, slide preparation is definitely not a bottleneck as opposed to labeling but could become one if  throughput of quality annotations.

\subsubsection{Potential solutions}
\label{sssec:backdp:ds-solutions}

The causes of data scarcity are numerous and it remains a challenge for \acrlong{dp} but the situation is slowly improving. 

Nowadays, there exists annotation strategies which allow to cut the cost per annotation and therefore produce larger datasets given the same budget. These strategies include crowdsourcing through citizen science \parencite{peplow2016citizen} or the intervention of medical students (\eg NuCLS dataset). These approaches obviously require either supervision by pathologists or more annotations per image to average out the inaccuracies (or even both) but these are in general less expensive than direct annotation by trained pathologists. It is also possible indirectly reduce the annotation cost by reducing the time spent per annotation. One way of achieving that is to use of AI to assist experts and accelerate the annotation process (see NuCLS, CoNiC, MIDOG 2021 from our Grand Challenge sample) \parencite{chai2020human}. An algorithm could for instance suggest cell boundaries with a weak algorithm. The annotator could then correct the boundaries if necessary which is less time-consuming then drawing the boundary from scratch. The use of dedicated and intuitive user interface with efficient drawing tools can also help in that regard.  

If annotating new data is not an option, it is also possible to combat data scarcity by the use of existing external data and proper computational methods. There exists several open resources for \acrlong{dp} \parencite{maree2019open} providing access to slides and annotations. We have already introduced \acrshort{tcga} which provides access to a spectacular amount of 30k \acrshort{wsi}. The Camelyon dataset \parencite{litjens2018camelyon} contains 1399 \acrshort{wsi} of which 209 were annotated at full resolution with detailed hand-drawn contours of all metastases. This is one of the largest most-precisely annotated dataset in the field to date. The Lizard dataset (subject of the CoNiC challenge) contains almost 500k segmented nuclei all labelled into one of 6 classes. Some promising projects are also ongoing such as Bigpicture \parencite{moulin2021imi} of which the goals are to ``\textit{create the first European, ethically compliant, and quality-controlled whole slide imaging platform, in which both large-scale data and AI algorithms will exist}'' and have the same impact on pathology as ImageNet had on the natural image domain. 

Regarding the computational methods, there exists machine and \acrlong{dl} algorithms which can make use of external data. We explore some of them in the thesis and present related works in Section \ref{sec:backdp:compmethods}. Sometimes the lack of data is such that the dataset is not representative enough of the different variations that could occur in a realistic context. The impact of this issue can be alleviated during the training process directly with an ad-hoc data augmentation procedure that would automatically generate the kind of variations that can be expected (\eg staining intensity, deformation, \etc). Data normalization during pre-processing can also help to reduce variability. \TODO{present data aug techniques???}

% AI-assisted annotation and crowdsourcing improve significantly the

\TODO{class imbalance, label noise}
% Data scarcity can also appear within a problem where annotations and images are plentiful. Indeed,  

\section{Computational methods and related works}  
\label{sec:backdp:compmethods}

In this section, we explore how \acrlong{tl}, \acrlong{mtl} and self-training (see Sections \ref{ssec:backdp:tl}, \ref{ssec:backdp:mtl} and \ref{ssec:backdp:st} respectively) were applied to medical and pathology tasks. These three families of techniques are popular for tackling data scarcity \parencite{litjens2017survey,van2019strategies} and we have applied them in the contributions of this thesis.


\subsection{Transfer learning}
\label{ssec:backdp:tl}

General purpose transfer learning was discussed in Sections \ref{ssec:backml:transfer} and \ref{ssec:backml:dl:deeptransfer}. In this section, we explore how deep transfer learning was applied to biomedical images. Some of the first applications were reported in \parencite{bar2015chest,ciompi2015automatic,van2015off} for pulmonary nodule detection in chest x-rays and \acrshort{ct}-scans using Decaf and OverFeat (feature extractors based on ImageNet and AlexNet) as feature extractors. While those works have revealed the potential of deep transfer learning in that field, the performances were not significantly better than those of previous methods. \parencite{ravishankar2016understanding} compared the performance of a pre-trained AlexNet-based CaffeNet \parencite{jia2014caffe} to well-established computer vision like \acrfirstit{hog} \parencite{mcconnell1986method} and found transfer to outperform these features.

Later, as new ImageNet architectures emerged, their transfer potential from ImageNet was also evaluated on various biomedical imaging tasks. \parencite{antony2016quantifying} evaluated the use fine-tuning and feature extraction of different archictures including VGG16 for quantifying knee osteoarthritis severity on radiographs. \parencite{kieffer2017convolutional} compared training from scratch to both feature extraction and fine-tuning for classification and retrieval of digital patholog images using InceptionV3 and VGG16. \parencite{shin2016deep} studies the interest of transfer of different architectures including GoogLeNet and AlexNet for thoracoabdominal lymph node detection and interstitial lung disease classification. 

As introduced earlier, some works have shown that transfer learning is likely to provide better performance when the source and target tasks are close \parencite{yosinski2014transferable} or if the source task includes the target domain \parencite{mensink2021factors}. This implies that pre-training models on pathology data directly is a sound approach although it requires a significant amount of source data to make a proper source task. This was confirmed in several contributions.

For instance, \parencite{khan2019improving} pre-train an InceptionV3 network on a custom dataset generated from Camelyon16 and then transfer the resulting model to a prostate cancer classification task. They show that their pre-trained model outperforms both training from scratch and using an ImageNet pre-trained model. \parencite{medela2019few} also make use of transfer learning between two digital pathology tasks. Follow a self-supervised pre-training approach rather than using a classical supervised approach. They train a siamese network to distinguish different parts of colorectal tissues. The network is then transferred as a feature extractor on the target task (tumour classification). \parencite{shang2019and} use several datasets (including some unrelated to their target task such as \textit{Dogs vs. cats}) and compare ImageNet and domain-specific pre-training in order to tackle colonoscopy image classification. They also show that pre-training on domain-specific data yield superior performance compared to using ImageNet. \parencite{kraus2017automated} train a custom deep neural architecture, DeepLoc, for classifying protein subcellular localization in budding yeast. Then, they assess the transferability of their pre-trained DeepLoc by fine-tuning it on different image sets including unseen classes and show that the pre-training is indeed beneficial.




% More recently, fine-tuning has been also investigated and compared with networks trained from scratch and classifiers trained on off-the-shelf features on a wide variety of biomedical imaging tasks \parencite{antony2016quantifying,esteva2017dermatologist,gulshan2016development, ravishankar2016understanding,shin2016deep,tajbakhsh2016convolutional}. More specifically, digital pathology and microscopy are not outdone with works on tissue texture \parencite{kieffer2017convolutional}, cell nuclei \parencite{bayramoglu2016transfer} and breast cancer \parencite{han2017breast} classification from histopathology images or analysis of high-content microscopy data \parencite{kraus2017automated}. Most works have used networks pre-trained on the ImageNet image classification dataset. Others, like \parencite{kraus2017automated}, have used a custom network architecture pre-trained on a medical dataset as source task. 

% While it is clear that training from scratch very deep networks is not viable in most case due to data scarcity \parencite{bayramoglu2016transfer,tajbakhsh2016convolutional}, there is currently no consensus and best practice about how transfer learning should be applied to digital pathology and microscopy. Some recent publications in the biomedical field have shown that fine-tuning outperforms off-the-shelf features \parencite{,,,}. However, as noted in \parencite{litjens2017survey}, experiments in these papers are often carried out on a single dataset, which does not allow to draw general conclusions. Moreover, those experiments do not use current state-of-the-art networks. A short review of the networks used in biomedical imaging is given in Supplementary Table 1 and shows that the more recent and efficient residual and densely connected networks have been underused or not used at all, in particular in digital pathology and microscopy.

% Transfer learning is not a recent field of research \parencite{pan2010survey} but has grown in popularity with deep learning as it has been shown that features learned on a source task by a neural network could be transferred to a possibly unrelated target task \parencite{sermanet2013overfeat, razavian2014cnn, yosinski2014transferable}. The success of this approach is mostly due to the possibility to use the large and versatile ImageNet \parencite{deng2009imagenet} dataset as a source task \parencite{kornblith2019better}. Medical imaging and digital pathology communities have therefore studied and used transfer learning \parencite{tajbakhsh2016convolutional, shin2016deep, mormont2018comparison, babaie2019tissuefold, ponzio2019dealing} as it provides a way of coping with data scarcity. Those works have explored and evaluated different transfer techniques mostly using ImageNet as a source task. The current consensus is that transfer is helpful in most cases and should be considered when tackling a new task. More recently, several works have focused on transferring a model pre-trained on medical, biology or digital pathology datasets. This is motivated by the fact that one can expect better performance from transfer learning when target and source task are close or related \parencite{yosinski2014transferable}. 

% Independently, multi-task learning \parencite{zhang2017survey} has been applied with great successes for a wide-range of application. The success of MTL is notably due to the fact that leveraging several tasks and/or datasets alleviates the need for large amounts of data. Moreover, training in multi-task has regularization effect preventing the model to overfit a particular task therefore yielding a better generalizing model. The modularity of neural networks also allows to embed multi-task specific components hence facilitating its application to deep learning \parencite{caruana1997multitask, zhang2017survey}. There are many ways how MTL can be implemented within deep learning with, for instance, architecture tricks \parencite{misra2016cross, strezoski2019many} or weight sharing \parencite{caruana1997multitask}. 

% Our work lies at the crossroad of multi-task and transfer learning and differs from the previously presented works mostly on the objective. Indeed, we do not use MTL nor transfer learning for solving a specific task but rather to pre-train a versatile network to be transferred to new tasks. 

\subsection{Multi-task learning}
\label{ssec:backdp:mtl}

Multi-task learning has been applied to medical imaging. Samala \etal \parencite{samala2017multi} jointly train a classifier on three mammography image datasets (digitized screen-film and digital mammograms) and compare it to single-task training and transfer learning. They show that multi-task trained network generalizes better than the single-task one. Zhang \etal \parencite{zhang2016deep} use transfer and multi-task learning to derive image features from Drosophila gene expression. MTL has also been applied more specifically to digital pathology. Pan \etal \parencite{pan2018multi} apply MTL for breast cancer classification by using a classification loss and a verification loss. The role of the latter is to ensure that features produced by the network differ for images of different classes. Arvaniti \etal \parencite{arvaniti2018coupling} use both weak and strong supervision at once to classify prostate cancer. Shang \etal (mentioned earlier) also evaluate multi-task learning which is the best performing approach on their target task. However, they suggest that more experiments would have to be carried out to assess whether their conclusions are generalizable.

\subsection{Self-training and weakly supervised learning}
\label{ssec:backdp:st}

We have introduced self-training in Sections \ref{ssec:backml:inbetween} and \ref{ssec:backml:dl:selftraining}. It is not surprising that self-training has also been applied to medical image tasks to combat data scarcity \parencite{tajbakhsh2020embracing, peng2021medical} and to digital pathology in particular. Most work in this domain currently treat with image classification \parencite{peikari2018cluster, su2019local, koohbanani2021self, jaiswal2019semi, shaw2020teacher} but detection and segmentation have also been explored. \parencite{li2018based} combine weakly-supervised learning and self-training to predict per-pixel Gleason score across entire WSIs. \parencite{li2019signet} build a signet ring cell detector using self-training. To mitigate the impact of erroneous pseudo-label, their approach features a cooperative training step where two models are trained on the pseudo-labels generated by one another during the previous round. Self-training has also been applied to segmentation for other image modalities. To segment cardiac MR images, \parencite{bai2017semi} propose a self-training approach to train a segmentation architecture. In particular, the teacher and student are the same model and the student is not reset between training rounds. Additionally, they apply a \acrfirstit{crf} to refine the model predictions on the unlabeled images. \parencite{fan2020inf} focus on lung infection segmentation in the context of the COVID-19 pandemic. Their approach features a self-training protocol where at every training round, they pseudo-label $K$ new unlabeled images which will be added to the learning set for the next round. 

Semi-supervised literature also concerns the use of imperfect data which relates more to \acrlong{wsl}. This approach has also be explored for biomedical tasks. \parencite{wolny2021sparse} learn from sparse instance segmentation masks using their sparse single object loss which combines an instance-based loss and an embedding consistency loss. They also evaluate their method on two microscopy images datasets. \parencite{bokhorst2018learning} segment tissues from colorectal cancer patients into 13 classes representing different types of tissues. Their dataset is composed of both exhaustively- and sparsely-labeled images. During training, they apply a weight map to tune and balance the contribution of individual pixels to the loss. The masks ignore unannotated pixels.
