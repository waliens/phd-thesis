\chapter{Digital pathology}
\label{chap:backdp}

\begin{overview}{Overview}
  The goal of this chapter is to provide \acrlong{dp} background and keys to understand our contributions. We will focus our attention on topics relevant to this thesis. 
  
  Section \ref{sec:backdp:whatisdp} introduces and defines medical terms such as \textit{pathology}, defines \acrfirstit{dp} and introduces the notion of \acrfirstit{wsi}. Section \ref{sec:backdp:wsi} presents the journey of a sample from the body to the \acrshort{wsi}, introducing the different sources of variability introduced by the whole conversion process. 
\end{overview}

% analogintelligence.com image dp illustration

\section{What is \acrlong{dp}?}
\label{sec:backdp:whatisdp}

Nowadays, medicine and healthcare rely heavily on analysis of human body samples to study and diagnose diseases. The branch of medicine focusing on this analysis is called \textit{pathology} which includes histology-based pathology (\aka histopathology) and cytology-based pathology (\aka cytopathology). Both of these sub-branches involve the study of microscope glass slides containing samples (see Figure \ref{fig:backdp:glassslides}). On the one hand, histology samples are tissue sections cut from a bodily specimen. Cytology, on the other hand, is concerned with samples of free cells or tissue fragments which can be extracted by different techniques. \TODO{experimental pathology ?}

\begin{figure}
  \centering
  \subfloat[Glass slide]{\includegraphics[scale=0.35]{backdp/microscope-slide.png}}\quad
  \subfloat[Whole-slide image]{\includegraphics[scale=0.22]{backdp/wsi.png}}
  \caption{Microscope slides with tissue samples (images from \parencite{img:glassslides} and \TODO{reference demo cytomine}).}
  \label{fig:backdp:glassslides}
\end{figure}

The trend of digitalization affecting our societies also impacts pathology as, using dedicated scanners, these glass slides can now be digitized into large image files called \acrfirstit{wsi}. These files associated with patient metadata are stored in computer systems commonly called \acrfirstit{pacs}. In this context, \acrfirstit{dp} can be defined as ``\textit{the acquisition, management, sharing and interpretation of pathology information — including slides and data — in a digital environment}'' \parencite{doolan2019whatisdp}. Working with \acrshort{wsi} instead of physical slides has several advantages and drawbacks (see Table 1 in \parencite{jahn2020digital} for a thorough list). Aside from easier sharing and storing of slides, digitization also opens the way for automated analysis sofware to automatically extract relevant information, typically with the help of \acrlong{ai} and \acrlong{dl}. The branch of \acrlong{dp} interested in such analysis is called \acrfirstit{cpath} and holds great promises for the future. Indeed, \acrshort{cpath} techniques have the potential to relieve pathologists from easy but time-consuming tasks allowing them to focus on challenging cases and research therefore reducing healthcare cost and improving diagnosis quality. On a larger scale, they hold promises for increased coverage and quality of healthcare around the world and especially in low incomes countries where the number of pathologists per inhabitant is typically insufficient. According to the \acrshort{who} cancer report \parencite{world2020report}, the ratio of pathologist per inhabitant was approximately 1 per 15000 in high income countries in 2020 but dramatically dropped to 1 per million or less in many low income countries. 

Interestingly, although digitization technologies are quite mature, adoption of \acrlong{dp} in healthcare facilities is not simple. Many still heavily rely on glass slides for day-to-day operations. Indeed, the transformation requires to modernize the whole hardware (scanners, workstation) and software (slide viewers, information system) infrastructure and to re-think entirely the processes of the facility \parencite{temprana2022digipatics}. This obviously requires significant investments both in time and money and careful planning to carry it out successfully which is not always comptabile with the workload of pathology services or research laboratories. Other difficulties might arise, slowing down the transformation, such as reluctance to change and lack of confidence in modern tools for slide visualization and analysis. 
%Moreover, some tasks cannot be performed easily on \acrlong{wsi}s but can on a glass slide (\eg exploring the depth of a sample in cytology by changing the focal plane). 

As far as automated analysis is concerned, it remains quite a challenge. Whole-slide images typically contain several gigapixels and cannot be loaded entirely in a typical computer memory at full resolution. Moreover, for most tasks, the image content is complex and classical computer vision methods (\eg thresholding) would often fail to distinguish structures of interest. This complexity is aggravated by the presence of artifacts \parencite{taqi2018review} appearing during the conversion process of a bodily specimen to an image (see Section \ref{sec:backdp:wsi}). The ability of learning techniques to train models that capture complex relationships in data makes \acrlong{ml} an ideal candidate to tackle \acrlong{dp} tasks. However, data scarcity is a prevalent issue in the field as quality data, especially annotated, can be difficult to obtain for various reasons: privacy concerns, time-consuming and expensive nature of the annotation process, \etc.     

Overall, \acrlong{dp} holds great promises but presents significant and interesting challenges on several fronts. This thesis focuses on the \acrshort{ml}-based automated analysis aspects of \acrlong{dp} and studies how to tackle data scarcity.

\section{A journey from the body to the computer}
\label{sec:backdp:wsi}

Turning a bodily specimen into \acrlong{wsi}s is a long and complex multi-step process typically involving the work of several highly-specialized technicians. Some steps can nowadays be automated but the chain remains mostly manual. In this section, we describe the different steps of this procedure (see \parencite{mccann2014automated} for an alternate presentation with illustrations). For the sake of brevity, this description focuses on histology with a tissue section prepared for brightfield microscopy and scanning, brightfield being one of the most common modality used in histo- and cytopathology. Sample preparation can differ more or less dependending on the nature of the sample (\eg histology, cytology, hematology) or target imaging technique (\eg brightfield, fluoresence, multispectral) but it is out of the scope this thesis to discuss these differences. 

During the description, we will discuss some of the possible visual and/or physical alterations of the sample resulting from a specific preparation step. The presence of these alterations, or artifacts, is one of the reason why processing \acrshort{wsi} manually or automatically can be challenging. Indeed, these alterations can at worst prevent any meaningful analysis by hiding, destroying or changing the appearance of the structures of interest. Our presentation of alterations and artifacts will not be exhaustive. A more thorough list of pre-scan artifacts with illustrations can be found in \parencite{taqi2018review}. 

\subsection{Samples collection, fixation, cutting and dehydratation}

Whereas we focus mostly on histo- and cyto-pathology in this thesis, a pathology service typically deals with more than these two modalities. The specimens they receive for analysis range from small (commonly referred to as \textit{biopsies}) to very large (whole organ or body). Whole bodies are usually destined for autopsies which might themselves generate biopsies. Smaller speciemens (organs, biopsies) must be fixated before going through the next steps. 

The goal of fixation is to put a stop to the natural decay of the specimen and increase its structural stability \parencite{rolls2012process}. This can be achieved, for instance, by immersing the specimen in a formaldehyde bath (\ie the fixative solution) for period of time depending on its size (\ie few hours to a whole day). 

When the specimen has been fixated, it is then placed into a standardized container called a \textit{cassette} (see Figure \ref{fig:backdp:cassette}). If the sample is too large for the cassette, a volume of interest is cut from the specimen. Depending on the later examination, the orientation of the cut can be crucial to exhibit relevant tissue structures of the specimen. In the remaininder, we will call a \textit{sample} the content of this cassette.

For a proper analysis, the cellular morphology of the sample must be preserved. This is most commonly achieved by infiltrating the tissue with paraffin wax. Infiltration however does not work on a raw fixated tissue because paraffin is hydrophobic. Therefore, one must first perform \textit{dehydratation}, that is, to replace water naturally present in the sample with a product miscible with parrafin. This is done by first plunging the sample into a succession of alcoholic solutions baths. Although this process achieve dehydratation, alcohol does not mix with paraffin neither. Therefore, the sample is then plunged into one or more xylene-based solutions baths, xylene being miscible with both alcohol and paraffin. The sample, infiltrated with xylene, is finally plunged in a paraffin bath under vacuum \TODO{how does it work the vaccum stuff???}. The dehydratation process takes few hours and is often automated using dedicated machines.  

Artifacts can appear as early as at extraction of the specimen and even before. Each of the specimen and sample processing steps presented in this section can also introduce artifacts. Tissue can be damaged at specimen extraction site by the use of medical tools or treatment. Bad fixation can lead to decaying tissue (\ie autolysis) and structural degradation (\eg tissue shrinkage). Improper cutting can also cause tissue damage like tearing, squeezing and burning. Dehydratation has also its own set of possible artifacts. Improper dehydratation can leave some parts of the sample with remaining water, alcholol or xylene which. Tissues can also be exposed to the different solutions for an excessive duration. These processing errors can for instance cause tearing, shrinkage, interference with the staining process (see Section \ref{ssec:backdp:staining}) and affect the structural properties of the tissue (\eg the tissue becomes brittle). Automation obviously reduces the risk of mistakes and artifacts.

\TODO{big figure illustrating ``all'' possible artifactsand variations}
\TODO{illustrate process}

% \begin{figure}
%   \centering
%   \subfloat[This is an example of well-fixed tissue showing good nuclear and cytoplasmic morphology with minimal shrinkage showing clearly defined basement membranes and cell margins.]{\includegraphics[scale=1.0]{backdp/fixationgood.png}\label{fig:backdp:goodfixation}}\quad
  
%   \subfloat[This is an example of poorly-fixed tissue showing inferior nuclear and cytoplasmic morphology with excessive shrinkage and poorly defined cell margins.]{\includegraphics[scale=1.0]{backdp/fixationbad.jpg}\label{fig:backdp:badfixation}}
    
%   \caption{Examples of good and bad fixations (images and sub-captions from \parencite{rolls2012process}).}
%   \label{fig:backdp:fixation}
% \end{figure}

\begin{figure}
  \centering
  \subfloat[Cassette with fixated samples (image \\ from \parencite{stidworthy2011getting})]{\includegraphics[scale=0.75]{backdp/cassette.jpg}\label{fig:backdp:cassette}}\quad
  \subfloat[Cassette with parrafin-embedded \\ samples (image from \parencite{img:cassetteparrafin})]{\includegraphics[scale=0.42]{backdp/cassette-parrafin.jpg}\label{fig:backdp:cassette-parrafin}}
  \caption{Tissue cassettes.}
\end{figure}

\subsection{Embedding, microtomy and glass-slide application}
\label{ssec:backdp:embedding}

At this point, the sample in the cassette has been infiltrated with paraffin. The next step consists in embedding the infiltrated sample in a block of paraffin to allow easier cutting. The sample is placed in a small container attached to the back of the cassette. This container will serve as a mold for casting the block of parrafin (see Figure \ref{fig:backdp:cassette-parrafin}). When the block has solidified, the sample can now be cut into thin slices to be applied on the glass slides. Cutting is performed with a dedicated tool called a \textit{microtome} (see Figure \TODO{illus microtome}). Operated by a technician, the microtome allows slices to be cut to an extremely small and precise thickness of around 3 or 4 $\mu m$. The cutted slices are then floated onto a water bath which helps mouting them on glass-slides. 

These steps should be performed carefully not to introduce artifacts. For instance, a warm parrafin block or a dull microtome blade can cause compression artifacts (\ie tissue displacement causing material accumulation). Another possible source of artifact is contamination of the water bath with previous samples, hair, dust which can in turn contaminate the floating slices.   

\subsection{Staining}
\label{ssec:backdp:staining}
At this point, mounted tissue slices are almost completely transparent which would prevent any meaningful analysis. They must therefore be stained to highlight structures of interest. Similarly as for dehydratation, this process consists in bathing the slide into a succession of stainning solutions. The nature of these solutions will depend on the content that should be highlighted for the future analysis. The most common and standard staining in histology is called \acrfirstit{heeo}. Hematoxylin stains nucleic acids in a deep blue-purple color (typically cell nuclei) and eosin nonspecifically stains proteins in a pink color (typically extracellular matrix and cytoplasm). The \acrshort{heeo} stain, although most common, is not the only available. There exists many other staining techniques such as \acrfirstit{ihc} which exploits the binding nature of some antibodies with specific proteins. Markers can then be used to highlight the antibodies, hence the proteins of interest they have bound with. An example of a tissue stained with \acrshort{heeo} and \acrshort{ihc} is given in Figure \ref{fig:backdp:heeo-ihc}. When the sample has been stained and cleaned from remaining excess of staining solutions, one must apply a cover slip on the sample in order to ensure that sample lies in one single plane as the focal plane of microscopes and scanners is usually quite narrow. The cover slip also protects the sample from external contamination and degradation. 

The choice of a staining and its clean application are crucial for an efficient analysis. The staining baths can become contaminated with samples, by-products resulting from chemical reactions (\eg precipitation or crystallization of chemical components resulting in the presence of pigments in the sample) or external objects (\eg hair, dust). The baths usually degrade over time and use which can cause variation in the staining intensities between earlier and later samples. The bath duration is important for proper staining and bathing samples for less or more time than recommanded can respective*ly cause under- or over-staining. Moreover, insufficient cleaning after staining can leave spots of stain on the slide. Improper application of the cover slip can for instance cause the presence of air bubbles. Nowadays, the staining process can be automated with dedicated machine reducing the variability of the process. 

\begin{figure}
  \centering
  %\includegraphics{backdp/heeo-ihc.jpg}
  \caption{\TODO{describe} Image from \parencite{litjens2018camelyon}.}
  \label{fig:backdp:heeo-ihc}
\end{figure}

\subsection{Scanning}
\label{ssec:backdp:scanning}

A glass slide coming out of the staining process described in Section \ref{ssec:backdp:staining} is ready to be analysed with an optical microscope but can also be digitized by a slide scanner (see Figure \TODO{illustrate a scanner}) into a \acrshort{wsi}. As briefly explained in Section \ref{sec:backdp:whatisdp}, the use of digital slide offers new ways to interact with a sample and opens the way for the use of new analysis tools.

Scanners are equipped with high-precision lenses and sensors allowing them to generate very high-resolution images required for a proper analysis. A scanner typically scans images at the standard $\times$20 or $\times$40 magnifications (\ie resulting samples appear 20 or 40 times larger than they actually are) which are often enough for routine analysis of \acrshort{heeo} or \acrshort{ihc} slides \parencite{zarella2019practical}. Higher magnification are also available with dedicated scanners if required for the analysis. Given the need for such a magnification and resolution, it is not possible to capture the whole slide in a single shot with currently available sensors. Therefore, scanners typically capture a sample step by step either tile by tile or in a in-line fashion (see Figure \TODO{tile-line-scanning.jpg} from \parencite{jahn2020digital}) and then assemble together the different parts using a stitching algorithm. Obviously, as a slide is scanned in several shots, the scanner must ensure that focus is correct for all the shots in order to avoid blur. Common focus strategies are for instance re-focusing every tile, or every $n^{\text{th}}$ tile. When it comes to the focus strategy, there is usually a trade-off between scanning time and focus precision: focusing every tile allows for ideal focus but takes time, whereas focusing every $n^{\text{th}}$ is faster but incurs a risk of incorrect focus and blur between the re-focusing steps. Thanks to different tricks, scanning time per slide nowadays ranges from 30 seconds to several minutes. These tricks include improved focus strategies and automatic tissue detection allowing to skip the empty parts of the slide during scanning. Most scanners also allow to load batches of few hundreds of slide at once therefore reducing the time spent interacting with the machine.

Scanning can also introduce artifacts including stiching problems (\ie misalignment between scanned tiles or lines), blur due to incorrect focus, tissue detection failure causing parts of the slide to be missing from the \acrshort{wsi}. The scanner should obviously remain as clean as possible to avoir external components to pollute the image (\eg dust, glass shards, hair, \etc). 

\section{Typical diagnosis tasks}
\label{sec:backdp:typicaldiagnosistasks}

When a slide has finally been prepared, pathologists take over and can start the analysis, whether on a computer screen or through the lenses of an optical microscope. One of the most frequent condition that has to be evaluated is malignancy of tumors but other diagnosis can also be performed on the basis of pathology slides. The indicators of malignancy can vary greatly from a disease to another and even from one type of cancer to another. The approach of the slide by the pathologist and the analysis can therefore vary accordingly. Some analysis can be qualitative but some diagnosis require grading the severity of the disease which is usually based on a quantitive evaluation of some indicators. 

Sometimes diagnosis require to evaluate tissues appearance, shape or localization at a macroscopic level\TODO{illustrate with images and examples}. For instance, localizing a malignant tumor in its support tissue can be important for guiding a future surgical operation \TODO{double check}. Measuring the size of a tumor (by measuring its surface on the slide) is useful for grading cancer. Evaluating whether a tumor still in-situ (\ie cancer cells have not spread from the location where they first formed) or proliferative is also paramount for determining a propre treatment as proliferation opens the possibility of metastases.  

Some analysis require to look at tissues at a more microscopic level. A single cell with a specific morphology in a slide\footnote{The extent of a tissue on a slide can be several orders of magnitude larger than the dimensions of the cells it contains.} can determine the malignant nature of a sample. A common, relatively simple yet time-consuming task is counting the number of mitosis (\ie cell division \TODO{figure}) in a tissue which is required for grading some cancer.


\TODO{illustrate with some of my datasets}


\section{Computational pathology and machine learning}
\label{sec:backdp:ml}

Digital pathology has opened the way for the application of \acrlong{ml} to automate analysis and diagnosis. Albeit promising, application of \acrlong{ml} remains challenging for various reasons. In this section, we discuss some of these challenges in Sections \ref{ssec:backdp:dataleakage} and \ref{ssec:backdp:datascarcity} then present some techniques that can be used, if not to overcome them, at least to alleviate them.

\subsection{Data leakage}
\label{ssec:backdp:dataleakage}

In Section \ref{ssec:backml:modelselinpractice}, we introduced the notion of data leakage that occurs when samples in different splits of a dataset are not independent from each other. Data leakage usually results in poor generalization performance when a \acrlong{ml} model is used in production. Obviously, this should be avoided at all cost especially when the prediction of this model impacts the patient diagnosis and treatment. It is worth noting that data leakage is considered an important obstacle for the application of \acrlong{ml} in biology and medicine in general, not only in \acrlong{dp} \parencite{ching2018opportunities}. 

In \acrlong{dp}, data leakage can occur in many, sometimes subtle, ways. Therefore, learning pipelines should be built carefully. One important potential source of data leakage is due to the fact that a whole-slide image does not only convey information about the tissue it contains but also about the slide preparation process (see Section \ref{sec:backdp:wsi}). For instance, staining solutions decay over time. Therefore, the staining intensity can reflect whether the sample has been dipped in a recently-changed or an heavily-used staining bath. When staining is performed manually, the intensity might also reflect the identity of the technician who performed the staining operation. Indeed, some technicians might dip slides for a little longer than others resulting in a stronger intensity. When a dataset is built by different laboratories, the origin of a slide can traceable due to differences in the slide preparation processes between the sites. 

These slide idiosyncracies are harmless as long as they are not correlated with the learning problem target. Whereas it might seem unlikely to happen, some simple though unfortunate choices can lead to correlation. For instance, supposing a problem of malignancy assessment, if one laboratory provides all the healthy samples and another all the malignant samples, there is significant risk that the learning algorithm would exploit the differences resulting from the preparation processes. This would obviously lead to poor generalization when the model will be applied to slide coming from another laboratory for instance. This could occur similarly if malignant and healthy slides are provided by the same laboratory but the former are prepared in the morning and the latter in the afternoon.

The slide preparation process is not the only culprit for data leakage. One other possible source occurs at the patient level. \parencite{bussola2021ai} \TODO{test score worse for patient wise splitting than for tile wise splitting in the article. That's unexpected.} have empirically studied patient-wise and random dataset splitting strategies. They have shown that overfitting and data leakage indeed occurs when splitting samples randomly. 

In general, it is difficult to completely prevent data leakage as one does not always have control over the whole \acrshort{wsi} generation process. However, good practices surely help reducing the problem to an acceptable minimum. A detailed list of guidelines and good practices to reduce the risk of data leakage can be found in \parencite{maree2017need}. This includes collecting data as representative as possible of the different variations that could naturally occur during the generation process. For model evaluation and selection, splitting the dataset into subsets should be performed considering the characteristics of the samples that could correlate with the target (\ie patient, laboratory, technician, staining, equipement, time of the day/week, \etc).

\subsection{Data scarcity}
\label{ssec:backdp:datascarcity}
% and imperfect annotations

As introduced in Section \ref{ssec:backml:sl}, \textit{data scarcity} refers to a context where data is lacking which usually hampers the performance of \acrlong{ml} methods, especially \acrlong{dl}. Data scarcity is often cited as one of the major challenges in \acrlong{dp} \parencite{tizhoosh2018artificial,litjens2017survey,robertson2018digital,komura2018machine}. A misconception would be to consider that data simply does not exist in a sufficient quantity. Indeed, hospitals and research institutes have accumulated a significant amount of data in different formats over the years (\eg imaging, text, \etc). What makes \acrlong{dp} but also the whole field of medical and biological image analysis a data scarce domain is a combination of factors preventing this data to be usable for \acrlong{ml} in a straightforward way.

Many successes in the application of machine and \acrlong{dl} to natural images problems were made possible by the availability of numerous large and exhaustively annotated datasets. For instance, the ImageNet classification dataset features 1000 fine-grained classes for 1.2 million images and has been a key element in \acrlong{dl} innovations since AlexNet. Another example is \acrfirstit{coco} by Microsoft \parencite{lin2014microsoft}, a large-scale dataset for segmentation, detection and captioning (\TODO{present captioning in chapter 1}). It contains more than 200k images annotated with fine segmentation masks over objects from 91 distinct categories (\ie humans, furnitures, animals, \etc). It counts more than 800k unique annotated instances of these categories. Initially published by Google in 2016, the most recent iteration of Open Images Dataset \parencite{kuznetsova2020open} contains approximately 9.2 million images each annotated with one or more labels from 19.8k concepts for a total of 30 million image-level labels. \TODO{figure to provide examples of these dataset}. 

Those were only few examples of the plethora of datasets available in the natural image domain. In \acrlong{dp}, the growing interest for \acrshort{ml}-based solutions has encouraged researchers and practitioners to increasingly share their data and annotations. Although the data scarcity situation is slowly improving, dataset size, versatility and variety are still subpar compared to the natural image domain.

\subsubsection{A mini review of Grand Challenge pathology datasets from 2021}
\label{sssec:backdp:grandchallenge}

\begin{table}
  \centering
  \footnotesize
  \begin{tabular}{|ccc|ccccccc|}
    \hline
    \multicolumn{3}{|c|}{Challenge} & \# \acrshort{wsi} & \# \acrshort{roi} & \acrshort{roi} size & Crowd & AI-assist. & Task & \# targ. \\
    \hline
    \multirow{3}{*}{(1)} & \multirow{3}{*}{TIGER} & tils & 82 & / &  / & yes & yes & REG & $r \in \left[1, 100\right]$\\
    & & rois & 195 & 2032 & $<$ 1.5k $\times$ 1.5k & no & no & SEG & 7\\
    & & bulk & 93 & / & / & no & no & SEG & 2 \\
    \hdashline
    (2) & \multicolumn{2}{c|}{CoNiC} & / & 4981 & 256 $\times$ 256 & no & yes & \acrshort{seg}, \acrshort{clf} & 6 \\
    (3) & \multicolumn{2}{c|}{MIDOG} 2021 & 50 & 200 & $<$ 5k $\times$ 5k & no & yes & \acrshort{det}, \acrshort{cnt} & 2 \\
    (4) & \multicolumn{2}{c|}{BCNB} & 1058 & / & / & no & no & \acrshort{clf} & 16$^{(a)}$ \\
    (5) & \multicolumn{2}{c|}{WSSS4LUAD} & 87 & 10k & $<$ 300 $\times$ 300 & no & no & \acrshort{clf} & 2\\
    (6) & \multicolumn{2}{c|}{BCSS} & / & 151 & $<$ 7k $\times$ 10k & no & no & \acrshort{seg} & 7\\
    (7) & \multicolumn{2}{c|}{NuCLS} & / & 3944 & $<$ 300 $\times$ 300 & yes & yes & \acrshort{seg}, \acrshort{det} & 12 \\
    (8) & \multicolumn{2}{c|}{PAIP 2021} & 150 & / & / & no & no & \acrshort{seg}$^{(b)}$ & 4 \\
    \hline
  \end{tabular}
  \caption{List of challenges published in 2021 with the ``histology'' modality on the Grand Challenge website. The ``Crowd'' and 
  ``AI-assist.'' columns relate to the annotation process and how it was performed. The former indicates whether or not non-pathologists 
  were involved in the process (\ie \textit{yes} for crowdsourcing). The latter indicates whether or not the annotation process involved 
  some kind of AI assistance. The ``\# targ.'' indicates the number of target categories/classes of the underlying \acrlong{ml} problem. 
  References for the challenges: $(1)$ \TODO{\cite{}}, $(2)$ \cite{graham2021conic}, $(3)$ \cite{aubreville2021mitosis}, $(4)$ \cite{xu2021predicting}, 
  $(5)$ \cite{han2021multilayer}, $(6)$ \cite{amgad2019structured}, $(7)$ \cite{amgad2021nucls}, $(8)$ \TODO{\cite{}}. $(a)$ The BCNB challenge 
  proposes 6 differents classification tasks each with up to 4 classes (for a total of 16 classes). $(b)$ The expected output is not a 
  classical segmentation mask but rather the boundary of the structure of interest.}
  \label{tab:backdp:datascarcity-grandchallenge}
\end{table}

In order to illustrate this point, we performed a search on the Grand Challenge website \parencite{grandchallenge}, a popular plateform which runs \acrlong{ml} challenges related to biomedical images, each challenge coming with an open-access dataset. We searched for \textit{histology} challenges published in 2021 and found 8 results (see Table \ref{tab:backdp:datascarcity-grandchallenge} for a brief description of relevant aspects of these datasets).

A first observation is that, whether the dataset is a set of \acrshort{wsi} or \textit{\acrlong{roi}s} (\acrshort{roi}), the number of provided images is several orders of magnitude below the number of images available in natural image datasets. It can be argued that the dimensions of \acrshort{wsi} and \acrshort{roi} images in \acrlong{dp} datasets is significantly larger that natural images (\eg average image size in ImageNet is 469 $\times$ 387 pixels, the largest of the 151 \acrshort{roi} of BCSS reaches approximately 7k $\times$ 10k pixels), however this must be in perspective in relation to the prediction task. For instance, hundreds of \acrshort{wsi}s represent a very large amount of raw data (\eg few terabytes) but, when the target task is whole-slide classification, this only amounts to a hundreds of annotated samples which is significantly fewer compared to ImageNet or others and can be considered a rather small sample size from a \acrshort{ml} perspective.  

Beyond the size of datasets, it is interesting to note that most prediction tasks in \acrlong{dp} only feature few classes (at most 12 in our Grand Challenge sample). It is common to encounter tasks presented as binary (\eg malignant \vs benign) even though it is often a simplification of the underlying biological or medical problem. Datasets with a large variety of classes have shown to be effective for learning efficient models on natural images. Therefore, on this matter, \acrlong{dp} is still lagging behind significantly. To the best of our knowledge, there is no single dataset that features more than 100 classes. \TODO{what is the largest?}

It is also interesting to consider how these datasets were built. Among them, four datasets use internal data acquired and annotated specifically for the challenge (PAIP 2021, MIDOG 2021, BCNB, BCSS), two of them mix both internal and external data (TIGER, WSSS4LUAD) and the last two use exclusively data from external sources (CoNiC, NuCLS). Regarding the use of external data, it either means that external \acrshort{wsi} were imported and annotated for the challenge or that data from other datasets were combined together. For instance, the CoNiC challenge is based on the Lizard dataset \parencite{graham2021lizard} which combines data from the \acrfirstit{tcga} \parencite{weinstein2013cancer}, PanNuke \parencite{gamper2019pannuke}, CRAG \parencite{graham2019mild}, CoNSeP \parencite{graham2019hover}, GlaS \parencite{sirinukunwattana2017gland} and DigestPath \parencite{li2019signet}. The \acrshort{tcga} is an open platform gathering various data related to cancer genomic including a little more than 30k \acrlong{wsi}s, making it one of the largest open database of \acrshort{wsi} to date. It is no surprise that, over the years, many datasets have been built using the \acrshort{tcga} as one of their sources. This also includes WSSS4LUAD, TIGER and NuCLS from our Grand Challenge sample. Interestingly, one of the components of Lizard, PanNuke has also been built from \acrfirst{tcga} and three external datasets of which two were built on top of the same platform (MoNuSeg \parencite{kumar2019multi} and CMP17 \parencite{vu2019methods}). Although, it raises the question of the risk of data leakage, assembling existing datasets to form a larger one certainly helps fighting data scarcity.

\subsubsection{Causes}
\label{sssec:backdp:ds-causes}

The mini-review performed in the previous section was not aimed at being a thorough evaluation of \acrshort{dp} datasets characteristics. It rather serves as a way to highlight different consequences of data scarcity in the field: small dataset size, lack variety and versatility, \etc. The scarcity in \acrlong{dp} is caused by a combination of factors.  

One of the main causes of scarcity is the cost of the annotation process. Pathology is not a simple subject and slide evaluation requires years of training and experience. Whereas classifying pictures into object categories can be done by mostly anyone, creating a ground truth \acrlong{dp} dataset requires trained pathologists for whom time is a precious and expensive resource. Therefore, annotation cost in \acrlong{dp} is significantly higher compared to the natural image domain. This is aggravated by the fact that disagreement between pathologists is not uncommon and quality ground truth usually requires confronting and aggregating annotations by several experts. 


Privacy and ethical concerns also play a role. Indeed, medical data is sensitive and cannot be shared without patient consent, rightfully so, preventing data to be made available for \acrlong{ml}. Privacy might incur additional costs as it might be necessary to keep track where data records have been used, for instance, in the context of the european \acrfirst{gdpr}. Indeed, in case a patient revokes his data sharing agreement, under the right-to-be-forgotten, models might have to be re-trained \parencite{humerick2017taking}. Overall, it can discourage researchers and partictioners to make their data available. 

The use of data-hungry \acrlong{dl} algorithms does not help and is aggravated by the need for a dataset to contain enough samples to account for the variability incurred by the slides content and preparation process. 

% On another front, complexity and duration of the slide preparation process makes it more difficult to produce new \acrshort{wsi} than natural images like pictures which can be captured with any smartphone nowadays. Currently, slide preparation is definitely not a bottleneck as opposed to labeling but could become one if  throughput of quality annotations.

\subsubsection{Potential solutions}
\label{sssec:backdp:ds-solutions}

The causes of data scarcity are numerous and it remains a challenge for \acrlong{dp} but the situation is slowly improving. 

Nowadays, there exists annotation strategies which allow to cut the cost per annotation and therefore produce larger datasets given the same budget. These strategies include crowdsourcing through citizen science \parencite{peplow2016citizen} or the intervention of medical students (\eg NuCLS dataset). These approaches obviously require either supervision by pathologists or more annotations per image to average out the inaccuracies (or even both) but these are in general less expensive than direct annotation by trained pathologists. It is also possible indirectly reduce the annotation cost by reducing the time spent per annotation. One way of achieving that is to use of AI to assist experts and accelerate the annotation process (see NuCLS, CoNiC, MIDOG 2021 from our Grand Challenge sample) \parencite{chai2020human}. An algorithm could for instance suggest cell boundaries with a weak algorithm. The annotator could then correct the boundaries if necessary which is less time-consuming then drawing the boundary from scratch. The use of dedicated and intuitive user interface with efficient drawing tools can also help in that regard.  

If annotating new data is not an option, it is also possible to combat data scarcity by the use of existing external data and proper computational methods. There exists several open resources for \acrlong{dp} \parencite{maree2019open} providing access to slides and annotations. We have already introduced \acrshort{tcga} which provides access to a spectacular amount of 30k \acrshort{wsi}. The Camelyon dataset \parencite{litjens2018camelyon} contains 1399 \acrshort{wsi} of which 209 were annotated at full resolution with detailed hand-drawn contours of all metastases. This is one of the largest most-precisely annotated dataset in the field to date. The Lizard dataset (subject of the CoNiC challenge) contains almost 500k segmented nuclei all labelled into one of 6 classes. Some promising projects are also ongoing such as Bigpicture \parencite{moulin2021imi} of which the goals are to ``\textit{create the first European, ethically compliant, and quality-controlled whole slide imaging platform, in which both large-scale data and AI algorithms will exist}'' and have the same impact on pathology as ImageNet had on the natural image domain. 

Regarding the computational methods, there exists machine and \acrlong{dl} algorithms which can make use of external data. We explore some of them in the thesis and present related works in Section \ref{ssec:backdp:compmethods}. Sometimes the lack of data is such that the dataset is not representative enough of the different variations that could occur in a realistic context. The impact of this issue can be alleviated during the training process directly with an ad-hoc data augmentation procedure that would automatically generate the kind of variations that can be expected (\eg staining intensity, deformation, \etc). Data normalization during pre-processing can also help to reduce variability. \TODO{present data aug techniques???}

% AI-assisted annotation and crowdsourcing improve significantly the

\TODO{class imbalance, label noise}
% Data scarcity can also appear within a problem where annotations and images are plentiful. Indeed,  

\subsection{Computational methods}
\label{ssec:backdp:compmethods}
\parencite{van2019strategies}


\subsubsection{Transfer learning}
\label{sssec:backdp:tl}

General purpose transfer learning was discussed in Sections \ref{ssec:backml:transfer} and \ref{ssec:backml:dl:deeptransfer}. In this section, we explore how deep transfer learning was applied to biomedical images. Some of the first applications were reported in \parencite{bar2015chest,ciompi2015automatic,van2015off} for pulmonary nodule detection in chest x-rays and \acrshort{ct}-scans using Decaf and OverFeat (feature extractors based on ImageNet and AlexNet) as feature extractors. While those works have revealed the potential of deep transfer learning in that field, the performances were not significantly better than those of previous methods. \parencite{ravishankar2016understanding} compared the performance of a pre-trained AlexNet-based CaffeNet \parencite{jia2014caffe} to well-established computer vision like \acrfirstit{hog} \parencite{mcconnell1986method} and found transfer to outperform these features.

Later, as new ImageNet architectures emerged, their transfer potential from ImageNet was also evaluated on various biomedical imaging tasks. \parencite{antony2016quantifying} evaluated the use fine-tuning and feature extraction of different archictures including VGG16 for quantifying knee osteoarthritis severity on radiographs. \parencite{kieffer2017convolutional} compared training from scratch to both feature extraction and fine-tuning for classification and retrieval of digital patholog images using InceptionV3 and VGG16.  \parencite{shin2016deep} studies the interest of transfer of different architectures including GoogLeNet and AlexNet for thoracoabdominal lymph node detection and interstitial lung disease classification. 

\parencite{mensink2021factors}


\parencite{khan2019improving} pretrain an InceptionV3 network on a custom dataset generated from Camelyon16 and then transfer the resulting model to a prostate cancer classification task. They show that their pre-trained model outperforms both training from scratch and using an ImageNet pre-trained model. \etal \parencite{medela2019few} also make use of transfer learning between two digital pathology tasks but use a different pre-training approach. Indeed, they train a siamese network to distinguish different parts of colorectal tissues. The network is then transferred as a feature extractor on the target task (tumour classification). \parencite{shang2019and} use several datasets (including some unrelated to their target task such as \textit{Dogs vs. cats}) and compare ImageNet and domain-specific pre-training in order to tackle colonoscopy image classification. They also show that pre-training on domain-specific data yield superior performance compared to using ImageNet. \parencite{kraus2017automated} train a custom deep neural architecture, DeepLoc, for classifying protein subcellular localization in budding yeast. Then, they assess the transferability of their pre-trained DeepLoc by fine-tuning it on different image sets including unseen classes and show that the pre-training is indeed beneficial.




% More recently, fine-tuning has been also investigated and compared with networks trained from scratch and classifiers trained on off-the-shelf features on a wide variety of biomedical imaging tasks \parencite{antony2016quantifying,esteva2017dermatologist,gulshan2016development, ravishankar2016understanding,shin2016deep,tajbakhsh2016convolutional}. More specifically, digital pathology and microscopy are not outdone with works on tissue texture \parencite{kieffer2017convolutional}, cell nuclei \parencite{bayramoglu2016transfer} and breast cancer \parencite{han2017breast} classification from histopathology images or analysis of high-content microscopy data \parencite{kraus2017automated}. Most works have used networks pre-trained on the ImageNet image classification dataset. Others, like \parencite{kraus2017automated}, have used a custom network architecture pre-trained on a medical dataset as source task. 

% While it is clear that training from scratch very deep networks is not viable in most case due to data scarcity \parencite{bayramoglu2016transfer,tajbakhsh2016convolutional}, there is currently no consensus and best practice about how transfer learning should be applied to digital pathology and microscopy. Some recent publications in the biomedical field have shown that fine-tuning outperforms off-the-shelf features \parencite{,,,}. However, as noted in \parencite{litjens2017survey}, experiments in these papers are often carried out on a single dataset, which does not allow to draw general conclusions. Moreover, those experiments do not use current state-of-the-art networks. A short review of the networks used in biomedical imaging is given in Supplementary Table 1 and shows that the more recent and efficient residual and densely connected networks have been underused or not used at all, in particular in digital pathology and microscopy.

% Transfer learning is not a recent field of research \parencite{pan2010survey} but has grown in popularity with deep learning as it has been shown that features learned on a source task by a neural network could be transferred to a possibly unrelated target task \parencite{sermanet2013overfeat, razavian2014cnn, yosinski2014transferable}. The success of this approach is mostly due to the possibility to use the large and versatile ImageNet \parencite{deng2009imagenet} dataset as a source task \parencite{kornblith2019better}. Medical imaging and digital pathology communities have therefore studied and used transfer learning \parencite{tajbakhsh2016convolutional, shin2016deep, mormont2018comparison, babaie2019tissuefold, ponzio2019dealing} as it provides a way of coping with data scarcity. Those works have explored and evaluated different transfer techniques mostly using ImageNet as a source task. The current consensus is that transfer is helpful in most cases and should be considered when tackling a new task. More recently, several works have focused on transferring a model pre-trained on medical, biology or digital pathology datasets. This is motivated by the fact that one can expect better performance from transfer learning when target and source task are close or related \parencite{yosinski2014transferable}. 

% Independently, multi-task learning \parencite{zhang2017survey} has been applied with great successes for a wide-range of application. The success of MTL is notably due to the fact that leveraging several tasks and/or datasets alleviates the need for large amounts of data. Moreover, training in multi-task has regularization effect preventing the model to overfit a particular task therefore yielding a better generalizing model. The modularity of neural networks also allows to embed multi-task specific components hence facilitating its application to deep learning \parencite{caruana1997multitask, zhang2017survey}. There are many ways how MTL can be implemented within deep learning with, for instance, architecture tricks \parencite{misra2016cross, strezoski2019many} or weight sharing \parencite{caruana1997multitask}. 

% Multi-task learning has been applied to medical imaging. Samala \etal \parencite{samala2017multi} jointly train a classifier on three mammography image datasets (digitized screen-film and digital mammograms) and compare it to single-task training and transfer learning. They show that multi-task trained network generalizes better than the single-task one. Zhang \etal \parencite{zhang2016deep} use transfer and multi-task learning to derive image features from Drosophila gene expression. MTL has also been applied more specifically to digital pathology. Pan \etal \parencite{pan2018multi} apply MTL for breast cancer classification by using a classification loss and a verification loss. The role of the latter is to ensure that features produced by the network differ for images of different classes. Arvaniti \etal \parencite{arvaniti2018coupling} use both weak and strong supervision at once to classify prostate cancer. Shang \etal (mentioned earlier) also evaluate multi-task learning which is the best performing approach on their target task. However, they suggest that more experiments would have to be carried out to assess whether their conclusions are generalizable.

% Our work lies at the crossroad of multi-task and transfer learning and differs from the previously presented works mostly on the objective. Indeed, we do not use MTL nor transfer learning for solving a specific task but rather to pre-train a versatile network to be transferred to new tasks. 


\parencite{van2019strategies}

\subsubsection{Multi-task learning}
\label{sssec:backdp:mtl}

\subsubsection{Self-training}
\label{sssec:backdp:st}
