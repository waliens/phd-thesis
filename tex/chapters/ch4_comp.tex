\chapter{Transfer learning from ImageNet}
\label{chap:comp}

%%%%%%%%% ABSTRACT
% \begin{abstract}
% In this paper, 
% \end{abstract}


\begin{overview}{Overview}
    In this chapter, we investigate deep heterogeneous model-based transfer learning as a way to overcome object recognition challenges encountered in the field of \acrlong{dp}. Through several experiments, we explore various uses of pre-trained neural network architectures and different combination schemes with random forests for feature selection. Our experiments on eight classification datasets show that densely connected and residual networks consistently yield best performances across strategies. It also appears that network fine-tuning and using inner layers features are the best performing strategies, with the former yielding slightly superior results. \\

    \textbf{References:} this chapter is an adapted version of the following article 

    \fullcite{mormont2018comparison} \\

    Supplementary materials can be found in Appendix \ref{app:comp}.
\end{overview}

%-------------------------------------------------------------------------------------------------------------------
%-------------------------------------------------------------------------------------------------------------------
%%%%%%%%% BODY TEXT
\section{Introduction}

% In pathology, tissues were traditionally examined under an optical microscope after being sectioned, stained and mounted on a glass slide. During the last years, progress in scanning technologies made possible the high-throughput digitization of glass slides at high resolution. \acrshort{dp} holds promise for biomedical research and clinical routine but raises great challenges for computer vision research \cite{automated-histology-signal-proc-2014}. 
% First and foremost, a laboratory can produce and scan large amounts of slides per day, each of them being a multi-gigapixel image. Moreover, those slides can contain many different kinds of tissues with different staining techniques. Their quantity, variability and size therefore require efficient and versatile computer vision methods. The second challenge is the scarcity of annotated data. Indeed, annotations of digitized slides require expertise and are therefore expensive and tedious to obtain.

% In parallel, \acrshort{dl} has recently had an impressive impact over the computer vision field starting with work of \citeauthor{krizhevsky2012imagenet} \cite{krizhevsky2012imagenet}, which improved previous natural image recognition performances by a large margin. More recently, researchers have been working at applying those new techniques to biomedical imaging \cite{greenspan2016guest,litjens2017survey}. While current results are promising, improvements were not as impressive as they had been for traditional computer vision tasks (recognition of natural scenes or objects such as ImageNet \cite{deng2009imagenet}, face recognition...) which can likely be attributed to the lack of annotated data.

Data scarcity in \acrlong{cpath} is a recurrent problem which can be tackled in several ways, one of them being transfer learning (see Section \ref{ssec:backdp:datascarcity}). In this chapter, we explore how \acrlong{cnn}s pre-trained on ImageNet \cite{deng2009imagenet} (source task) can be transferred to \acrlong{cpath} target tasks as these architectures have shown interesting transfer properties \cite{donahue2014decaf,yosinski2014transferable,sermanet2013overfeat}. At the time of writing the article this chapter is based on (early 2018), the feature extraction and fine-tuning transfer approaches had not been compared thoroughly yet in \acrlong{cpath} and there was no consensus about whether one was better than the other (see related works in Sections \ref{ssec:backml:dl:deeptransfer} and \ref{ssec:backdp:tl}). Moreover, most works involving transfer learning in medical imaging used old architectures such as AlexNet \cite{shin2016deep,bayramoglu2016transfer,antony2016quantifying,ravishankar2016understanding,tajbakhsh2016convolutional,kumar2017comparative,kim2016deep}, GoogLeNet \cite{shin2016deep,bayramoglu2016transfer} or VGG \cite{kieffer2017convolutional,bayramoglu2016transfer,antony2016quantifying,yu2017deep,hou2016automatic,kumar2017comparative}. To the best of our knowledge, only one article used residual networks for transfer learning at that time \cite{yu2017deep} and none were using dense networks.

Therefore, we study thoroughly and compare several strategies that involve feature extraction and fine-tuning. We carry out several experiments over eight object classification histology and cytology datasets. Different combinations of \acrlong{sota} networks and feature selection techniques using random forests are proposed in order to answer questions of high pratical relevance: which network provides best-performing features? How should those features be extracted and then exploited to get the best performance? Is fine-tuning better than using features from off-the-shelf networks? More generally, our empirical study also contributes to confirm the interest of deep transfer learning for tackling the recurrent data scarcity problem in \acrlong{cpath}.

This chapter is organized as follows. We first introduce the eight datasets we have used in Section \ref{sec:comp:datasets}. We present how we implement feature extraction and fine-tuning in Section \ref{sec:comp:methods}. Our experiments and results are presented and discussed in Section \ref{sec:comp:experiments}. We finally conclude in Section \ref{sec:comp:conclusion}. 

\section{Datasets}
\label{sec:comp:datasets}

\begin{table}
    \center 
    \small
    \begin{tabular}{|c|c|c||ccc|c|}
        \hline
        \multirow{2}{*}{Dataset} & \multirow{2}{*}{Domain} & \multirow{2}{*}{Classes} & \multicolumn{4}{c|}{Images} \\
        \cline{4-7}
        & & & Train & Validation & Test & Total \\
        \hline
        CellInclusion (C) & Cyto & 2 & 1644 & 173 & 1821 & 3638 \\ % & 21 & 2 & 22 & 45   % cells_no_aug
        ProliferativePattern (P) & Cyto & 2 & 1179 & 167 & 511 & 1857 \\ % & 19 & 4 & 13 & 36   % patterns_no_aug
        Glomeruli (G) & Histo & 2  & 12157 & 2448 & 14608 & 29213 \\ % & 91 & 12 & 102 & 205   % glomeruli_no_aug
        Necrosis (N) & Histo & 2 & 695 & 96 & 91 & 882 \\ % & 9 & 1 & 3 & 13   % ulg_lbtd2_chimio_necrose
        Breast (B) & Histo & 2 & 14055 & 4206 & 4771 & 23032 \\ % & 22 & 8 & 4 & 34   % ulg_breast
        MouseLba  (M) & Cyto & 8 & 1722 & 716 & 1846 & 4284 \\ % & 9 & 4 & 7 & 20   % ulg_lbtd_lba
        Lung (L) & Histo & 10 & 4881 & 562 & 888 & 6331 \\ % & 669 & 73 & 140 & 882   % ulg_lbtd_tissus
        HumanLba (H) & Cyto & 9 & 4051 & 346 & 1023 & 5420 \\ % & 50 & 5 & 9 & 64   % ulb_anapath_lba
        \hline
    \end{tabular}
    \caption{Sizes and splits of the datasets. The ``Glomeruli'' dataset was first used in \cite{maree2016approach}.}
    \label{tab:comp:dataset_information}
\end{table}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.225\textwidth}
        \centering
        \includegraphics[width=\textwidth]{comp/illus_necrose.png}
        \caption{Necrosis}
        \label{sfig:comp:dataset:necrosis}
    \end{subfigure}
    \begin{subfigure}[t]{0.225\textwidth}
        \centering
        \includegraphics[width=\textwidth]{comp/illus_patterns.png}
        \caption{ProliferativePattern}
        \label{sfig:comp:dataset:proliferativepattern}
    \end{subfigure}
    \begin{subfigure}[t]{0.225\textwidth}
        \centering
        \includegraphics[width=\textwidth]{comp/illus_cells.png}
        \caption{CellInclusion}
        \label{sfig:comp:dataset:cellinclusion}
    \end{subfigure}
    \begin{subfigure}[t]{0.225\textwidth}
        \centering
        \includegraphics[width=\textwidth]{comp/illus_breast.png}
        \caption{Breast}
        \label{sfig:comp:dataset:breast}
    \end{subfigure} \\
    \begin{subfigure}[t]{0.225\textwidth}
        \centering
        \includegraphics[width=\textwidth]{comp/illus_glomeruli.png}
        \caption{Glomeruli}
        \label{sfig:comp:dataset:glomeruli}
    \end{subfigure}
    \begin{subfigure}[t]{0.225\textwidth}
        \centering
        \includegraphics[width=\textwidth]{comp/illus_lbtd_lba.png}
        \caption{MouseLba}
        \label{sfig:comp:dataset:mouselba}
    \end{subfigure}
    \begin{subfigure}[t]{0.225\textwidth}
        \centering
        \includegraphics[width=\textwidth]{comp/illus_anapath.png}
        \caption{HumanLba}
        \label{sfig:comp:dataset:humanlba}
    \end{subfigure}
    \begin{subfigure}[t]{0.225\textwidth}
        \centering
        \includegraphics[width=\textwidth]{comp/illus_tissus.png}
        \caption{Lung}
        \label{sfig:comp:dataset:lung}
    \end{subfigure}
    \caption{Overview of our eight classification datasets (the display size does not reflect actual image size). For binary classification datasets, negative and positive samples were respectively placed at the top and bottom of the figures.}
    \label{fig:comp:dataset_samples}
\end{figure}
 

Our experimental study uses datasets collected over the years by biomedical researchers and pathologists using the Cytomine \cite{maree2016collaborative} web application. Using this platform, eight image classification datasets were collected which are summarized in Table \ref{tab:comp:dataset_information}. These contain tissues and cells from human or animal organs (thyroid, kidney, breast, lung, \etc.).

For all datasets except Breast, each sample image is the crop of an annotated object extracted from a whole-slide image, a crop being a rectangle image containing exactly the object. The Breast dataset is composed of patches for which the label encodes the type of tissue in which the central pixel is located. Selected image samples for each dataset are shown in Figure \ref{fig:comp:dataset_samples}. 

\section{Methods}
\label{sec:comp:methods}

In this section, we introduce the architectures we have used in Section \ref{ssec:comp:deep_networks}, we describe how we have implemented feature extraction in Section \ref{ssec:comp:feature_extr} and fine-tuning in Section \ref{ssec:comp:meth_fine_tuning}. We also provide details about the training procedure of the final classifier in Section \ref{ssec:comp:final_classifier} and the inference procedure in Section \ref{ssec:comp:prediction}. As discussed in Section \ref{ssec:backml:dl:deeptransfer}, the interest of feature extraction is that it is not as computationally demanding as fine-tuning but, because the transferred model has not been retrained on the target, the features might not be specific enough. Fine-tuning is much more computationally demanding as it implies retraining a usually large model (\ie tens of millions of parameters or more) but make the features more specific and therefore more relevant to the target which usually has a positive effect on performance. 

\subsection{Deep networks}
\label{ssec:comp:deep_networks}

We follow the feature extraction and classification process presented in Figure \ref{fig:comp:diag_feat_extract}, which starts from a deep \acrlong{cnn} $\theta_s$ pre-trained on a source task $\mathcal{D}_s$. In particular, we use ImageNet as the source dataset $\mathcal{D}_s$ and, for each network, the pre-trained weights were retrieved from Keras \cite{chollet2015keras}. For the network $\theta_s$, we evaluate several architectures that have been \acrlong{sota} on the ImageNet classification dataset \cite{deng2009imagenet} or that present interesting trade-off between computational requirements and performances: VGG16, VGG19 \cite{simonyan2014very}, InceptionV3 \cite{szegedy2016rethinking}, ResNet50 \cite{he2016deep}, InceptionResNetV2 \cite{szegedy2017inception}, DenseNet201 \cite{huang2017densely}, and MobileNet \cite{howard2017mobilenets}. In the sequel, those networks will be respectively referred to as VGG16, VGG19, IncV3, ResNet, IncResV2, DenseNet, and Mobile. These networks can be used as feature extractors or fine-tuned, as explained in the following sections.

\subsection{Feature extraction}
\label{ssec:comp:feature_extr}

\begin{figure*}
    \center
    \includegraphics[scale=0.50]{comp/offtheshelf_schema.pdf}
    \caption{Feature extraction from pre-trained convolutional neural networks}
    \label{fig:comp:diag_feat_extract}
\end{figure*}

Images are first resized to match the input dimension of the network. Respectively denoting by $r_I \times c_I \times b$ the height, width, and number of channels of the input image $I$, we extract a square patch $\mathbf{p}$ of (maximum) height and width $min\left(r_I, c_I\right)$ in the center of the image, which is then resized to the network input size $r_p\times c_p\times b$. This extraction process is parameter-free and preserves the aspect ratio of the image, since all pre-trained networks takes square images as inputs (\ie $r_p=c_p$).

The resized patch is then forwarded through $\theta_s$ (loaded with pre-trained weights). Given this input, the output $\mathbf{a}$ of an arbitrary layer $l$ (\ie a set of feature maps) of dimensions $r_a \times c_a \times d$ is extracted where $d$ is the number of feature maps and $r_a$ and $c_a$ are respectively their height and width. Because this tensor can be high-dimensional, one usually applies a dimensionality reduction procedure (\eg global average pooling, principal component analysis, \etc.) to reduce it to $f$ features, yielding a feature vector $\mathbf{z} \in \mathbb{R}^f$.  Here, we limit our analysis to global average pooling (\ie feature maps averaging), which, unlike principal component analysis for example, has the advantage of being parameter-free.

\subsection{Fine-tuned feature extraction}
\label{ssec:comp:meth_fine_tuning}

For our experiments on network fine-tuning, we replace the final fully-connected layer by a fully-connected layer with as many neurons as there are classes in the current dataset. This newly-appended layer being randomly initialized, it is not correlated at all with the transferred network $\theta_s$ yet. In general, it is a good idea to freeze $\theta_s$ and to train this new layer for a few epochs to correlate them. In our case, we perform this ``warm-up'' phase for 5 epochs with a learning rate of $10^{-2}$. Then, we train the whole network for 45 epochs with a learning rate of $10^{-5}$. We use the Adam optimizer \cite{kingma2014adam} with parameters $\beta_1$ and $\beta_2$ respectively set to 0.99 and 0.999 and no weight decay as suggested in the original paper. We use the categorical cross-entropy as a loss function. Fine-tuning is performed using the training set exclusively. Optionally, we use data augmentation to virtually increase the size of the training set. First, a maximum square crop is taken at a random position in the input image. %% (unlike the extraction procedure presented in Section \ref{ssec:comp:meth_general_framework} which took the crop at the center of the image)
Then, random flips and rotations are applied to the resized patches before they are forwarded through the network. The model is evaluated on the validation set and then saved at the end of each epoch. When the fine-tuning is over, we select among the saved models the one that performed the best on the validation set.

\subsection{Final classifier learning}
\label{ssec:comp:final_classifier}
When the features have been extracted for all images of a dataset (either using off-the-shelf or fine-tuned networks), they can be used for training the classifier $h_c$. 
We use either linear \acrshort{svm} \cite{boser1992training} (with a \acrlong{ovr} scheme for multi-class problems), \acrfirst{et} \cite{geurts2006extremely}, or a \acrlong{fc} single layer perceptron, all as implemented in \texttt{scikit-learn} \cite{scikit-learn}. \acrshort{svm} is the most popular classification method when it comes to classifying extracted features. \acrshort{et} are incorporated mainly for their ability to compute feature importance scores. \acrshort{fc} is a natural choice to mimic how the pre-trained network exploits the features. Hyperparameters of all methods were tuned by slide-wise (or patient-wise) $n$-fold \acrlong{cv} on the merged training and validation sets. Namely, they are the penalty $C$ for \acrshort{svm}, the maximum number of features for \acrshort{et}, and the learning rate and number of iterations for the single layer perceptron. Selected values for tuning are given in Appendix Section \ref{app:comp:sec:selectedhyperparameters}.

\subsection{Prediction}
\label{ssec:comp:prediction}
For prediction, the process is similar: patches are extracted from target images, forwarded through $\theta_s$, reduced by global average pooling and classified with the learned classifier $h_c$. As for the fine-tuning, we make additional experiments using directly the fine-tuned network for classifying the images (see Section \ref{ssec:comp:meth_fine_tuning} for parameters).

\section{Experiments}
\label{sec:comp:experiments}

In this section, we propose and thoroughly compare different strategies for extracting and using features from the deep networks introduced previously. We follow a rigourous evaluation protocol described in Section \ref{ssec:comp:protocol}. Strategies are presented and evaluated one after the other in Sections \ref{ssec:comp:exp_last_layer} to \ref{ssec:comp:exp_fine_tuning}, then an overall comparison of strategies is discussed in Section \ref{ssec:comp:exp_comparing}.


\subsection{Performance metrics and baseline}
\label{ssec:comp:protocol}
For performance evaluation, each dataset (presented in Section \ref{sec:comp:datasets}) is randomly splitted into training, validation and test sets. Following the guidelines in \cite{maree2017need}, image patches from the same slide (or patient when this information was available) are all put in the same set to avoid any data leakage (see Section \ref{ssec:backdp:dataleakage}). %Ideally, one should also avoid putting images from the same patient in different sets but this information was not available, except for the Breast data (which was therefore splitted accordingly).  

To evaluate the different transfer strategies, we use two different metrics: the area under the receiving operating curve (ROC AUC) for binary classification problems and the classification accuracy for the multi-class problems. For the sake of readibility, a summary of the scores for all experiments and datasets is given in Table \ref{tab:comp:res_best_scores_per_strategy} whereas the detailed scores are only given in Appendix Section \ref{app:comp:sec:detailed_scores}. In all figures, we plot instead for each method its rank among all methods compared in the same graph averaged over all eight datasets. To associate high rank with best results, we compute the rank when methods are sorted in reverse order of performance (AUC or accuracy). For example, since ten methods are compared in Figure \ref{fig:comp:avg_ranks_last_layer}, the maximum average rank is 22, corresponding to a method being the best one on all eight datasets, and the minimum average rank is 1, corresponding to a method always worse than all others.

As a baseline for comparison, we use the \acrshort{etfl} variant of the tree-based random subwindows classification algorithm presented in Section \ref{ssec:backml:et_image} because it is fast, generic and requires only a slight tuning of the hyperparameters which are either set to their default value or tuned by \acrlong{cv} (see in Appendix Section \ref{app:comp:sec:selectedhyperparameters} for default values and ranges). We do not compare to training from scratch as it has already been shown that transfer learning from ImageNet was generally superior this this approach \cite{shin2016deep, tajbakhsh2016convolutional}.

\subsection{Last layer features}
\label{ssec:comp:exp_last_layer}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{comp/last_baseline_bars.png}
    \caption{Average ranks of the methods for the ``\textit{Last layer features}" experiment. Colors encode the choice of classifier for $h_c$ (orange for \acrshort{svm}, green for \acrshort{et} and red for \acrshort{fc}).}
    \label{fig:comp:avg_ranks_last_layer}
\end{figure}

Our first strategy follows the common approach where features are extracted at the last layer of a pre-trained network. In our case, we take the features from the last feature maps before the first fully connected layer (the numbers of extracted features per network are given in Table \ref{tab:comp:n_features_per_net}). For each dataset and network, we then tune and train the three types of classifiers $h_c$ (\acrshort{et}, \acrshort{svm}, and \acrshort{fc}) with the extracted features, on the union of the training and validation sets, and then we evaluate them on the test set. The resulting average ranks for all classifiers and all datasets are given in Figure \ref{fig:comp:avg_ranks_last_layer}.

\begin{table}
    \center 
    \begin{tabular}{|c|c|c|c|}
        \hline
        \multirow{2}{*}{$\mathcal{N}$} & \multicolumn{1}{c|}{\textbf{Last layer}} & \multicolumn{2}{c|}{\textbf{Merged layers}} \\
        \cline{2-4}
        & \# feat. & \# feat. & \# cut \\
        \hline
        Mobile & 1024 & / & /\\ 
        DenseNet & 1920 & 7744 & 9 \\ 
        IncResV2 & 1536 & 17088 & 12 \\ 
        ResNet & 2048 & 15168 & 17 \\ 
%          IncV3 & 2048 & 10112 & 12 \\ 
        IncV3 & 2048 & / & / \\ 
        VGG19 & 512 & / & / \\ 
        VGG16 & 512 & / & / \\  
        \hline
        \textbf{Total} & 9600 & / & / \\
        \hline 
    \end{tabular}
    \caption{Number of features extracted for the ``\textit{Last layer}'' experiment. Total number of features for the ``\textit{Merging features across networks}". Number of features and cut points for the ``\textit{Mering layers features}'' experiment.}
    \label{tab:comp:n_features_per_net}
\end{table}

We observe that \acrshort{svm} and single layer perceptron are more efficient at classifying from deep features than extremely randomized trees, with a slight advantage to \acrshort{svm}. Mobile, DenseNet, IncResV2 and ResNet yield best performances when combined with \acrshort{svm} or single layer perceptron, while for extremely randomized trees, only DenseNet and ResNet are leading the way. Last layer features from VGG16, VGG19 and IncV3 allow most of the time to beat the baseline but they are clearly not competitive with features from the other networks whatever the classifier. Overall, the best performance is obtained by combining ResNet features with \acrshort{svm}.

\subsection{Last layer feature subset selection}
\label{ssec:comp:exp_feat_sel}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{comp/rfe_last_et_bars.png}
    \caption{Average ranks for last layers' features classified with \acrshort{et} before (orange) and after (green) selection with \acrlong{rfe}.}
    \label{fig:comp:exp_rfe_last_et_cmp}
\end{figure}

%% While carrying out the ``\textit{Last layer features}'' experiment (see Section \ref{ssec:comp:exp_last_layer}), we have analyzed the feature importances produced by extremely randomized trees. We have noticed that few features of the last layer seemed actually important for predicting the outcome.
Our second strategy aims at checking whether all features extracted from the last layer of the network are important for the final classification or if a small subset of them would be sufficient to obtain optimal performance. To answer this question, we use cross-validated \acrfirstit{rfe} \cite{guyon2002gene} using importance scores derived from extremely randomized trees to rank the features (where the importance of a given feature is computed by adding up the weighted impurity decreases for all tree nodes where the feature is used, averaged over all trees). This method outputs a sequence $\{(S_t,s_t)|t=1,\ldots,T\}$ (built in reverse), where $S_t$ are nested feature subsets of increasing sizes (with $|S_1|=1$ and $|S_T|=k$) and $s_t$ is the \acrlong{cv} performance of an \acrshort{et} model trained on $S_t$. From this sequence, we compute for each dataset:
$$k_{min} = \min_{\{t=1,\ldots,T: s_t\geq \underset{t'}{max} (s_{t'}-l_a)\}} |S_t|,$$
where $l_a$ is a small performance tolerance (set to $0.005$ in our experiment). $k_{min}$ is thus the minimum number of features needed to reach a performance not smaller than the optimal one by more than $l_a$.

On average across datasets and networks, this method selected 7.5 \% of the features (detailed numbers are given in Appendix Tables \ref{app:comp:tab:rfe_selected} and
\ref{app:comp:tab:rfe_selected_prop}). The models re-trained using the selected features yielded comparable performance when using \acrshort{et} as classifier (see Figure \ref{fig:comp:exp_rfe_last_et_cmp}). Feature selection even improved \acrshort{et} performance on the IncV3, VGG19, and VGG16 networks. Using \acrshort{svm} on the selected features however leads to a performance drop compared to \acrshort{svm} with all features (see Appendix Table \ref{app:comp:tab:detailed_first}). We believe that this difference is due to the fact that the selection is optimized for \acrshort{et} and it is thus likely to remove features that are useful for linear \acrshort{svm} and not for \acrshort{et}, which are non-linear approximators.

Our experiments show that, among the available features from the last layer, most of them are uninformative or redundant and therefore only few of them are actually needed for the prediction when using \acrshort{et} as classifier. This conclusion can also be drawn by observing the \acrlong{rfe} \acrlong{cv} curves (see Appendix Figures \ref{app:comp:fig:rfe_1} and 
\ref{app:comp:fig:rfe_2}). For all datasets and networks, the accuracy converges very abruptly to a plateau when the number of selected features increases, which indicates that removing features from the learning set does not impact negatively the predictive power of the models.  

It is also interesting to note that the selected features are not the same across datasets. For instance with DenseNet, for a feature to appear in the subset of best features (determined with the importances obtained during the ``\textit{Last layer features}" experiment) for all datasets, we need to consider a subset of size 1477 (\ie 77\% of the features). We observe similar results for the other networks (see Appendix Table \ref{app:comp:tab:best_subset_size}). On the other hand, there can be a significant overlap between features selected by \acrshort{rfe} for specific pairs of datasets (see Appendix Tables \ref{app:comp:tab:rfe_sub_overlap} and \ref{app:comp:tab:rfe_sub_overlap2}). These results suggest that the best features are task-dependent and that there would be no interest in restricting a priori the subset of transferred features, even when focusing on the domain of \acrlong{cpath}. The fact that we observe a drop of performance when using the best features subset with \acrshort{svm} also suggests that these conclusions cannot be extended to other classifiers without applying \acrshort{rfe} to them as well.
 
 %-------------------------------------------------------------------------------------------------------------------
\subsection{Merging features across networks}
\label{ssec:comp:exp_merge_net}

\begin{figure}
    \centering 
    \includegraphics[height=\textheight]{comp/all_exp_bars.png}
    \caption{Average ranks for all the evaluated methods.}
    \label{fig:comp:res_avg_ranks_all_methods}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{comp/merged_all_merged_perc.png}
    \caption{Average relative importances (across datasets) brought by each studied network when all their last layer features are aggregated. The black bars quantify the proportion of features of each network. The colors indicate the information brought by features of decreasing importances: blue and red features are respectively the most informative and least informative ones. Blue, orange, green and red bars regroup importances of features that respectively and cumulatively bring 10\%, 25\%, 50\% and 100\% of the information for predicting the outcome. For example, the DenseNet bar  reaches approximately $26\%$, meaning that DenseNet features brings about $26\%$ of the information for predicting the outcome. Moreover, the orange and blue segments for DenseNet together reach approximately $10\%$, meaning that, when selecting the subset of most informative features bringing $25\%$ of the information (orange + blue), those of DenseNet bring $10\%$ of information (or $40\%$ of the information of the subset).}
    \label{fig:comp:models_merged_avg_imp}
\end{figure} 

\begin{table}
    \center
    \small
    \begin{tabular}{|l||c|c|c|c|c||c|c|c|}   
      \hline 
      & \multicolumn{8}{c|}{\textbf{Datasets}} \\
      \hline 
      \textbf{Strategy} & \textbf{C} & \textbf{P} & \textbf{G} & \textbf{N} & \textbf{B} & \textbf{M} & \textbf{L} & \textbf{H} \\
      \hline
      Baseline (ET-FL)      & 0.9250 & 0.8268 & 0.9551 & 0.9805	& 0.9345 & 0.7568 & 0.8547 & 0.6960 \\
      Last layer    & 0.9822 & 0.8893 & 0.9938 & \cellcolor{Dandelion}0.9982 & 0.9603 & 0.7996 & 0.9133	& 0.7820 \\
      Feat. select.	& 0.9676	& 0.8861	& 0.9843	& \cellcolor{LimeGreen}0.9994	& 0.9597	& 0.7438	& 0.8941	& 0.7703 \\
      Merg. networks	& \cellcolor{Dandelion}0.9897	& \cellcolor{LimeGreen}0.8984	& 0.9948	& 0.9864	& 0.9549	& \cellcolor{Dandelion}0.8169	& 0.9155	& 0.7928 \\
      Merg. layers	& 0.9808	& 0.8906	& 0.9944	& 0.9964	& 0.9639	& 0.7941	& 0.9268	& 0.7977 \\
      Inner ResNet	& 0.9748	& \cellcolor{Dandelion}0.8959	& 0.9949	& 0.9964	& 0.9664	& 0.8131	& \cellcolor{Dandelion}0.9291	& \cellcolor{Dandelion}0.8113 \\
      Inner DenseNet	& 0.9862	& \cellcolor{LimeGreen}0.8984	& \cellcolor{Dandelion}0.9962	& 0.9917	& 0.9699	& 0.8012	& 0.9268	& 0.7967 \\
      Inner IncResV2	& 0.9873	& 0.8948	& \cellcolor{Dandelion}0.9962	& \cellcolor{Dandelion}0.9982	& \cellcolor{Dandelion}0.9720	& 0.8137	& 0.9234	& 0.7713 \\
%       Inner IncV3		& 0.9836	& 0.8899	& 0.9951	& 0.9964	& 0.9731	& 0.8104	& 0.9201	& 0.7507 \\
       Fine-tuning		& \cellcolor{LimeGreen}0.9926	& 0.8797	& \cellcolor{LimeGreen}0.9977	& 0.9970	& \cellcolor{LimeGreen}0.9873	& \cellcolor{LimeGreen}0.8727	& \cellcolor{LimeGreen}0.9405	& \cellcolor{LimeGreen}0.8641 \\
%%      \hline
%%      \textbf{Best} & 0.9926 & 0.8984 & 0.9977 & 0.9994 & 0.9873 & 0.8727 & 0.9405 & 0.8641 \\
      \hline
      \textbf{Metric} & \multicolumn{5}{c||}{Roc AUC} & \multicolumn{3}{c|}{Accuracy (multi-class)} \\
      \hline
    \end{tabular}
      \caption{Best score for each strategy and each dataset. The best and second best scores are respectively highlighted in green and orange.}
    \label{tab:comp:res_best_scores_per_strategy}
\end{table}

The third strategy consists in merging features from the last layer of all the studied networks. Aggregating all features results in a feature vector of size $9600$. We observe in Table \ref{tab:comp:res_best_scores_per_strategy} and Figure \ref{fig:comp:res_avg_ranks_all_methods} that despite the fact that features from all networks are combined, this strategy gives performance results similar to but not better than using the best single network, both with \acrshort{et} and \acrshort{svm}.

Using forest importance ranking procedure described previously, we further analyze the information brought by the last layer of each network (feature importances averaged across datasets are given in Figure \ref{fig:comp:models_merged_avg_imp}). We observe that the features of DenseNet bring about $26\%$ of information on average while they only account for $20\%$ of all the features. Moreover, the proportion of information brought by the most informative features of DenseNet is higher than for any other network. Following DenseNet, the next most informative networks are IncResV2, ResNet, IncV3 and finally the Mobile, VGG16 and VGG19 networks. 
Surprisingly, the importances brought by the VGG networks relatively to the number of features is non-negligible and higher than the one of ResNet and IncV3. This may indicate that features of those networks are redundant with features of DenseNet and IncResV2 while features of VGG16 and VGG19 are not.


\subsection{Merging features across layers}
\label{ssec:comp:exp_merged_layers}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{comp/merged_layers_all_bars.png}
    \caption{Average ranks for models learned on merged layers of networks compared to the baseline.}
    \label{fig:comp:res_avg_ranks_merged_layers}
\end{figure}

This strategy aims at merging features across layers (at several depths) for a given network. Given the results in Section \ref{ssec:comp:exp_last_layer}, we limit our analysis to the IncResV2, ResNet and DenseNet networks. Those three networks have complex structures and many layers which yield plenty of possible cut points for feature extraction. To reduce the number of possibilities, we limit the extraction to bottlenecks of the networks, a bottleneck being a point were several paths are merged into a single one. For ResNet, we have selected the \acrshort{relu} activations of the merging layer after each residual block. For IncResV2, considering all bottlenecks (following an inception-resnet or a reduction block) yields approximately fifty possible cut points, that is more than 100k features. We have decided to subsample those cut points to obtain a number of features closer to those of other networks while covering the network as uniformly as possible along its depth. As far as DenseNet is concerned, we extract features only at the end of dense blocks and after pooling blocks. Indeed, extracting features inside dense blocks would have resulted in duplicated features because of the dense connections.  

In Table \ref{tab:comp:n_features_per_net} are given the information about the generated features vectors for the ``\textit{Last layer}", ``\textit{Merging features across networks}" and ``\textit{Merging layers features}" experiments. Information about the dimensions of the extracted features from inside the networks (before global average pooling) for ResNet, IncResV2 and DenseNet are respectively given in Appendix Tables \ref{app:comp:tab:inner_layers_resnet}, \ref{app:comp:tab:inner_layers_incresv2} and \ref{app:comp:tab:inner_layers_densenet}. The layer names given in those tables are the ones given by the Keras package \cite{chollet2015keras}. 

The average ranks for each layer of all studied networks are given in Figure \ref{fig:comp:res_avg_ranks_merged_layers}. One first observation is that there is no significant difference between \acrshort{svm} and \acrshort{et} in terms of performance, unlike when we use the last layer only. Surprisingly, DenseNet is not performing well with respect to the other network while it was competitive with using the last layer only. Merging the layers actually leads to a drop of performance with respect to using only the last layer for DenseNet, while it leads to a small improvement for the other two networks (see Section \ref{ssec:comp:exp_comparing}).

 
\begin{figure}
    \centering
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{comp/dense_net_201_all_merged_perc.png}
        \caption{DenseNet}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{comp/inception_resnet_v2_all_merged_perc.png}
        \caption{IncResV2}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{comp/resnet50_all_merged_perc.png}
        \caption{ResNet}
    \end{subfigure}
%      \subfigure[IncV3]{\includegraphics[width=0.45\textwidth]{images/inception_v3_all_merged_perc.png}}
    \caption{Average relative importances (across datasets) brought by each extracted layer of a network. See Figure \ref{fig:comp:models_merged_avg_imp} for explanation.}
    \label{fig:comp:res_layers_merged_avg_imp}
\end{figure}

As in the previous section, we use feature importances to identify most informative features. Detailed importances plots for each network are given in Figure \ref{fig:comp:res_layers_merged_avg_imp}. These plots clearly show that the most informative features are spread over all layers. We also observe that the relative importance of features in the early layers is higher than the ones in deeper layers. One possible reason is that last layers actually have more features than earlier ones and as shown in Section \ref{ssec:comp:exp_feat_sel}, most of those features are either irrelevant or redundant. Therefore, the extremely randomized trees actually discard most of them them during training. 

Merging features from several layers results in large feature vectors for describing the images. Those large vectors make this method less attractive as it results in longer classifier training time. This is especially true for extremely randomized trees. Unlike in the previous strategy however, feature extraction does not increase computation requirements as only one forward pass through the network is needed to extract all the features.

%-------------------------------------------------------------------------------------------------------------------
\subsection{Inner layers features}
\label{ssec:comp:exp_inner_layers}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{comp/all_per_layer_bars.png}
    \caption{Average ranks for the baseline and the models trained using features from layers inside the networks. For each network, the layers are sorted by decreasing depth (from top to bottom).}
    \label{fig:comp:res_avg_ranks_per_layer}
\end{figure}
 
For this strategy, we assess features extracted from each layer separately. The motivation is to determine if there is a layer that, taken alone, yields better performance than the others, and in particular, the last one. We use the same cut points as for the previous experiment (see Section \ref{ssec:comp:exp_merged_layers}), we learn as many \acrshort{svm} classifiers as there are layers, each using the features of a single layer.

Average ranks for each inner layer and each network are given in Figure \ref{fig:comp:res_avg_ranks_per_layer}. In all cases, the last layer features are always outperformed by features taken from an inner layer of the network. The optimal layer is however always located rather at the end of the network, while the first layers are clearly never competitive. Unfortunately, we have not found that a specific layer was better for all datasets, so in practice the choice of the layer should be determined by internal \acrlong{cv} as we did. Interestingly, the baseline either outperforms the early layers of the networks or yield comparable results which tends to indicate that the features provided by ET-FL are somewhat generic, \ie they could be assimilated to features that you would for instance find in the early layers of a classification \acrshort{cnn} (see Section \ref{ssec:backml:dl:deeptransfer}). 

%-------------------------------------------------------------------------------------------------------------------
\subsection{Fine-tuned features}
\label{ssec:comp:exp_fine_tuning}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{comp/ft_baseline_bars.png}
    \caption{Average ranks for fine-tuned networks compared to the baseline. Evaluation was done either by using \acrshort{svm} (orange) or \acrlong{et} (green) on the fine-tuned features or by predicting the outcome using the fine-tuned \acrlong{fc} layer directly (red).}
    \label{fig:comp:res_avg_ranks_ft}
\end{figure}

All previous experiments explored strategies using features extracted from off-the-shelf networks. In this last strategy, we investigate fine-tuning as described in Section \ref{ssec:comp:meth_fine_tuning}. We focus on the same three networks as in the previous sections (ResNet, IncResV2 and DenseNet).

The average ranks for the different fine-tuning methods are given in Figure \ref{fig:comp:res_avg_ranks_ft}. With the three networks, best performances are obtained by making predictions directly from the fine-tuned fully connected layer. \acrshort{svm} and \acrshort{et} trained on the features extracted from the fine-tuned networks are clearly inferior, in particular with IncResV2. Note that, for the other two, last layer features extracted from the fine-tuned network are nevertheless better than last layer features from the original network, when used as inputs to \acrshort{svm} (see Figure \ref{fig:comp:res_avg_ranks_all_methods}). Fine-tuning is thus globally improving the quality of the features for these two networks. Overall, the best performance is obtained with fine-tuned DenseNet.


\subsection{Discussion}
\label{ssec:comp:exp_comparing}

To allow comparison of all strategies, the best scores per strategy and dataset are summarized in Table \ref{tab:comp:res_best_scores_per_strategy} and the average ranks of all methods evaluated in the previous experiments are given in Figure \ref{fig:comp:res_avg_ranks_all_methods}.

Concerning the networks, ResNet and DenseNet often yield the best performing models whatever the way they are exploited. They are followed by the IncResV2, Mobile, and IncV3 networks. Performances obtained with the VGG networks are below those of the others.

Regarding the methods, fine-tuning (and predicting with the network) usually outperforms all other methods whatever the network. Especially, this strategy yields significant improvements for the multi-class datasets. For binary datasets, the improvement is often not as impressive, but on three of these datasets, the performances of all methods are already very high (greater than 0.9).

Moreover, for each dataset, there is at least one inner layer that yields the best or second best scores and this best layer is never the last one. This is confirmed by the rank plot (see Figure \ref{fig:comp:res_avg_ranks_per_layer}) that shows that the ranks of the models learned on last layer features are below those using inner layer features. This might be explained by the fact that last layer features are too specific to the source task (\ie natural images).

Merging features across networks and layers yield results similar to using last layer features but they are outperformed by the best inner layers and also by fine-tuning. The inferior performances of these methods could be attributed to the fact that important features are lost among many redundant and/or uninformative ones and they are thus suffering from overfitting. One way to improve these methods could be to perform feature selection. However, given that these methods are already more computationally demanding, they are definitely less interesting than fine-tuning and selecting the best inner layer. In particular, merging features across networks requires a forward-pass through all the selected networks. Although only one pass is needed when merging the layers, it still yields a large feature vector which makes further training and tuning slower, especially for \acrshort{et}.

Throughout the experiments, we have also gained more insights about the extracted features. By performing feature selection, we have discovered that very few features are actually useful for learning efficient \acrshort{et} classifiers on our datasets and the best features are task-dependent. This conclusion might extend to other classifiers as well but it would require additional experiments to prove it.

\section{Conclusion}
\label{sec:comp:conclusion}

We have empirically investigated various deep transfer learning strategies for recognition in \acrlong{cpath}. We have observed that residual and densely connected networks often yielded best performances across the various experiments and datasets. We have also observed that fine-tuning outperformed features from the last layer of off-the-shelf networks. It also appeared that using one network's inner layer features yielded performances slightly superior to using those of the last layer and inferior to fine-tuning but with the advantage of not having to re-train the network. 

