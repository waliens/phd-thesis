\chapter{Conclusion and future works}
\label{chap:conclusions}

\section{Wrapping up}
\label{sec:concl:wrappingup}

In this thesis, we investigated how image classification and segmentation \acrlong{ml} techniques could be applied in the context of \acrlong{dp}. In particular, we studied how different methods could be used to alleviate the consequences of data scarcity, a prevalent issue in the domain. Causes and consequences of data scarcity are numerous and are discussed Chapter \ref{chap:backdp}. Some of the consequences include small dataset size, lack of variety and oversimplification of the learning tasks, \etc. These can have a significant negative impact on the efficiency of machine and especially \acrlong{dl} methods which are known to be particularly data-hungry. 

In Part \ref{part:transfer}, we first investigated transfer learning. In Chapter \ref{chap:comp}, we compared different ways seven popular \acrlong{dl} architectures pre-trained on ImageNet could be transferred to pathology classification tasks. We studied both feature extraction and fine-tuning. Regarding feature extraction, we considered features not only from the last layer of the networks but also from inner layers. Moreover, we devised an experiment to evaluate the redundancy of these features using \acrlong{rfe}. We showed that, although source (\ie natural images) and target (\ie pathology images) domains are quite dissimilar, both feature extraction and fine-tuning from ImageNet were effective transfer techniques as they were able to improve significantly over our baseline. Moreover, fine-tuning proved to be more efficient than feature extraction. Among the architectures we have tested, ResNet50 and DenseNet121 often yielded the best performance. We also showed that some of the last layer features were redundant as only a small fraction of them were actually useful for the prediction. However, we observed that the ``useful'' subset was actually task-dependent. 

Moving on to Chapter \ref{chap:mtask}, guided by the fact that transfer learning is known to perform better when source and target tasks are close, we searched for a way to create a transferrable model pre-trained on pathology data. However, because of data scarcity, it was (and still is to this date) not possible to find a sufficently large and versatile pathology dataset to pre-train a \acrlong{dl} model. Therefore, we decided to exploit the fact that many small and medium pathology datasets had been released over the years and decided to pre-train our model in a multi-task fashion using as many of these datasets as possible. We collected 22 classification tasks featuring 81 classes and almost 900k images and designed a multi-task pre-training protocol to exploit them. We showed that our models used as feature extractors either outperformed or provided comparable performance compared to models pre-trained on ImageNet. We also observed that fine-tuning multi-task or ImageNet pre-trained models yielded comparable performance, indicating that fine-tuning was able to recover the lack of specificity of ImageNet features. We also confirmed the conclusions from Chapter \ref{chap:comp} as we showed fine-tuning to outperform feature extraction. Moreover, we also confirmed the consensus that transfer was more efficient than training from scratch.

In Part \ref{part:segmentation} and Chapter \ref{chap:strain}, we focused on \acrlong{ssl} and self-training in particular. We proposed a self-training algorithm for image segmentation able to make use of both exhaustively and sparsely-labeled data. Every training epoch, our approach generates pseudo-labels for unlabeled pixels in the training set using the currently trained model and combines them with the sparse ground truth. The model can then be trained on the combination of the pseudo-labeled and exhaustively-labeled set. Using three exhaustively-labeled datasets that we artificially made sparse, we showed that our self-training approach was able to improve performance over a fully supervised approach learning from the exhaustively-labeled data only. For one of our datasets, self-training was even able to reach performance of a model trained on a completely annotated dataset with only $~30\%$ of the training annotations. We confirmed our findings on an in-house sparsely-labeled dataset where self-training improved performance by a $5\%$ margin compared to our baselines. We finally showed that when annotating a new dataset, it was not necessarily better to annotate it exhaustively. 

We hope that, with these three contributions, we have confirmed that data scarcity in digital pathology was not an inextricable issue and that there exists algorithmic solutions to at least alleviate it. 

\section{Future works}
\label{sec:concl:futureworks}

In this section, we discuss future perspectives for our works.

\paragraph{Transfer with new architectures.} In 2018, when we experimented with deep \acrlong{tl}, we investigated the \acrlong{tl} potential of many neural network architectures that were quite recent at that time. Since we published our first article (which is the subject of Chapter \ref{chap:comp}), many new architectures have been proposed and it would be interesting to evaluate how good these architectures would be in terms of transfer performance to pathology tasks. This includes new \acrlong{cnn} architectures (\eg NASNets \cite{zoph2018learning}, EfficientNets \cite{tan2019efficientnet}, EfficientNetsV2 \cite{tan2021efficientnetv2}, \etc.) but more prominently transformer architectures like \acrshort{vit} which are currently \acrlong{sota} on ImageNet \cite{yu2022coca}.

\paragraph{Improve transfer performance with multi-task pre-training.} There are several areas of improvement to investigate for our multi-task pre-training approach. For instance, we did not implement a way to prevent catastrophic interference which might hinder the transfer performance of the resulting model. This could be achieved by normalizing the input images with a normalization strategy tailored for each input task. The strategy could be as simple as a statistical normalization but we could also investigate normalization techniques specific to pathology like stain normalization \cite{kang2021stainnet, runz2021normalization}. Similarly, data augmentation is known to help with model generalization but we have only applied a minimal augmentation procedure with random flipping and cropping. Therefore, it would be interesting to consider other augmentation techniques: color perturbation, blurring, \etc. 

\paragraph{Consider more tasks and types of task.} We have used 22 classification tasks for pre-training our models. It would be interesting to add more tasks to the pool to reinforce even more the versatility of the pre-trained model. Since the publication or the related article in 2020, new pathology datasets have been released and would be good candidate to be integrated (\eg CoNiC and NuCLS introduced in Table \ref{tab:backdp:datascarcity-grandchallenge}).  Moreover, only supporting classification tasks is restrictive as it prevents from exploiting other kind of datasets like detection or segmentation (those that cannot be easily converted to classification at least). One way of including these types of task would be to use new head architectures. For instance, the classification head could be replaced by a decoder from a UNet model to support a segmentation task. This kind of change would require careful consideration because adding a large head for a bunch of tasks would increase the memory and time requirements for training the model. In the same spirit, it would be interesting to study the effects of using more complex heads for classification as well.  

\paragraph{Study further our self-training method.} It would be interesting to test our self-training approach with more than our four datasets. This would allow to study more precisely what would be the effect of the density or size of annotations on self-training performance, especially with regard to the weighting strategy as it seems that the choice of such a strategy is dataset dependent. It would be also interesting to study further how hyperameters of the weighting strategies or training hyperparameters (\eg learning rate, model complexity, \etc.) impact self-training performance.

\paragraph{Extend the method to multi-class segmentation.} Currently, the method only supports binary segmentation. This extension would start by changing the sigmoid output layer of UNet by a softmax function which implies that a label (and by extention a pseudo-label) is not a probability anymore but rather a probability distribution. Many aspects of our approach can be trivially adapted to this change (cross-entropty loss, soft labeling, entropy weighting strategy, \etc.). There are two elements in particular that require a little more thoughts. The first is hard labeling as thresholding is not relevant anymore. The second is the consistency weighting strategy as a new function to compare probability distributions must be chosen.      

\paragraph{Study self-training with larger datasets.} We have mostly studied how our method performed in high data scarcity conditions. However, self-training methods have shown to improve performance in context where data is plentiful (\eg \cite{zoph2020rethinking}). This can be done for instance by pseudo-labeling entirely unlabeled datasets. When working with \acrlong{wsi}s, it is common to only have regions of interest annotated instead of entire slides. Therefore, this leaves out many unlabeled area that could be included as training data after pseudo-labeling. It would be interesting to study how including these would help improve performance. 

\paragraph{Self-training as an interactive annotation algorithm.} More and more, \acrlong{ai} methods are used to assist dataset annotation. Because a dataset being annotated is sparse by nature, we believe that a self-training approach could be leveraged to train a model using the data being annotated. The resulting model could then be used to suggest annotation that could be refined by the annotator.   

\paragraph{Mixing it all up.} There is nothing that prevents the different methods explored in this thesis to be used together. Indeed, transfer learning can be used to initialize the encoder part of a Unet, multi-task learning can be used to train a segmentation model. More interestingly, we could use self-training and multi-task learning together to train a classification or segmentation model. For instance, we could train a multi-task UNet on the datasets used in Chapter \ref{chap:strain} (\acrshort{monuseg}, \acrshort{segpc} and \acrshort{glas} and Thyroid \acrshort{fnab}) simultaneously. Every epoch we would pseudo-label unlabeled regions of the Thyroid \acrlong{wsi}s that could be included as training data for the next training epoch.



% differential learn weighting strategy

% evaluate an interactive use of the algorithm, for instance to assist the labeling process of a new dataset  

% \paragraph{Mixing it all up}

% mixing all methods:
% - use a pre-trained model for initializing the segmentation network before starting warm-up and self-training
% - multi task self-training ? 