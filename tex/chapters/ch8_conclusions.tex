\chapter{Conclusion and future works}
\label{chap:conclusions}

\section{Wrapping up}
\label{sec:concl:wrappingup}

In this thesis, we investigated how image classification and segmentation \acrlong{ml} techniques could be applied in the context of \acrlong{dp}. In particular, we studied how different methods could be used to alleviate the consequences of data scarcity, a prevalent issue in the domain. Causes and consequences of data scarcity are numerous and are discussed Chapter \ref{chap:backdp}. Some of the consequences include small dataset size, lack of variety and oversimplification of the learning tasks, \etc. These can have a significant negative impact on the efficiency of machine and especially \acrlong{dl} methods which are known to be particularly data-hungry. 

In Part \ref{part:transfer}, we first investigated transfer learning. In Chapter \ref{chap:comp}, we compared different ways seven popular \acrlong{dl} architectures pre-trained on ImageNet could be transferred to pathology classification tasks. We studied both feature extraction and fine-tuning. Regarding feature extraction, we considered features not only from the last layer of the networks but also from inner layers. Moreover, we devised an experiment to evaluate the redundancy of these features using \acrlong{rfe}. We showed that, although source (\ie natural images) and target (\ie pathology images) domains are quite dissimilar, both feature extraction and fine-tuning from ImageNet were effective transfer techniques as they were able to improve significantly over our baseline. Moreover, fine-tuning proved to be more efficient than feature extraction. Among the architectures we have tested, ResNet50 and DenseNet121 often yielded the best performance.

Moving on to Chapter \ref{chap:mtask}, guided by the fact that transfer learning is known to perform better when source and target tasks are close, we searched for a way to create a transferrable model pre-trained on pathology data. However, because of data scarcity, it was (and still is to this date) not possible to find a sufficently large and versatile pathology dataset to pre-train a \acrlong{dl} model. Therefore, we decided to exploit the fact that many small and medium pathology datasets had been released over the years and decided to pre-train our model in a multi-task fashion using as many of these datasets as possible. We collected 22 classification tasks featuring 81 classes and almost 900k images and designed a multi-task pre-training protocol to exploit them. We showed that our models used as feature extractors either outperformed or provided comparable performance compared to models pre-trained on ImageNet. We also observed that fine-tuning multi-task or ImageNet pre-trained models yielded comparable performance, indicating that fine-tuning was able to recover the lack of specificity of ImageNet features. We also confirmed the conclusions from Chapter \ref{chap:comp} as we showed that fine-tuning outperformed feature extraction. Moreover, we also confirmed the consensus that transfer was more efficient than training from scratch.

In Part \ref{part:segmentation} and Chapter \ref{chap:strain}, we focused on \acrlong{ssl} and self-training in particular. We proposed a self-training algorithm for image segmentation able to make use of both exhaustively and sparsely-labeled data. Every training epoch, our approach generates pseudo-labels for unlabeled pixels in the training set using the currently trained model and combines them with the sparse ground truth. The model can then be trained on the combination of the pseudo-labeled and exhaustively-labeled set. Using three exhaustively-labeled datasets that we artificially made sparse, we showed that our self-training approach was able to improve performance over a fully supervised approach learning from the exhaustively-labeled data only. For one of our datasets, self-training was even able to reach performance of a model trained on a completely annotated dataset with only $\sim 30\%$ of the training annotations. We confirmed our findings on a sparsely-labeled dataset annotated by collaborators using Cytomine. On this dataset, we showed that self-training improved performance by a $5\%$ margin compared to our baselines. We finally showed that when annotating a new dataset, it was not necessarily better to annotate it exhaustively. 

We hope that, with these three contributions, we have confirmed that data scarcity in \acrlong{dp} was not an inextricable issue and that there exists algorithmic solutions to at least alleviate it. 

\section{Future works}
\label{sec:concl:futureworks}

In this section, we discuss future perspectives for our works.

\paragraph{Transfer with new architectures.} In 2018, when we experimented with deep \acrlong{tl}, we investigated the \acrlong{tl} potential of many neural network architectures that were quite recent at that time. Since we published our first article (which is the subject of Chapter \ref{chap:comp}), many new architectures have been proposed and it would be interesting to evaluate how good these architectures would be in terms of transfer performance to pathology tasks. This includes new \acrlong{cnn} architectures (\eg NASNets \cite{zoph2018learning}, EfficientNets \cite{tan2019efficientnet}, EfficientNetsV2 \cite{tan2021efficientnetv2}, \etc.) but more prominently transformer architectures like \acrshort{vit} which are currently \acrlong{sota} on ImageNet \cite{yu2022coca}.

\paragraph{Consider more tasks and types of task.} We have used 22 classification tasks for pre-training our models in multi-task. It would be interesting to add more tasks to the pool to reinforce even more the versatility of the pre-trained model. Since the publication or the related article in 2020, new pathology datasets have been released and would be good candidate to be integrated (\eg CoNiC and NuCLS introduced in Table \ref{tab:backdp:datascarcity-grandchallenge}).  Moreover, only supporting classification tasks is restrictive as it prevents from exploiting other kind of datasets like detection or segmentation (those that cannot be easily converted to classification at least). One way of including these types of task would be to use new head architectures. For instance, the classification head could be replaced by a decoder from a UNet model to support a segmentation task. This kind of change would require careful consideration because adding a large head for a bunch of tasks would increase the memory and time requirements for training the model. In the same spirit, it would be interesting to study the effects of using more complex heads for classification as well.  

\paragraph{Study self-training with larger datasets.} We have mostly studied how our method performed in high data scarcity conditions. However, self-training methods have shown to improve performance in context where data is plentiful (\eg \cite{zoph2020rethinking}). This can be done for instance by pseudo-labeling entirely unlabeled datasets. When working with \acrlong{wsi}s, it is common to only have regions of interest annotated instead of entire slides. Therefore, this leaves out many unlabeled area that could be included as training data after pseudo-labeling. It would be interesting to study how including these would help improve performance. There exists pathology datasets that could directly be used for this purpose like, for instance, Camelyon \cite{litjens2018camelyon} which features 209 \acrlong{wsi}s with pixel-wise segmentation labels and 1190 unannotated slides. Another example is the Thyroid \acrshort{fnab} dataset used in Chapter \ref{chap:strain}. Implementing such an experiment raises interesting technical questions regarding processing time as it is probably not necessary nor efficient to predict pseudo-labels for billions of unlabeled pixels every training round as only a subset of them will likely be used for the next training round.  

\paragraph{Self-training as an interactive annotation algorithm.} As discussed in Chapter \ref{chap:strain}, one possible application of our self-training algorithm would be to use it as an interactive assistant for annotating new datasets. Because a dataset being annotated is sparse by nature, we believe that a self-training approach could be leveraged to train a model using the data being annotated. The model, while being trained, could also be used to generate segmentation masks to be refined and corrected by human annotators. The resulting corrected annotations could be incuded in the training set. Investigating this idea would require not only to re-implement the self-training in an interactive way but also raises practical questions about graphical user interface and user experience. The system should for instance minimize latency when a user requests annotations for a selected area meaning that the model should be able to receive queries while being trained. The training process should handle addition of new training data on-the-fly as provided by the annotator. The interface should provide an \acrshort{wsi} viewer and efficient annotation and correction tools (\eg Cytomine).

\paragraph{Systematic benchmarking of representation learning methods in pathology.} Representation learning is interested in methods that train a model able to generate a rich representation for a data sample (\eg in our case, feature vectors for pathology images). In Part \ref{part:transfer}, we have investigated few representation learning approaches focused on transfer from ImageNet, fine-tuning and multi-task pre-training. These are only a few examples of representation learning as there exist many more ways of learning a representation which have been explored by the community. A particularly popular approach is self-supervised learning (see Section \ref{ssec:backml:usl}) which has seen a surge of interest by the \acrlong{cpath} community in the past few years (\eg \cite{xie2020instance, wang2021transpath, boyd2021self, koohbanani2021self, chen2022self}). Although each contribution individually provides a comparison against popular techniques, they are not easily comparable between each other because they do not use the same reference datasets as benchmarks. In the same spirit as Biaflows \cite{rubens2020biaflows} (see Appendix \ref{app:biaflows}), it would be interesting to provide a way to systematically benchmark the performance of a representation learning methods on an open platform.

\paragraph{Mixing it all up.} There is nothing that prevents the different methods explored in this thesis to be used together. Indeed, transfer learning can be used to initialize the encoder part of a Unet, multi-task learning can be used to train a segmentation model. More interestingly, we could use self-training and multi-task learning together to train a classification or a segmentation model. For instance, we could train a multi-task UNet on the datasets used in Chapter \ref{chap:strain} (\acrshort{monuseg}, \acrshort{segpc} and \acrshort{glas} and Thyroid \acrshort{fnab}) simultaneously. Every epoch we would pseudo-label unlabeled regions of the Thyroid \acrlong{wsi}s that could be included as training data for the next training epoch.
