\chapter{Machine learning}
\label{chap:backml}

\begin{overview}{Overview}
The goal of this chapter is mainly to give some keys to understand our contributions to a reader with basic knowledge of \acrlong{ml}. It is not aimed at being a thorough presentation of the field or of the different methods but rather an overview. For the readers who would still like to deepen their knowledge about these methods, we provide pointers to relevant litterature. In this chapter, we also introduce the notations that we use throughout this thesis. 

Section \ref{sec:backml:whatisml} provides a short definition of \acrlong{ml} and a first example of an \acrshort{ml} problem. Section \ref{sec:backml:families} explores the different ways the field of \acrlong{ml} can be structured (\eg supervised vs. unsupervised learning, classification vs. regression, classical machine vs. \acrlong{dl}...) in order to position our work in its context.   
\end{overview}

\section{What is machine learning ?} 
\label{sec:backml:whatisml}

A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$ \parencite{mitchell1997machine}. Machine learning concerns the study of such programs, commonly referred to as \textit{models}, and how to build them by learning. A \textit{model} $h$ can be seen a function taking an input $\vect{x} \in \mathcal{X}$ (\aka observation, example, instance) and producing an ouput $h(\vect{x})$ or $\hat{y} \in \mathcal{Y}$. Entities $\vect{x}$ and $h(\vect{x})$ can be  n-dimensional tensors and encode many kind of data types: data record, image, graph, time series, text... When $\vect{x}$ is a vector, its components are commonly called \textit{features}, \textit{variables} or \textit{attributes}. A model can have several inputs (\resp outputs) in which case $\vect{x}$ (\resp $y$) is a $n$-tuple.

In \acrlong{ml}, a model is built by a \textit{learning algorithm}. Formally, a learning algorithm is defined by a set $\mathcal{H}$ of candidate models called the \textit{hypothesis space}, a performance measure $P$ for a model and an optimization strategy. As input, the algorithm is provided with \textit{training data} (the experience $E$, \aka learning or training set) that it uses to build, optimize and evaluate the model. The algorithm output is a model $h \in \mathcal{H}$ that maximises the performance criterion. 

As an example, a common task is \textit{natural image classification} where the model must assign a label to a picture. For instance, one would want to detect whether a picture contains a human, an animal or an inanimate object. In this case, considering that the images are encoded with integers, the input space is the set of all possible color images: $\mathcal{X} \subset \mathbb{N}^{h\times w\times c}$ where $w$, $h$ and $c$ are respectively the image width, height and number of channels (which equals to 3 in RGB color images). The output space contains the 3 labels of interest: $\mathcal{Y} = \left\{\textit{human}, \textit{animal}, \textit{object}\right\}$. The model would take an image $\vect{x} \in \mathcal{X}$ as input and, based on its content, output $\hat{y}$, one of the predefined labels. This label $\hat{y}$ might be false if the model makes a mistake. Therefore, for the sake of distinction, the correct label is denoted $y$ (\aka ground truth). As a performance measure $P$, one could assess the correctness of the ouput label by assigning 0 to correct predictions and 1 to errors. This performance measure is called the \textit{zero-one loss} and is written as:

\begin{equation}
\ell_{0-1}(y, \hat{y}) = \mathbb{1}_{y\neq\hat{y}}
\end{equation}

There are numerous tasks beyond natural image classification to which \acrlong{ml} can be applied nowadays. In the next sections, we will discuss some of them and dive a little deeper into algorithms and topics related to learning which are relevant to this thesis.


\section{Families of learning methods}
\label{sec:backml:families}

There are many ways to structure the ecosystem of \acrlong{ml} methods. This section explores some of them.

\subsection{Supervised learning}
\label{ssec:backml:sl}

\textit{Supervised learning} (\acrshort{sl}) regroups methods where the learning process is guided by an output signal. We formalize a supervised task as the tuple $\left(\mathcal{X}; \mathcal{Y}; p(\vect{x}, y)\right)$ where $\mathcal{X}$ and $\mathcal{Y}$ are respectively the input and output spaces and $p(\vect{x}, y)$ is a probability distribution over those joint spaces. The learning algorithm is provided with a training set 
\begin{equation}
\label{eqn:backml:supervised}
\left\{(\vect{x}_i, y_i) \mid i = 1,...,n ; (\vect{x}_i, y_i) \sim p(\vect{x}, y) \right\}
\end{equation}
where $y_i \in \mathcal{Y}$ is the output signal for the observation $\vect{x}_i \in \mathcal{X}$. The learning algorithm objective is to find a model $h \in \mathcal{H}$ that approximates at best the output. 

When the output space is a finite set of discrete scalar values $\mathcal{Y} = \left\{1, 2, ..., C\right\}$, the problem is called \textit{classification}. $C$ denotes the cardinality of the set $\mathcal{Y}$ or the number of classes. When $C = 2$, the problem is said to be \textit{binary}. In classification, the model should predict the most probable output class given an example:
\begin{equation}
\hat{y}_i = \argmax{y_k \in \mathcal{Y}} p(y=y_k|\vect{x}=\vect{x}_i)
\end{equation}
Examples of classification tasks are for instance assigning a label to an image or detecting whether an email is a spam or not. 

When the output space is continuous and the output is a real scalar value, the problem is called \textit{regression}. In this case, the model should predict the expected value $y$ given any input $\vect{x}_i$:
\begin{equation}
h(\vect{x}_i) = \mathbb{E}\left[y|\vect{x}=\vect{x}_i\right]
\end{equation}
Examples of regression problems are trying to predict the price of a house given its area, the amount of product generated by a factory in a given time period or the review score of a product on an e-commerce platform. 

Beyond scalar outputs, some models sometimes predict structured outputs. This is the case for \textit{image segmentation} which focuses on classifying each pixel of an image (\ie what kind of object does this pixel belong to in the image?). The output of the model is a segmentation mask where pixel at row $i$ and column $j$ is classified as $\hat{y}_{ij} \in \mathcal{Y}$. For some tasks, a mask is not always necessary, one is rather interested in the coarse location of the objects. This kind of task is called \textit{detection} where the output of the model, the location, can be encoded as image coordinates $(i, j)$ representing the object's center of gravity or any of its point. Another common representation is the bounding box, a box containing exactly the object of interest, encoded by the position of a corner of the box in the image and its height and width.

In \acrlong{sl}, the training signal is often created by humans by manually annotating examples. Such guidance allows using well-studied methods and usually helps learning strong models but also comes at a cost because of the human intervention. This is especially aggravated when the target task is difficult as, the more complex, the more data is required to sample sufficiently the input space. In some domains, annotations are particularily cumbersome to obtain because one lacks raw data, or because the annotation process requires the intevention of experts (\eg medical data). In this case, this issue is referred to as \textit{data scarcity}. In general, the lack of data hampers the successful application of \acrlong{sl} but several approaches exist to work it around which are presented briefly in the following sections. 

\subsection{Unsupervised learning}
\label{ssec:backml:usl}

In opposition to \acrlong{sl}, \acrfirstit{usl} regroups methods where no output signal is provided to guide the learning process. The learning algorithm is provided with 
\begin{equation}
\left\{\vect{x}_i \mid i = 1,..., n ; \vect{x}_i \sim p(\vect{x})\right\}
\end{equation} 
and attempts to extract information from this dataset. A common unsupervised task is \textit{density estimation} where the goal is to model the generating distribution $p(\vect{x})$ but there also exists other types of methods. For instance, \textit{clustering} where the algorithm searches for natural groups of observations or features. Another example is \textit{dimensionality reduction} where the algorithm projects high-dimensional data into a lower-dimensional space while attempting to preserve as much information as possible. An interested reader will find more information about these methods in \parencite{friedman2017elements}. There exists another family of \acrlong{usl} methods that was first explored in the 1980s with autoencoders but has gained much traction recently. It is called \textit{self-supervised learning} \parencite{lecun2021self}. The idea behind this family of methods is to exploit supervised learning algorithms but rather than guiding the learning process with human annotations, the training signal is found in the data itself. An example of such methods is \textit{image reconstruction}. Random parts of the input images are truncated (\eg replaced by black squares) and the model must be able to re-generate the truncated parts. In this case, input and output signal are respectively the original and the truncated image. The model input can be generated from the original data without human intervention. 

\subsection{Between supervised and unsupervised learning}
\label{ssec:backml:inbetween}

The families discussed in Sections \ref{ssec:backml:sl} and \ref{ssec:backml:usl} do not cover all the existing \acrlong{ml} methods but are rather at the ends of a spectrum. There also exists intermediate families of methods. Halfway between supervised and \acrlong{usl} is \acrfirstit{ssl} which focuses on methods that use a dataset where only a part of the observations have an associated output signal. The dataset is composed of two subsets:
\begin{eqnarray}
S_a = \left\{(\vect{x}_i, y_i) \mid i = 1,...,n_a; (\vect{x}_i, y_i) \sim p_{\mathcal{X},\mathcal{Y}}; \vect{x}_i \sim p_a(\vect{x})\right\} \\
S_u = \left\{\vect{x}_i \mid i = 1,...,n_u; \vect{x}_i \sim p_u(\vect{x})\right\}
\end{eqnarray}
These  methods make some assumptions about the ``closeness'' of the input distributions $p_a(\vect{x})$ and $p_u(\vect{x})$ \parencite{chapelle2006semi} which allow exploiting both sets to solve some particular tasks. One of the earliest forms of \acrlong{ssl} is \textit{self-training} where the model is initially trained in a supervised manner on the set $S_a$, then iteratively refined by using both $S_a$ and $S_u$ as training data until a certain quality criterion is met. In order to use the second set, an output signal is predicted for all samples of $S_i$ with the model being trained. 

Closer to \acrlong{sl} is \acrfirstit{wsl} which focuses on methods where the annotations are noisy (\eg $y$ can be incorrect or imprecise), coarse or incomplete (similar to \acrlong{usl}\TODO{check any diff with usl}). The challenge of working with coarse annotations consists in having a model output $h(\vect{x})$ that contains more information than the training signal $y$. Coming back to our example of Section \ref{sec:backml:whatisml}, a weakly-supervised problem would consist in locating the human, animal or object in the image using only the label as training signal.


\subsection{Shallow versus \acrlong{dl}}
\label{ssec:backml:shallowdeep}

Many things in our world can be viewed as hierarchies of concepts. For instance, a human body is composed of body parts like a head. The head includes the face which is itself composed of several elements: cheeks, eyes, nose, \etc. This decomposition could be refined even more and could also be applied to other concepts. Understanding hierarchies is sometimes key to learn efficiently. In computer vision, for example, an image can be seen as a hierarchy going from the actual objects it contains down to the pixels. Direct interpretation of individual pixels is rarely enough for learning anything meaningful. However, pixels combined together make edges, which themselves make textures, which are eventually combined in several rounds to reach meaningful semantic elements (see Figure \TODO{add figure to illustrate hierarchy of concept}). Therefore, being able to understand a hierarchy is a first step towards image understanding.   

Most of the \acrlong{ml} methods developed until recently can arguably be considered ``\textit{shallow}'' which means that they are not complex enough (\eg a linear model) or their learning process is not adapted (\eg a decision tree) to learn or exploit such hierarchies. In order to successfully apply these methods to complex data with hierarchical structure, one usually needs to help the learning algorithm by pre-processing the data and extract meaningful information using field knowledge. This process is called \textit{manual feature extraction} or \textit{engineering} and has been an important part of the application of machine learning algorithms. In computer vision, a great body of work is focused on creating complicated pipelines of feature extraction that produce hundreds of different features that can be used for image understanding (\eg SURF \parencite{bay2006surf}, ORB \parencite{rublee2011orb}). However, more recently ``\acrlong{dl}'' methods based on neural networks have shown that manual feature extraction was usually not the best performing approach for a wide variety of tasks.

Although neural network research is as old as machine learning itself, the real breakthrough of \acrlong{dl} happened in 2012 when the third iteration of a \acrlong{ml} challenge called ImageNet \parencite{russakovsky2015imagenet} was organized. One of the tasks was image classification and all but the best methods used combinations of shallow learning algorithms and features extraction. However, the winning team did not and, building upon neural network research form the previous decades, used a deep convolutional neural network \parencite{krizhevsky2012imagenet} trained on the raw images instead. They beat the second best method by a margin of 11\% error rate. This extraordinary improvement revived the interest of the \acrlong{ml} community to neural networks and launched the ``\acrlong{dl} revolution''. Nowadays, it is a bit clearer why the winning method (\ie the AlexNet architecture) worked so well. Indeed, in its design, the deep neural network incorporates some particularily beneficial \textit{inductive bias}, a set of assumptions that narrow the search of a good model in the hypothesis space. This inductive bias includes a trainable multi-layered (hence ``deep'') structure that can automatically learn hierarchical concepts from the data directly. In other words, instead of manual feature engineering, features are learned automatically. Nowadays, \acrlong{dl} is a thriving research field that has grown far beyond image classification. Section \ref{sec:backml:deeplearning} dives a little further into \acrlong{dl} concepts relevant to this thesis.

\subsection{Transfer learning}
\label{ssec:backml:transfer}

As humans, we have extraordinary learning capabilites. Throughout our lives, we learn to move, communicate, interact with our environments and more. One specific learning ability that we possess is to use knowledge we have acquired in a given context to learn faster in a different context. For example, someone who already plays the violin will probably feel it easier to learn to play the piano than someone who has no music education at all. In a way, we ``transfer'' knowledge and skills from a task to another. 

This idea has been applied in \acrlong{ml} and from this application emerged \acrfirstit{tl} \parencite{yang2020transfer}. This field studies the ways knowledge learned from one or more tasks, called the \textit{source tasks}, can be exploited to learn more effectively on another task, the \textit{target task}. This subject has been researched for few decades now, as the first contributions about \acrlong{tl} date back to the end of the 1970s \parencite{bozinovski2020reminder}. A surge of interest happened in the 1990s notably with a NIPS-95 workshop called ``\textit{Learning to Learn: Knowledge Consolidation and Transfer in Inductive Systems}'' which discussed the importance of retaining previously-learned information for efficient learning. Since then, the interest has only been growing and the emergence of \acrlong{dl} has created new possibilities for \acrlong{tl}. 

Transfer learning methods are organized based on the properties of the source and target tasks. The different types of supervision (or lack thereof) discussed in Sections \ref{ssec:backml:sl} and \ref{ssec:backml:usl} also apply for \acrlong{tl} in which case the supervision qualifier relates to the target task only. For instance, in supervised \acrlong{tl}, the target task is a supervised dataset as described in Equation \ref{eqn:backml:supervised}. For the remainder of the section, we will assume that the source tasks are also supervised. 

Transfer learning can be \textit{homogeneous} (see Definition \ref{def:backml:homotransfer}) when the source and target tasks only differ by the distributions of their data. As an example, let us suppose we would want classify pictures taken with a camera equipped with a certain sensor (dataset $B$) and that we also have at hands another dataset of pictures taken with a camera equipped with another type of sensor (dataset $A$). Each captor has a certain noise pattern which results in dataset $A$ and $B$ to have a slighlty different distributions in the pixel intensities (\ie $p_A(x) \neq p_B(x)$). This specific setup where only the inputs distributions differ is also called \textit{domain adaptation}.  

\begin{definition}
\label{def:backml:homotransfer}
Transfer learning between a source task $\left(\mathcal{X}_{s}, \mathcal{Y}_{s}, p_{s}(\vect{x}, y)\right)$ and a target task $\left(\mathcal{X}_t, \mathcal{Y}_t, p_t(\vect{x}, y)\right)$ is said to be \textbf{homogeneous} when $\mathcal{X}_s \cap \mathcal{X}_t \neq \emptyset$ and $\mathcal{Y}_s = \mathcal{Y}_t$ but $p_s(\vect{x}) \neq p_t(\vect{x})$ or $p_s(y|\vect{x}) \neq p_t(y|\vect{x})$. 
\end{definition}

Transfer learning can be \textit{heterogeneous} (see Definition \ref{def:backml:heterotransfer}). An example that will be addressed later in this thesis is the transfer of a model trained for natural image classification to medical image classification. In this case, the tasks are different as the first consists in identifying the presence of a type of object in the image whereas the other consists in assessing the malignancy of a tumor from an image of a tissue. The input distributions also differ as medical images have completely different content and appearance.  

\begin{definition}
\label{def:backml:heterotransfer}
Transfer learning between a source task $\left(\mathcal{X}_{s}, \mathcal{Y}_{s}, p_{s}(\vect{x}, y)\right)$ and a target task $\left(\mathcal{X}_t, \mathcal{Y}_t, p_t(\vect{x}, y)\right)$ is said to be \textbf{heterogeneous} when $\mathcal{X}_s \cap \mathcal{X}_t = \emptyset$ and/or $\mathcal{Y}_s \neq \mathcal{Y}_t$.
\end{definition}

When the decision to apply \acrlong{tl} has been taken, remains the choice of the transfer approach. Based on how they operate, \parencite{yang2020transfer} have identifed four different categories of \acrlong{tl} algorithms: 

\begin{enumerate}
  \item \textit{instance-based}: knowledge transferred corresponds to the weights attached to the source examples,
  \item \textit{feature-based}: knowledge transferred corresponds to the subspace spanned by the features in the source and target domains,
  \item \textit{model-based}: knowledge to be transferred is embedded as a part of the source domain models,
  \item \textit{relation-based}: knowledge to be transferred corresponds to rules specifying the relations between the examples in the source domain. 
\end{enumerate}

Transfer learning performance is influenced by serveral factors including how well the method is able to capture and use transferrable knowledge. The task-relatedness is also an important factor: usually the more similar the tasks, the better the performance. Sometimes, performance are worsen by the use of transfer learning. This happens for instance when the source and target tasks are not related enough. Moreover, the training process on the target task can cause some of the previously-learned knowledge to be lost as most \acrlong{ml} methods do not have a memory mechanism to retain such information. These two phenomena are respectively called \textit{negative transfer} \parencite{zhang2020overcoming} and \textit{catastrophic interference} \parencite{french1999catastrophic}. Although some methods have been proposed to tackle these challenges, how to anticipate and correct them are still open research questions.

\subsection{Multi-task learning}
\label{ssec:backml:mtl}

In \acrlong{tl}, the transfer process happens in two steps. First, knowledge is extracted from the source tasks one way or another, then later used for learning the target task. A similar approach is \acrfirstit{mtl} where, rather than performing the transfer in two steps, everything happens at once: a model is trained on all tasks simultaneously. Compared to learning each task individually, this approach has several advantages: it increases the total amount of data available for training a model, a more robust and universal representations can be learned by sharing knowledge between tasks and, to a certain extent, it prevents the model to overfit a specific task\footnote{See more on overfitting in Section \ref{ssec:backml:bvtradeoff}.}. In the best scenarii, the use of \acrlong{mtl} improves the performance of each individual task compared to a setup where the tasks are treated independently. In opposition, it happens that antagonistic tasks cause the resulting individual task performance to be worse. This issue is related to negative transfer and catastrophic interference introduced in Section \ref{ssec:backml:transfer}. 

Similarily to \acrlong{tl}, \acrlong{mtl} methods can be either \textit{heterogeneous} when the tasks are of different types (\eg supervised, unsupervised, classification, regression), or \textit{homogeneous} when tasks have only one type. In supervised \acrlong{mtl}, \parencite{zhang2017survey} have identified three families of methods:

\begin{enumerate}
  \item \textit{feature-based}: share knowledge through learning features common among the tasks
  \item \textit{instance-based}: identify useful data instances and share knowledge through these intances
  \item \textit{model} or \textit{parameters-based}: use learned models as proxies to extract information about tasks relatedness
\end{enumerate}


% ===================
% Eval and selection
% ===================
\section{Model evaluation and selection}

As stated in Section \ref{sec:backml:whatisml}, evaluation is a core principle of \acrlong{ml} as a learning algorithms should select a model that maximizes a performance criterion. In this section, we introduce different concepts related to the evaluation and selection of machine learning models. In Sections \ref{ssec:backml:bvtradeoff} and \ref{ssec:backml:bvtradeoff}, we discuss the importance of generalization for \acrlong{ml} models and the related topics of bias-variance trade-off and overfitting. In Section \ref{ssec:backml:modelselection}, we discuss further practical consideration related to model selection. In Section \ref{ssec:backml:metrics}, we finally introduce different metrics used in this thesis. 


\subsection{Empirical risk minimization}
\label{ssec:backml:modelselection}
As stated in the introduction, the objective of a learning algorithm is to find a model $h \in \mathcal{H}$ that maximizes a performance measure or, alternatively, minimize a risk which can be formalized as the expected risk (for a supervised problem) \parencite{vapnik1992principles}:

\begin{equation}
\label{eqn:backml:expriskmin}
R(h) = \mathbb{E}_{\mathcal{X},\mathcal{Y}}\left\{\ell\left(h(\vect{x}), y\right)\right\}
\end{equation}

where $\ell: \mathcal{Y}\times\mathcal{Y} \rightarrow \mathbb{R}$ is a loss function which measures the closeness between $h(\vect{x})$ and $y$. The expected risk is also called the \textit{generalization error}. The model that minimizes $R(h)$ is therefore the best possible model for the task and is called the Bayes model $h_B$. In practice, it is rarely possible to directly minimize the expected risk as one does not have access to the true distributions. It is therefore more convenient to work with an unbiased estimator of the estimated risk, the empirical risk, which is evaluated using the available supervised training set $\mathcal{S}$:

\begin{equation}
\label{eqn:backml:empiricalrisk}
R_e(h) = \frac{1}{n} \sum_{i=1}^{|S|} \ell(h_\mathcal{S}(\vect{x}_i), y_i), (\vect{x}_i, y_i) \in \mathcal{S}
\end{equation}

The \acrfirstit{erm} principle suggests that a learning algorithm should pick a model that minimizes the emprical risk in order to approximate $h_B$ as well as possible. Most machine learning algorithms apply this principle.

\subsection{Bias-variance trade-off and overfitting}
\label{ssec:backml:bvtradeoff}

Whereas it can be shown that the empirical risk, under some assumptions, converges to the expected risk when the training set grows ($n \rightarrow \infty$), in practice, one does not have access to an infinitely-large dataset. Therefore, the learned model will often differ from $h_B$ and its expected risk will often be larger. The effect of the ``\textit{choice}'' of a finite training set on the error can be studied using the \textit{bias-variance decomposition} of the error \parencite{geman1992neural, geurts2009bias, friedman2017elements}:

\begin{equation}
\label{eqn:backml:biasvariancedec}
\mathbb{E}_{LS}\left\{\mathbb{E}_{\mathcal{X},\mathcal{Y}}\left\{ \ell(h_{LS}(\vect{x}), y)\right\}\right\} = \text{noise}(\vect{x}) + \text{bias}^2(\vect{x}) + \text{variance}(\vect{x})
\end{equation}

where $\mathbb{E}_{LS}$ is the expectation over all training sets of size $n$ that can be extracted from $p(\vect{x},y)$ and $h_{\mathcal{S}} \in \mathcal{H}$ is the model learned from learning set $\mathcal{S} \in LS$. The \textit{noise} term is the error of $h_B$ and is therefore irreductible. The \text{bias} is the difference between $h_B$ and the average model (over models generated from $LS$). The \textit{variance} measures the variability of the predictions around the average model caused by the training set randomness. Several elements have a direct impact on these error terms such as: model capacity, training set size, noise in the data, \etc. 

The \textit{capacity} or \textit{complexity} of a model is related to its ability to capture complex relationships in the data. Linear models are examples of low-capacity models as they are only able to capture linear relationships. Non-linear models such as random forests (see Section \ref{sec:backml:treebased}) or deep neural networks have a higher complexity or capacity compared to linear models. 

Low-capacity models usually have low variance as their limited expressiveness does not allow them to change much based on the training data but they also have high bias as failing to capture complex relationships prevents the model from approximating $h_B$ correctly, this is \textit{underfitting}. In opposition, high-capacity models have low bias as their expressiveness allow them to approximate $h_B$ more accurately but this expressiveness also leads to high-variance as the model can learn a function which is too expressive compared to $h_B$ and, even worse, can fit the noise in the training data. This phenomenon is called \textit{overfitting} and hampers the models ability to generalize. The \textit{bias-variance trade-off} results from these observations and states that finding a good model (or minimizing the expected risk) consists in finding the trade-off between low- and high-capacity or underfitting and overfitting. In Figure \ref{fig:backml:biasvariancetradeoff} is plotted the classical representation of this trade-off.\TODO{what about the empirical risk in this setting ? in plot ?}

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{backml/biasvariancetradeoff.png}
  \caption{The bias-variance trade-off illustrated.}
  \label{fig:backml:biasvariancetradeoff}
\end{figure}

\subsubsection{Over-parametrized models}

With the advent of \acrlong{dl}, model complexity has exploded as typical \acrlong{dl} models have millions or even billons of parameters (see Section \ref{ssec:backml:dl:modernarchi}). The bias-variance trade-off tells us that these models should be extremely prone to overfitting but the reality is different. Typically, it is not uncommon to find tasks for which \acrlong{dl} models reach an empirical risk close to zero but actually generalize well to unseen data \parencite{zhang2021understanding}. Although the models are able to basically memorize the training data, the learned function is actually very robust. This phenomenon is currently being investigated and several explanations have been proposed such as the deep double-descent \parencite{belkin2019reconciling}. \TODO{check if more to say + add plot}

\subsection{Model selection and evaluation in practice}

These considerations have direct consequences on practical applications of machine learning. In particular, the presence of overfitting causes the empirical risk to reflect poorly the generalization error of a model. There are two common tasks where using the empirical risk only is not reliable which are \textit{model selection} and \textit{model evaluation}. The former consists in finding the model that would generalize best among a set of candidates and the latter consists in evaluating the expected performance of a model when it will be used on new data. Both tasks involve evaluating the generalization of a model. There exists several approaches to solve these tasks. 

The simplest one is the use of an independant test set: the source dataset is split into two subsets, the train and test sets, which are used to respectively train and evaluate the model. The size of the subsets should be made such that the train set is large enough for the model to be able to learn but the test set should be large enough for the estimate of the error to be reliable. Those two objectives are obviously antagonistic. Moreover, this approach is not viable if the source dataset is too small as reducing even more the size of the sets will make both the training and evaluation unreliable. The test-set approach actually evaluates the generalization error of the model for the given dataset but not the expected generalization error used in Section \ref{ssec:backml:bvtradeoff}.

Another approach that actually estimates the expected generalization error is \acrfirstit{cv} which simply repeats the train-test approach with different splits of the source dataset. For each train-test split, a model is trained on the train set and evaluated on the test set. The \acrlong{cv} score is computed by averaging the test scores of all splits. There exists several approaches for \acrlong{cv} whhich differ by how they generate their splits. A common approach is \textit{$k$-fold \acrlong{cv}} where the data is randomly split into $k$ subsets and each subset is selected in turn to be the test set (and the remaining folds make the train set). 

An important consideration is to never use a model selection score as the evaluation score of a model. Indeed, optimizing the choice of a model usually results in the selection score to be an overly optimistic estimator of the generalization performance of the model. In practice, this results in a three-way split of the training dataset: one extracts some train, validation and test sets. The models are trained on the first, selected on the second and the final model is evaluated on last. When the dataset is too small for such a three-way split, a common approach consists in extracting a test set from the initial dataset and then performing \acrlong{cv} on the train set. As a general rule, every decision influenced one way or another by the output $y$ should be done within a validation loop to avoid overfitting (using either \acrlong{cv} or an independant test-set).

These two approaches are based on a strong assumption that the extracted sets are independant. If they are not, this would most probably lead to overfitting and the resulting selection or evaluation scores being overly optimistic. Therefore, it is sometimes not enough to split the sets randomly and domain knowledge must be used during the splitting procedure to enforce sets independance. This issue is sometimes called \textit{data leakage} \parencite{kaufman2012leakage} as information ``leaks'' between the sets. This topic will be discussed in Section \ref{sec:backdp:dataset} as \acrlong{dp} datasets must treated carefully to avoid this issue. 

\subsection{Metrics}
\label{ssec:backml:metrics}

% F1 ?
The metrics presented in this section are mostly supervised classification performance measures. Given a supervised dataset as described in Equation \ref{eqn:backml:supervised}, the metrics evaluate how the predicted class $\hat{y}_i$ compares to the ground truth class $y_i$. In the remainder of the thesis, when the higher the better for a metric, we will call it a \textit{score}. In opposition, when the lower the better, we will call it a \textit{loss}. In terms of notations, a loss and a score computed over a set of data will respectively be noted $\mathcal{L}$ and $\mathcal{M}$.

\subsubsection{Accuracy}
\label{sssec:backml:metric:acc}
The accuracy was introduced in our first example of a machine learning problem in Section \ref{sec:backml:whatisml} with its dual, the zero-one loss. The accuracy assigns 1 to correct predictions and 0 to misclassified samples. The accuracy score can be computed over a dataset:

\begin{equation}
\label{eqn:backml:accuracy}
\mathcal{M}_{\text{acc}} = \dfrac{1}{n} \sum\limits_{i = 1}^n (1 - \ell_{\text{0-1}}(y_i, \hat{y}_i))
\end{equation} 

It has the advantage of being simple to interpret, to compute and it applies to both binary and multi-class problems. However, it is affected by \textit{class imbalance}, the situtation when there is a disparity in the numbers of examples belonging to each of the problem classes in the dataset. For instance, given binary dataset where $n-1$ examples belong to class $A$ and the last example to class $B$, a classifier predicting only class $A$ would obtain an accuracy close to $1$ which seems very good but in reality the classifier did not learn anything from the data.      

\subsubsection{Area under the ROC curve}
\label{sssec:backml:metric:rocauc}

In binary classification, a specific name is given to each type of successful or unsuccessful predictions and these different types of elements form what is called the \textit{confusion matrix} (see Table \ref{tab:backml:confusion}). Each element of this table can be either a number or a proportion of samples falling in the category. The accuracy presented in Section \ref{sssec:backml:metric:acc} can be re-expressed based on this confusion matrix: $\frac{TN + TP}{N + P}$. In some context, however, it is more informative to look at other types of errors. For instance, when the target task consists in diagnosing cancer, one is more interested in assessing the number of false positives and negatives of the method using, for instance, the \textit{specificity} or \textit{sensitivity} (\aka recall) scores.

\begin{equation}
\label{eqn:backml:specifity}
\textit{Specifity} = \frac{TN}{TN + FP} = 1 - \frac{FP}{N}
\end{equation}

\begin{equation}
\label{eqn:backml:sensitivity}
\textit{Sensitivity} = \frac{TP}{P}
\end{equation}

\begin{table}
  \centering
  \begin{tabular}{c|cc|c}
  & \multicolumn{2}{c}{Predicted ($\hat{y}$)} & \\
  Actual ($y$) & 0 & 1 & Total \\
  \hline
  0 & \textbf{T}rue \textbf{N}egative & \textbf{F}alse \textbf{N}egative & \textbf{N}egative \\
  1 & \textbf{F}alse \textbf{P}ositive & \textbf{T}rue \textbf{P}ositive & \textbf{P}ositive \\
  \end{tabular}
  \caption{A confusion matrix for a binary classification problem.}
  \label{tab:backml:confusion}
\end{table}

These kind of metrics should always be studied together as it is often possible to maximise one at the expense of the other. Therefore a common analysis consists in plotting a model as a point $(1 - \textit{specificity}, \textit{sensitivity})$ in two-dimensional graph (in Figure \TODO{use the same as after for roc ??}). When the model outputs class probabilities, it is possible to generate several pairs of the metrics by varying the threshold applied to the probabilities (or any number that can be meaningfully applied a threshold). Such a plot is called the \acrfirstit{roc} curve (see Figure \TODO{find a figure}). From this plot can be derived a new metric, the \textit{area under the \acrshort{roc} curve} (\rocauc). This score can only be computed for model which produce a ``thresholdable'' number, it is relatively computationally expensive to obtain as one have to evaluate all possible thresholds and only applies to binary classification. However, it provides a measure which is not impacted by class imbalance which is a very interesting property in domains where this issue is common. Another advantage of the the \rocaucs is its finer grasp of how a model performs as it evaluates the probablities unlike the accuracy where the model is either wrong or right but there is no in-between.

\subsubsection{Cross-entropy loss}
\label{sssec:backml:metric:crossentropy}

Cross-entropy is rooted in information theory and measures the similarity of two probability distributions $p$ and $q$ that, when defined over a finite and discrete set of events $\mathcal{X}$, is given by:
\begin{equation}
\label{eqn:backml:crossentropy}
H(p, q) = - \sum\limits_{x_i \in \mathcal{X}} p\left(x_i\right) \log q\left(x_i\right)
\end{equation}

When the model outputs class probabilities, this similarity measure can be used to evaluate the discrepency between the prediction and the ground-truth. The cross-entropy becomes a loss function called the \textit{categorical cross-entropy}:
\begin{equation}
\label{eqn:backml:crossentropyloss}
\mathcal{L}_{ce} = - \frac{1}{n} \sum\limits_{i=1}^n \sum\limits_{c=1}^{\left|\mathcal{Y}\right|} y^{(c)}_{i} \log \hat{y}^{(c)}_i
\end{equation}
where $\hat{y}^{(c)}_i$ is the probability predicted by the model for example $i$ and class $c$ and $y^{(c)}_i$ is the one-hot encoding of the ground truth:
\begin{equation}
\label{eqn:backml:onehotencoding}
y^{(c)}_i = 
\begin{cases}
1,\,\text{if}\, y_i = c \\
0,\,\text{otherwise}
\end{cases}
\end{equation} 

In the case of binary classification, the metric falls back to the \acrfirstit{bce}:
\begin{equation}
\label{eqn:backml:bce}
\mathcal{L}_{bce} = - \frac{1}{n} \sum\limits_{i=1}^n \left(y_i \log \hat{y}_1 + (1 - y_i) \log (1 - \hat{y}_i)\right) 
\end{equation} 

Similarly to \rocauc, the cross-entropy evaluates probabilities and therefore has a fine grasp on how the model performs. Another important advantage is its differentiability as it can be used as a loss for directly training differentiable models (\eg \acrlong{dl} models).

\subsubsection{Dice score}
\label{sssec:backml:metric:dice}

As opposed to the previous metrics, the dice score is a set similarity measure that is used to evaluate binary image segmentation. Considering two sets $\mathcal{A}$ and $\mathcal{B}$, the dice score is defined as:
\begin{equation}
\label{eqn:backml:diceAB}
\mathcal{D} = \frac{2 \left|\mathcal{A}\cap \mathcal{B}\right|}{\left|\mathcal{A}\right| + \left|\mathcal{B}\right|}
\end{equation}
When working with image segmentation, $\mathcal{A}$ and $\mathcal{B}$ become the sets of true and predicted binary labels for the pixels in the image and the intersection falls back to a dot product. The resulting formula is:
\begin{equation}
\label{eqn:backml:dice}
\mathcal{M}_{dice} = \dfrac{2 \sum_i\sum_j y_{ij} \hat{y}_{ij} + \epsilon}{\sum_i\sum_j y_{ij} + \sum_i\sum_j \hat{y}_{ij} + \epsilon}
\end{equation}
where $\epsilon$ is a small value added for numerical stability, $y_{ij}$ is the actual class of pixel $(i, j)$ and $\hat{y}_{ij}$ the predicted class for this pixel. The predicted $\hat{y}_{ij}$ can either be binary or a probability. In the latter case, the metric is called the \textit{soft dice score} which is differentiable. When the model outputs class probabilities, instead of using the soft dice score, $\hat{y}_{ij}$ can be binarized using a threshold. 

\subsubsection{Rankings of metrics}
\label{ssec:backml:metric:rankings}

In Part \ref{part:transfer} of this thesis, in order to increase the strength of our results, we use a relatively large set of image classification datasets to study transfer learning. In order to compare the different methods, we evaluate how it performs on those datasets. Unfortunatly, for reasons that will be explained later, it is not possible to use the same metric for all datasets. Moreover, a metric computed on a dataset is not always comparable to the same metric computed on a different dataset. For this reason, we resort to using \textit{rankings of methods}. First, we evaluate each method on each dataset with the best metric for this dataset, then rank all methods on the dataset: the best method gets rank 1 and the worst gets rank $m$ (where $m$ is the number of methods). Finally, in order to draw general conclusions, we average the rankings over all the dataset. The averaged ranks therefore provides a single metric for comparing the methods across datasets. This metric is not ideal \TODO{find references}.


% ===================
% Methods
% ===================
\section{Support vector machines}

The \acrfirstit{svm} algorithm was invented in the early 1990s \parencite{boser1992training}. It is originally a linear method that can be used for regression and classification. By using the \textit{kernel trick}, \acrshort{svm} can also learn non-linear models but it is out of the scope of this thesis. An interested reader will learn more about kernel methods and the kernel trick in \parencite{friedman2017elements}.

In general, assuming a supervised dataset where inputs are such that $\vect{x} \in \mathbb{R}^m$ where $m$ is the number of features, a linear model is an hyperplane:

\begin{equation}
\label{eqn:backml:linearmodel}
f(\vect{x}) = \theta_0 + \sum_{j=1}^m \theta_j x_j  
\end{equation}

where $\theta_0$ and the $\theta_j$ are the learnable parameters, respectively called the \textit{bias} and the \textit{weights}. This model can be used for regression in which case the model $h(\vect{x}) = f(\vect{x})$. A linear model can also be used for binary classification where the linear function is called the \textit{decision boundary} and is considered to separate positive from negative samples in the input space: $h(\vect{x}) = \text{sign}(f(\vect{x}))$ (\ie $\mathcal{Y} = \left\{-1, 1\right\}$). Linear methods differ from each other by the way they learn and generate these weights.

Support vector machines optimize the parameters $\theta_0$ and $\theta_j$ to generate $f$ in such a way that the minimal distance from the hyperplane to the nearest points from the learning set is maximized. These nearest points are called the \textit{support vectors} and the margin between the support vectors is called the \textit{gutter}. An example of a \acrshort{svm} hyperplane is given in Figure \ref{fig:backml:svm}. It can be shown that the \acrshort{svm} optimization problem falls back to the Lagrange dual:
 
\begin{align}
\label{eqn:backml:svmoptimizationproblem}
\underset{\boldsymbol{\alpha}}{\text{max}}& \sum_{k=1}^n \alpha_k - \frac{1}{2} \sum_{i,j=1}^n \alpha_i\alpha_j y_i y_j \vect{x}_i^T \vect{x}_j \\
\text{subject to}~& 0 \leq \alpha_k \leq C, \forall k = 1, ..., n \\
& \sum_{i=1}^n \alpha_i y_i = 0
\end{align}

which can be solved efficiently with classical linear solvers such as LIBLINEAR \parencite{fan2008liblinear}\TODO{libsvm}. When the optimal solution has been found, the weights can be derived by solving the system: 

\begin{equation}
\label{eqn:backml:svmweights}
\alpha_k\left(y_k \left(\theta^T \vect{x}_k + b\right) - 1\right) = 0, \forall k = 1, ..., n
\end{equation}

In Equation \ref{eqn:backml:svmweights}, $\alpha_k > 0$ for the support vectors $x_k$ and $\alpha_k = 0$ for the other samples. This is an interesting property as it implies that only the support vectors and their weights $\alpha_k$ should be stored for future inference. This makes the model very memory efficient for problems where $m \gg n$. This property has also a negative side effect as it means that the model is highly dependent on the support vectors $x_k$. It can therefore change significantly if the $x_k$ are removed or changed which indicates high variance. The $C$ term in the Lagrange dual formulation is an hyperparameter that relaxes the need for linear separability of the training data and allows for training examples to lie within the gutter or to be misclassified. Low values of $C$ make the optimal solution tolerate more of such examples. Reducing the dependency of the model on very few points also decreases the variance but increases the bias. In opposition, high values of $C$ puts a large penalty on points that lie withing the gutter or are misclassified which increases variance but decreases bias. 

\begin{figure}
  \centering
  \subfloat[$C=10000$]{\includegraphics[scale=0.6]{backml/svm_c10000.png}}
  \subfloat[$C=25$. Notice the new points \\ of class $1$ in the class $0$ cluster.]{\includegraphics[scale=0.6]{backml/svm_c25.png}}
  \caption{Example of an hyperplane (the thick black line) learned with \acrshort{svm} applied to a binary classification task ($\vect{x} = (x_1, x_2) \in \mathcal{X} \subseteq \mathbb{R}^2$ and $\mathcal{Y} = \left\{0, 1\right\}$). The support vectors are delineated with bold coloured border. A large value of $C$ penalizes more severely points in the gutter or misclassified.}
  \label{fig:backml:svm}
\end{figure}


\subsection{Multi-class problems}

There exists few approaches to use a binary classification algorithm like \acrshort{svm} in a multi-class context. One of them is called \acrfirstit{ovo} and consists in training $K=\frac{C(C-1)}{2}$ binary classifiers (where $C$ is the number of classes), one for each pair of classes of the dataset. The prediction for a new sample will be the majority class among the predictions of these $K$ models. This approach has the drawback of requiring to learn $\mathcal{O}(C^2)$models which can be inefficient especially if the dataset is large.

Another approach is called \acrfirstit{ovr} and consists in training $C$ models each of which deals with the classification problem of a class versus all the others. The final prediction is given by the model of which the decision function returned largest value. This approach is more computationally efficient than \acrshort{ovo} as only $C$ classifier have to be trained.

\section{Tree-based methods}
\label{sec:backml:treebased}

Tree-based methods are a popular family of methods invented at least twice in the 1980s by the \acrshort{ai} \parencite{quinlan1986induction} and the statistics \parencite{breiman2017classification} communities. Section \ref{ssec:backml:dt} presents the basics of decision tree inference and induction, Section \ref{ssec:backml:rf} introduces the random forest algorithms. Section \ref{ssec:backml:et} presents a variant of the random forest called \acrfirstit{et} and Section \ref{ssec:backml:et_image} finally explores how this method can be used for image classification.

\subsection{Decision trees}
\label{ssec:backml:dt}

Let us consider a supervised dataset where each $\vect{x}_i$ is a vector of attributes:
\begin{equation}
\label{eqn:backml:dt:vectattributes}
\vect{x}_i = \left(a^{(i)}_1, a^{(i)}_2, ..., a^{(i)}_m\right),~a^{(i)}_j \in \mathcal{X}_j
\end{equation}
and each attribute can be either numerical or categorical (binary or multi-valued). A decision tree is a model structured as a tree of which the nodes $\mathcal{Q}$ are divided in two subsets: the internal and leaf nodes $\mathcal{I} \subset \mathcal{Q}$ and $\mathcal{F} \subset \mathcal{Q}$. Each internal node $q \in \mathcal{I}$ tests an input attribute $k_q \in \left\{1,...,m\right\}$ and its $E_q$ outwards edges are associated with non-overlapping subsets:
\begin{equation}
\label{eqn:backml:dt:splitsgeneric}
\mathcal{A}^{(q)}_l \subset \mathcal{X}_{k_q},~l = 0,...,E_q-1
\end{equation}
which are called \textit{splits}. The leaf nodes of the tree are labelled with the predictions of the model. In classification, the prediction can either be a class $\hat{y} \in \mathcal{Y}$ or a probability distribution over $\mathcal{Y}$. In regression, the output is a real value $\hat{y} \in \mathcal{Y}$. Given $x_i$, predicting an output value consists in a top-down traversal of the tree. When reaching an internal node $q$ during the traversal, $\vect{x}_i$ will follow the edge $l$ such that $a^{(i)}_{k_q} \in \mathcal{A}^{(q)}_l$ which leads to one of the children nodes of $q$. This process is repeated until the algorithm reaches a leaf of which the associated labelling is the output of the algorithm for $\vect{x}_i$. 

In the remainder of the thesis, we will only consider numerical attributes and binary splits at each internal node ($E_q = 2$). These splits are defined by a threshold $v_q$ such that:
\begin{align}
\label{eqn:backml:dt:splitsbinary}
\mathcal{A}^{(q)}_0 &= \left]-\infty, v_q\right]\\
\mathcal{A}^{(q)}_1 &= \left]v_q, +\infty\right[
\end{align}
This representation allows for an efficient check at each node as the attribute has just to be compared to the treshold. The inference algorithm is formalized in Algorithm \ref{algo:backml:dtinference} for classification with numerical attributes. 

\begin{algorithm}[t]
  \SetAlgoLined
  \KwData{A sample $\vect{x}_i = \left(a^{(i)}_1, a^{(i)}_2, ..., a^{(i)}_m\right)$ and the root $r$ of a decision tree.}
  \KwResult{A probability distribution over $\mathcal{Y}$, the prediction for $\vect{x}_i$.}
  \SetKwFunction{dtinfer}{TreeInference}
  \Fn{\dtinfer{$r$, $\vect{x}_i$}}{
    $q = r$\;
    \While{$q \not\in \mathcal{F}$}{
      \eIf{$a^{(i)}_{k_q} \leq v_q$}{
        $q = q.\text{left}$\;
      }{
        $q = q.\text{right}$\;
      }
    }
    \KwRet{$q.\text{pred}$}\;
  }
  \caption{Inference with a classification tree and numerical attributes. $q.\text{left}$ and $q.\text{right}$ denote the left and right children of a node and the prediction associated with the leaf node $f \in \mathcal{F}$ is accessed by $f.\text{pred}$. The regression algorithm only differs by its type of output.}
  \label{algo:backml:dtinference}
\end{algorithm}

\subsubsection{Decision tree induction}
\label{sssec:backml:dtinduction}

Top-down tree induction is the name of the process of building a decision tree based on a supervised dataset. It is a greedy algorithm that, for a node and a set of examples $S$, will select an attribute and a split which reduce the most the so-called \textit{impurity} of the node. Then, $S$ will be divided in two subsets $S_0$ and $S_1$ based on the selected split and these will be used to build recursively the left and right sub-trees respectively. This procedure is repeated until a certain stopping criterion is met. An impurity measure $I(S)$ evaluates the disparity of the output labels $y_i$ in a set $S$ of training examples. For classification, a common impurity measure is the Shannon entropy computed on the class frequencies $p_i$:
\begin{equation}
\label{eqn:backml:shannon_entropy}
I(S) = - \sum_{i=1}^C p_i \log p_i
\end{equation}
The simplest stopping criterion would be to stop developing the tree when the subset at the node cannot be further divided because either it is pure (\eg only one class represented in $S$) or because all attributes have a constant value. This algorithm is formalized in Algorithm \ref{algo:backml:dtinduction}. 


\begin{algorithm}[t]
  \SetAlgoLined
  \SetKwFunction{dtinduct}{TreeInduction}
  \SetKwFunction{stopsplit}{StopSplit}
  \SetKwFunction{computepred}{ComputePred}
  \KwData{A  supervised dataset $S$ where each sample $\vect{x}_i = \left(a^{(i)}_1, a^{(i)}_2, ..., a^{(i)}_m\right)$.}
  \KwResult{The root node of a decision tree built for $S$.}
  
  \Fn{\dtinduct{$S$}}{
    \If{\stopsplit($S$)}{
      \KwRet{\text{a node associated with most appropriate labelling given} $S$}\;
    }
    Find the best split $v_q$ for attribute $k_q$ for new node $q$:\;
    $\argmin{v,k} \left[\frac{|S_0|}{|S|} I\left(S_0\right) + \frac{|S_1|}{|S|} I\left(S_1\right)\right] : S_0 = \left\{(\vect{x}_i, y_i) \mid a^{(i)}_k < v\right\}, S_1 = S \setminus S_0 $\; \label{algo:backml:dtinduction:argmax}
    $q.\text{left} =~$\dtinduct{$S_0$}\;
    $q.\text{right} =~$\dtinduct{$S_1$}\;
    \KwRet{$q$}\;
  }
  \;
  \Fn{\stopsplit{$S$}}{
    \KwRet{$true~$\text{if either the $y_i$ or all the attributes are constant in} $S$, $false~$\text{otherwise}}\;
  }
\caption{Decision tree induction. The \texttt{ComputePred($S$)} function computes the most appropriate prediction for a leaf node from $S$ (\eg by taking the majority class in classification or the classes probability distribution).}
\label{algo:backml:dtinduction}
\end{algorithm}

\begin{figure}
  \centering
  \subfloat[Fully-developped]{\includegraphics[scale=0.6]{backml/dt_full.png}}
  \subfloat[Maximum depth of 3]{\includegraphics[scale=0.6]{backml/dt_max_depth3.png}}
  \caption{Decision tree decision boundary (white line). Pre-pruning using the maximum depth parameter reduces the complexity of the model.}
  \label{fig:backml:dt_boundary}
\end{figure}

A tree built using this simple stopping criterion is said to be \textit{fully-developed}. Such a tree is able to capture the training set perfectly which, in practice, hurts generalization as it usually leads to overfitting. In order to alleviate the problem and reduce the variance, there exists several methods such as pruning which consists in preventing the final tree to grow too deep. This can be done during the induction (pre-pruning) or a posteriori (post-pruning). Simple (pre-)pruning techniques are, for example, stopping the induction when a branch reaches a certain depth, or ensuring that all leaf nodes contain at least a given number of samples (see Figure \ref{fig:backml:dt_boundary}). An advantage of decision trees are their interpretability as the rules learned by the model can be easily understood by looking at the tree. In practice, decision trees are rarely used alone because of their high variance.

\subsection{Random forests}
\label{ssec:backml:rf}

Random forests \parencite{breiman2001random} is an ensemble method based on decision trees. Ensemble methods combine the predictions of several models to produce a final prediction. Random forest is part of the family of averaging techniques where models are built independently and their predictions are averaged (\eg a majority vote for classification). It can be shown that this approach reduces variance compared to that of the base model (\eg decision trees) and the more decorrelated the individual models, the more the variance reduction.

In random forests, decorrelation is achieved through \textit{bagging} (\aka bootstrap aggregating) and \textit{random attribute subset selection}. The former consists in training each individual model on a bootstrap sample of the training set: the training set for the $t^{\text{th}}$ decision tree in the ensemble is a set $S_t$ built by sampling with replacement in the original training set $S$. Regarding the attribute subset selection, each tree only considers a subset of $K \in \left\{1,..., m\right\}$ randomly selected features when looking for the best split. In other words, only $K$ random features are considered when searching for $k_q$ at line \ref{algo:backml:dtinduction:argmax} in Algorithm \ref{algo:backml:dtinduction}. These two sources of randomness usually increase the bias as the individual models do not have access to all the original training examples and attributes. However, the variance reduction resulting from ensembling is usually of much greater magnitude resulting in a significant performance improvement compared to using a decision tree alone.

\subsection{Extremely randomized trees}
\label{ssec:backml:et}

The \textit{\acrlong{et}} (\acrshort{et}, \aka extra-trees) \parencite{geurts2006extremely} are a variant of the random forests algorithm and introduces yet another source of decorrelation by selecting the split $v_q$ at random during the induction (instead of optimizing it, see line \ref{algo:backml:dtinduction:argmax} in Algorithm \ref{algo:backml:dtinduction}). Moreover, to attenuate the bias increase, each individual tree is built on the whole training set instead of a boostrap sample. These choices makes the algorithm particularily computationally efficient as it is not necessary to iterate over the training samples to optimize the split anymore.

\subsection{Image classification with extra-trees}
\label{ssec:backml:et_image}

Extra-trees can be used for image classification as presented in \parencite{maree2016towards}. The core idea behind this algorithm is to represent an image as a set of $n_w$ random subwindows. Subwindows are rectangle patches extracted from the image, resized to have all the target height $t_h$ and width $t_w$ and flattened into a vector of $t_h \times t_w \times c$ ($c$, the number of color channels). The original dimensions of the rectangular patches are drawn uniformly at random in a range of proportions $s_{min}$ and $s_{max}$ of the image size. The position of the patches are also drawn uniformly at random. Each subwindow is attributed the class of its parent image. The resulting training dataset containing $n \times n_w$ examples, each a vector of dimension $t_h \times t_w \times c$, can be used to train an extra-trees classifier. At inference, the same extraction process is applied and the final prediction for the image is determined by a majority vote on the predicted classes for its subwindows. Regarding parameters, the more windows and the more trees in the forest, the better. In addition to the tree complexity hyperparameters, the image colorspace, $s_{min}$ and $s_{max}$ should be tuned as well to optimize generalization.

This variant of the algorithm is called \acrfirstit{etdic} but another variant exists where the forest is used as a feature learner (\acrshort{etfl}). The idea of \acrshort{etfl} is to train a limited number of trees with the subwindows similarly to the \acrshort{etdic} approach. But rather than using the forest as a direct classifier, it is used to create a new representation for the image. This representation is a vector of the same dimension as the number of leaves in the forest and value $\nu_i$ for leaf $i$ corresponds to the frequency of subwindows of the image reaching the leaf when propagated into the forest. This representation can then be used to train another classifier like \acrshort{svm}. At inference, the representation for the new samples is extracted from the forest which are then classified with the second classifier. In general, \acrshort{etfl} is superior to \acrshort{etdic} in terms of performance.


\section{Deep learning}
\label{sec:backml:deeplearning}

As introduced in Section \ref{ssec:backml:shallowdeep}, \acrlong{dl} is a broad research field that has grown rapidly since the breakthrough of AlexNet in 2012. It encompasses many different research topics covering all areas of machine learning. In this section, we dive a little further in \acrlong{dl} concepts and methods relevant to this thesis. We introduce the basic components of modern neural network architectures in Section \ref{ssec:backml:dp:components}. Section \ref{ssec:backml:dl:opti} discusses how neural networks are learned and optimized in practice. Section \ref{ssec:backml:dl:modernarchi} presents interesting modern neural network architectures. Finally, in Section \ref{ssec:backml:dl:deeptransfer}, we discuss deep transfer learning.   

\subsection{Components of modern neural networks}
\label{ssec:backml:dp:components}

A feedforward neural network can be seen as a parametrized function approximator $h$ which combines several intermediate functions $h_l(\cdot, \theta^{(l)})$ ($l=1, ..., L$), also called \textit{layers}, through composition: $h(\vect{x}, \theta) = (h^{(1)} \circ h^{(2)} \circ ... \circ h^{(L)})(\vect{x})$. $\theta$ is the set containing all the learnable parameters of the neural network $h$ and $\theta^{(l)}$ are the parameters of the function $h_l$ (or layer $l$). The feedforward nature of the network specifies that information only flows one-way, from the input $\vect{x}$ to the output $y$. In other words, there is no feedback loop. For a reason that will become clearer in Section \ref{ssec:backml:dl:opti}, one additional constraint on the functions $h_l$ is that they must be differentiable. 

Unlike decision trees (for instance) which learn the tree structure automatically, the design of a neural network is performed manually most of the time and consists in choosing and combining the appropriate functions $h^{(l)}$. Although it introduces complexity when it comes to engineering the best architecture for a problem, this modularity actually offers a great flexibility to practioners as, nowadays, there exists a large number of functions to choose from. Moreover, decades of research have highlighted best practices and guidelines for choosing such functions based on the type of data to be processed for instance. 

The idea behind one of the first machine learning algorithm ever published, the perceptron \parencite{rosenblatt1958perceptron}, is still at the core of most \acrlong{dl} architectures today. Loosely modeling the working of brain neurons, the perceptron can be represented as: 
\begin{equation}
\label{eqn:backml:perceptron}
f(\vect{x}) = \sigma\left(\theta_0 + \sum_{i=0}^m \theta_i x_i\right)
\end{equation}
where $\sigma(\cdot): \mathcal{X} \rightarrow \mathbb{R}$ is a non-linear differentiable function called the \textit{activation function}, $\theta_0$ and $\theta_i$ are respectively the learnable bias and weights.

A perceptron alone is simply a linear model and is therefore not complex enough for many type of tasks. However, perceptrons can be combined together into a perceptron layer: a parallel combination of $u$ perceptrons ($u$ is the width of the layer) each with their own set of weights and biases and all taking the same vector as input. A perceptron layer therefore outputs a vector $\vect{z} \in \mathbb{R}^u$. 

In the spirit of learning a hierarchy, perceptron layers can be connected one after another to form a \acrfirstit{mlp} (see Figure \TODO{illustrate mlp}) where each individual perceptron is also called a \textit{neuron}. It can be shown that sufficiently complex \acrshort{mlp}s are universal function approximators (\ie they can learn any function) \parencite{hornik1989multilayer} which is an intersting property. However, realistically dimensioned \acrshort{mlp}s have the drawback of being very complex models easily reaching millions or even billions of learnable parameters. This makes them difficult to optimize and prone to overfitting. Regularization techniques exist to reduce model complexity such as weight decay (\ie smaller absolute weights values are preferred over large ones) or dropout \parencite{srivastava2014dropout} (\eg during training, some neurons are disabled at random) but unfortunately are often not enough. Therefore, whereas \acrshort{mlp}s still have their use in some applications and or as part of larger and more diverse architectures, they are rarely used by themselves nowadays.

\subsubsection{Convolutional neural networks}
\label{sssec:backml:dl:cnn}

As introduced in Section \ref{ssec:backml:shallowdeep}, \textit{\acrlong{cnn}s} (\acrshort{cnn}) are the origin of the so-called \acrlong{dl} revolution. Despite the recently-revived interest, \acrshort{cnn}s have been reasearched since the late 1980s \parencite{lecun1989handwritten}. A \acrshort{cnn} is a network containing at least one convolutional layer. Nowadays, such layers are a core component of all networks processing structured data (time series, images, video, etc). For instance, in image classification, typical architectures stack several convolutional and pooling layers followed by a fully-connected network (\ie a \acrshort{mlp}). Convolutional and pooling layers are introduced in this section.

Let us suppose a naive implementation of an image classifier with a \acrshort{mlp} where each neuron in the first layer is connected to each input pixel. This model is extremely complex and prone to overfitting, as it would realistically contain billions of weights for a regular image. Natural images and related \acrlong{ml} tasks have interesting properties that can be used to improve this naive model. 

A first property is that \textit{pixel correlations are local}, meaning that pixels far from each other in the image are less likely to be correlated than close pixels. It implies that connecting individual neurons to all input pixels is excessive and one would rather want to connect them to patches of close pixels. A second property is that an image can be considered a \textit{stationary signal} (\ie pixel statistics are similar at different locations of the image). Therefore, it is relevant to use the same set of weights for all the locally-connected neurons of a layer. These architectural choices are examples of inductive bias and motivate the \textit{convolutional layer}. Although, we have discussed the case of images, convolutional layers can also be generalized for 1D (\eg time series) or 3D (\eg 3D image volume) when the locality and stationarity assumptions hold.   

A two-dimensional convolutional layer $h^{(l)}$ can be seen as a function generating an arbitrary number $f_{out}$ of \textit{feature maps} $\vect{a}^{(l)} \in \mathbb{R}^{h_{out} \times w_{out} \times f_{out}}$ from an input $\vect{z}^{(l-1)} \in \mathbb{R}^{h_{in} \times w_{in} \times f_{in}}$. Feature map $k \in \left\{1, 2, ..., f_{out}\right\}$ is the result of convoluting a learnable weight tensor $\pmb{\theta}^{(l)}_k$ of dimensions $h_f \times w_f \times f_{in}$ called \textit{kernel} or \textit{filter} over the input $\vect{z}^{(l-1)}$:

\begin{equation}
\label{eqn:backml:convnet}
a^{(l)}_{ijk} = \theta^{(l)}_{0;k} + \sum_{r=1}^{h_f} \sum_{s=1}^{w_f} \sum_{t=1}^{f_{in}} \theta^{(l)}_{r,s,t;k} \times z^{(l-1)}_{i+r,j+s,t}
\end{equation}

where $(i, j)$ are the coordinates of a vector of $\vect{z}^{(l-1)}$, $\theta^{(l)}_{0;k}$ is the bias and $\theta^{(l)}_{r,s,t;k}$ is the weight at coordinates $(r,s,t)$ of tensor $\pmb{\theta}^{(l)}_k$. Similarly as for the perceptron, this linear operation is then followed by a non-linearity:

\begin{equation}
\label{eqn:backml:convnetwithactiv}
\vect{z}^{(l)} = h^{(l)}(\vect{x}) = \sigma\left(\vect{a}^{(l)}\right)
\end{equation}

Modern architectures often use the \acrfirstit{relu} activation function (see Figure \TODO{example}) because it is very efficient to compute and exhibits convenient propreties when it comes to optimizing the network (see Section \ref{ssec:backml:dl:opti}).

Convolutional layer have many hyperparameters starting with the number of feature maps $f_{out}$ and filter dimensions $h_f, w_f$ (typically $h_f \times w_f = 3 \times 3$ or $5 \times 5$) which both directly control the number of learnable parameters of the layer (and therefore its complexity). In addition to those, \textit{stride} $s$ allows reducing the amount of computations of the layer by evaluating the kernel every $s$ pixels (instead of every pixels) along a dimension (see Figure \TODO{stride example}). The \textit{padding} defines how convolution behaves near the edges of the image where all the evaluated pixels might not exist (see Figure \TODO{padding example}). Typical padding options are considering missing pixels to have value 0, mirroring the image at the edge, \etc. Unlike stride, \textit{dilation} is a skipping mechanism which concerns the filter rather than the input pixels (see Figure \TODO{dilation example}). 

In addition to exploiting locality and stationarity, \acrshort{cnn}s have other interesting properties which contribute to their efficiency for processing structured data. They have \textit{equivariance to translation} which means that translating the convolutional layer input along some of the dimensions only translates the output along the same dimensions. For tasks like image classification, object location is often irrelevant for inferring the class. Therefore, the network does not have to learn this invariance which makes training easier. It has also been shown that, for image classification, the learned kernels are similar to classical computer vision filters capturing edges or color patterns in early layer and, as one ascends the network, kernels show compositionability, invariance and class discrimination \parencite{zeiler2014visualizing} confirming the ability of such a network to learn and use the inherent hierarchical structure of the data. Stacking convolutional layers also brings the benefit of increasing the receptive field of late layers. The receptive field of an activation $a^{(l)}_{ijk}$ in a feature map $k$ at layer $l$ is the set of pixels of the input that influence this activation. The larger the receptive field, the more information from the input is considered by the layer. 

Pooling layers are another common components of \acrshort{cnn}s. Their role is to reduce the dimensionality of feature maps by aggregating close activations. In a way, pooling layer can be seen as a special convolutional layer with stride where that the kernel is not learned (but rather computes a possibly non-linear statistics) and the depth of the feature maps before and after pooling do not change (\ie $f_{in} = f_{out}$). Like convolution, pooling has some kernel size, stride and padding hyperparameters. Remains the choice of aggregation function, the most common being \textit{max pooling} and \textit{average pooling}. The former will output the largest activation among the one covered by the kernel:
\begin{equation}
\label{eqn:backml:maxpooling}
a_{ijk} = \text{max} \left\{ x_{i+r,j+s,k} \mid r = 1, ..., h_f; s = 1, ..., w_f \right\}
\end{equation} 
The latter will average the activations covered by the kernel:
\begin{equation}
\label{eqn:backml:avgpooling}
a_{ijk} = \frac{1}{h_f \times w_f} \sum_{r=1}^{h_f} \sum_{s=1}^{w_f}  x_{i+r,j+s,k} 
\end{equation} 

Pooling, similarly to convolution, brings equivariance to translation and increase the receptive field of deeper layers. The reduction of dimensionality also reduces the computational cost of the network as deeper layers have smaller feature maps to process. There exists other type of pooling layers which are described in \parencite{gholamalinezhad2020pooling}. 

\subsection{Neural network optimization}
\label{ssec:backml:dl:opti}
% backprop, optimizer, scheduling, learning rate, batch norm

\subsection{Modern network architectures}
\label{ssec:backml:dl:modernarchi}
% resnet 
% densenet
% unet

\subsubsection{Unet}

Image segmentation tasks generally use \textit{fully-convolutional neural network} where almost all layers are convolutional. Some successful architectures based on \acrshort{cnn}s are presented in Section \ref{ssec:backml:dl:modernarchi}. 

% transformers

\subsection{Deep transfer learning}
\label{ssec:backml:dl:deeptransfer}


\section{Wrapping up}

In the previous sections, we gave an overview of different families of \acrlong{ml} methods. We can now position our work in this context. In Chapters \ref{chap:comp} and \ref{chap:mtask}, both contributions explore heterogeneous model-based \acrlong{tl} by transferring \acrlong{dl} classification models. We use several target tasks to study how \acrlong{tl} performs in the context of \acrlong{dp} (more on this in Chapter \ref{chap:backdp}). The first contribution studies transfer from ImageNet. Motivated by the fact that transfer works better when the source and target tasks are related, the second contribution use homogeneous features-based \acrlong{mtl} as a way to pre-train a model for transfer on \acrlong{dp} data directly.

In Chapter \ref{chap:sdist}, we move to a different type of task: image segmentation. Working with an sparsely-annotated \acrlong{dp} dataset, we use self-training to complete the missing annotations and train a U-Net segmentation model.  

\TODO{biaflows software}